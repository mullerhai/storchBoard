// Generated by the Scala Plugin for the Protocol Buffer Compiler.
// Do not edit!

package org.tensorflow.framework.config

/** Session configuration parameters.
  * The system picks appropriate values for fields that are not set.
  *
  * @param deviceCount
  *   Map from device type name (e.g., "CPU" or "GPU" ) to maximum
  *   number of devices of that type to use.  If a particular device
  *   type is not found in the map, the system picks an appropriate
  *   number.
  * @param intraOpParallelismThreads
  *   The execution of an individual op (for some op types) can be
  *   parallelized on a pool of intra_op_parallelism_threads.
  *   0 means the system picks an appropriate number.
  *  
  *   If you create an ordinary session, e.g., from Python or C++,
  *   then there is exactly one intra op thread pool per process.
  *   The first session created determines the number of threads in this pool.
  *   All subsequent sessions reuse/share this one global pool.
  *  
  *   There are notable exceptions to the default behavior described above:
  *   1. There is an environment variable  for overriding this thread pool,
  *      named TF_OVERRIDE_GLOBAL_THREADPOOL.
  *   2. When connecting to a server, such as a remote `tf.train.Server`
  *      instance, then this option will be ignored altogether.
  * @param interOpParallelismThreads
  *   Nodes that perform blocking operations are enqueued on a pool of
  *   inter_op_parallelism_threads available in each process.
  *  
  *   0 means the system picks an appropriate number.
  *   Negative means all operations are performed in caller's thread.
  *  
  *   Note that the first Session created in the process sets the
  *   number of threads for all future sessions unless use_per_session_threads is
  *   true or session_inter_op_thread_pool is configured.
  * @param usePerSessionThreads
  *   If true, use a new set of threads for this session rather than the global
  *   pool of threads. Only supported by direct sessions.
  *  
  *   If false, use the global threads created by the first session, or the
  *   per-session thread pools configured by session_inter_op_thread_pool.
  *  
  *   This option is deprecated. The same effect can be achieved by setting
  *   session_inter_op_thread_pool to have one element, whose num_threads equals
  *   inter_op_parallelism_threads.
  * @param sessionInterOpThreadPool
  *   This option is experimental - it may be replaced with a different mechanism
  *   in the future.
  *  
  *   Configures session thread pools. If this is configured, then RunOptions for
  *   a Run call can select the thread pool to use.
  *  
  *   The intended use is for when some session invocations need to run in a
  *   background pool limited to a small number of threads:
  *   - For example, a session may be configured to have one large pool (for
  *   regular compute) and one small pool (for periodic, low priority work);
  *   using the small pool is currently the mechanism for limiting the inter-op
  *   parallelism of the low priority work.  Note that it does not limit the
  *   parallelism of work spawned by a single op kernel implementation.
  *   - Using this setting is normally not needed in training, but may help some
  *   serving use cases.
  *   - It is also generally recommended to set the global_name field of this
  *   proto, to avoid creating multiple large pools. It is typically better to
  *   run the non-low-priority work, even across sessions, in a single large
  *   pool.
  * @param placementPeriod
  *   Assignment of Nodes to Devices is recomputed every placement_period
  *   steps until the system warms up (at which point the recomputation
  *   typically slows down automatically).
  * @param deviceFilters
  *   When any filters are present sessions will ignore all devices which do not
  *   match the filters. Each filter can be partially specified, e.g. "/job:ps"
  *   "/job:worker/replica:3", etc.
  * @param gpuOptions
  *   Options that apply to all GPUs.
  * @param pluggableDeviceOptions
  *   Options that apply to pluggable devices.
  * @param allowSoftPlacement
  *   Whether soft placement is allowed. If allow_soft_placement is true,
  *   an op will be placed on CPU if
  *     1. there's no GPU implementation for the OP
  *   or
  *     2. no GPU devices are known or registered
  *   or
  *     3. need to co-locate with reftype input(s) which are from CPU.
  * @param logDevicePlacement
  *   Whether device placements should be logged.
  * @param graphOptions
  *   Options that apply to all graphs.
  * @param operationTimeoutInMs
  *   Global timeout for all blocking operations in this session.  If non-zero,
  *   and not overridden on a per-operation basis, this value will be used as the
  *   deadline for all blocking operations.
  * @param rpcOptions
  *   Options that apply when this session uses the distributed runtime.
  * @param clusterDef
  *   Optional list of all workers to use in this session.
  * @param isolateSessionState
  *   If true, any resources such as Variables used in the session will not be
  *   shared with other sessions. However, when clusterspec propagation is
  *   enabled, this field is ignored and sessions are always isolated.
  * @param shareClusterDevicesInSession
  *   When true, WorkerSessions are created with device attributes from the
  *   full cluster.
  *   This is helpful when a worker wants to partition a graph
  *   (for example during a PartitionedCallOp).
  */
@SerialVersionUID(0L)
final case class ConfigProto(
    deviceCount: _root_.scala.collection.immutable.Map[_root_.scala.Predef.String, _root_.scala.Int] = _root_.scala.collection.immutable.Map.empty,
    intraOpParallelismThreads: _root_.scala.Int = 0,
    interOpParallelismThreads: _root_.scala.Int = 0,
    usePerSessionThreads: _root_.scala.Boolean = false,
    sessionInterOpThreadPool: _root_.scala.Seq[org.tensorflow.framework.config.ThreadPoolOptionProto] = _root_.scala.Seq.empty,
    placementPeriod: _root_.scala.Int = 0,
    deviceFilters: _root_.scala.Seq[_root_.scala.Predef.String] = _root_.scala.Seq.empty,
    gpuOptions: _root_.scala.Option[org.tensorflow.framework.config.GPUOptions] = _root_.scala.None,
    pluggableDeviceOptions: _root_.scala.Option[org.tensorflow.framework.config.GPUOptions] = _root_.scala.None,
    allowSoftPlacement: _root_.scala.Boolean = false,
    logDevicePlacement: _root_.scala.Boolean = false,
    graphOptions: _root_.scala.Option[org.tensorflow.framework.config.GraphOptions] = _root_.scala.None,
    operationTimeoutInMs: _root_.scala.Long = 0L,
    rpcOptions: _root_.scala.Option[tensorboard.rpc_options.RPCOptions] = _root_.scala.None,
    clusterDef: _root_.scala.Option[org.tensorflow.distruntime.cluster.ClusterDef] = _root_.scala.None,
    isolateSessionState: _root_.scala.Boolean = false,
    shareClusterDevicesInSession: _root_.scala.Boolean = false,
    experimental: _root_.scala.Option[org.tensorflow.framework.config.ConfigProto.Experimental] = _root_.scala.None,
    unknownFields: _root_.scalapb.UnknownFieldSet = _root_.scalapb.UnknownFieldSet.empty
    ) extends scalapb.GeneratedMessage with scalapb.lenses.Updatable[ConfigProto] {
    @transient
    private[this] var __serializedSizeMemoized: _root_.scala.Int = 0
    private[this] def __computeSerializedSize(): _root_.scala.Int = {
      var __size = 0
      deviceCount.foreach { __item =>
        val __value = org.tensorflow.framework.config.ConfigProto._typemapper_deviceCount.toBase(__item)
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
      }
      
      {
        val __value = intraOpParallelismThreads
        if (__value != 0) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeInt32Size(2, __value)
        }
      };
      
      {
        val __value = interOpParallelismThreads
        if (__value != 0) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeInt32Size(5, __value)
        }
      };
      
      {
        val __value = usePerSessionThreads
        if (__value != false) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(9, __value)
        }
      };
      sessionInterOpThreadPool.foreach { __item =>
        val __value = __item
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
      }
      
      {
        val __value = placementPeriod
        if (__value != 0) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeInt32Size(3, __value)
        }
      };
      deviceFilters.foreach { __item =>
        val __value = __item
        __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(4, __value)
      }
      if (gpuOptions.isDefined) {
        val __value = gpuOptions.get
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
      };
      if (pluggableDeviceOptions.isDefined) {
        val __value = pluggableDeviceOptions.get
        __size += 2 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
      };
      
      {
        val __value = allowSoftPlacement
        if (__value != false) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(7, __value)
        }
      };
      
      {
        val __value = logDevicePlacement
        if (__value != false) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(8, __value)
        }
      };
      if (graphOptions.isDefined) {
        val __value = graphOptions.get
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
      };
      
      {
        val __value = operationTimeoutInMs
        if (__value != 0L) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeInt64Size(11, __value)
        }
      };
      if (rpcOptions.isDefined) {
        val __value = rpcOptions.get
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
      };
      if (clusterDef.isDefined) {
        val __value = clusterDef.get
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
      };
      
      {
        val __value = isolateSessionState
        if (__value != false) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(15, __value)
        }
      };
      
      {
        val __value = shareClusterDevicesInSession
        if (__value != false) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(17, __value)
        }
      };
      if (experimental.isDefined) {
        val __value = experimental.get
        __size += 2 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
      };
      __size += unknownFields.serializedSize
      __size
    }
    override def serializedSize: _root_.scala.Int = {
      var __size = __serializedSizeMemoized
      if (__size == 0) {
        __size = __computeSerializedSize() + 1
        __serializedSizeMemoized = __size
      }
      __size - 1
      
    }
    def writeTo(`_output__`: _root_.com.google.protobuf.CodedOutputStream): _root_.scala.Unit = {
      deviceCount.foreach { __v =>
        val __m = org.tensorflow.framework.config.ConfigProto._typemapper_deviceCount.toBase(__v)
        _output__.writeTag(1, 2)
        _output__.writeUInt32NoTag(__m.serializedSize)
        __m.writeTo(_output__)
      };
      {
        val __v = intraOpParallelismThreads
        if (__v != 0) {
          _output__.writeInt32(2, __v)
        }
      };
      {
        val __v = placementPeriod
        if (__v != 0) {
          _output__.writeInt32(3, __v)
        }
      };
      deviceFilters.foreach { __v =>
        val __m = __v
        _output__.writeString(4, __m)
      };
      {
        val __v = interOpParallelismThreads
        if (__v != 0) {
          _output__.writeInt32(5, __v)
        }
      };
      gpuOptions.foreach { __v =>
        val __m = __v
        _output__.writeTag(6, 2)
        _output__.writeUInt32NoTag(__m.serializedSize)
        __m.writeTo(_output__)
      };
      {
        val __v = allowSoftPlacement
        if (__v != false) {
          _output__.writeBool(7, __v)
        }
      };
      {
        val __v = logDevicePlacement
        if (__v != false) {
          _output__.writeBool(8, __v)
        }
      };
      {
        val __v = usePerSessionThreads
        if (__v != false) {
          _output__.writeBool(9, __v)
        }
      };
      graphOptions.foreach { __v =>
        val __m = __v
        _output__.writeTag(10, 2)
        _output__.writeUInt32NoTag(__m.serializedSize)
        __m.writeTo(_output__)
      };
      {
        val __v = operationTimeoutInMs
        if (__v != 0L) {
          _output__.writeInt64(11, __v)
        }
      };
      sessionInterOpThreadPool.foreach { __v =>
        val __m = __v
        _output__.writeTag(12, 2)
        _output__.writeUInt32NoTag(__m.serializedSize)
        __m.writeTo(_output__)
      };
      rpcOptions.foreach { __v =>
        val __m = __v
        _output__.writeTag(13, 2)
        _output__.writeUInt32NoTag(__m.serializedSize)
        __m.writeTo(_output__)
      };
      clusterDef.foreach { __v =>
        val __m = __v
        _output__.writeTag(14, 2)
        _output__.writeUInt32NoTag(__m.serializedSize)
        __m.writeTo(_output__)
      };
      {
        val __v = isolateSessionState
        if (__v != false) {
          _output__.writeBool(15, __v)
        }
      };
      experimental.foreach { __v =>
        val __m = __v
        _output__.writeTag(16, 2)
        _output__.writeUInt32NoTag(__m.serializedSize)
        __m.writeTo(_output__)
      };
      {
        val __v = shareClusterDevicesInSession
        if (__v != false) {
          _output__.writeBool(17, __v)
        }
      };
      pluggableDeviceOptions.foreach { __v =>
        val __m = __v
        _output__.writeTag(18, 2)
        _output__.writeUInt32NoTag(__m.serializedSize)
        __m.writeTo(_output__)
      };
      unknownFields.writeTo(_output__)
    }
    def clearDeviceCount = copy(deviceCount = _root_.scala.collection.immutable.Map.empty)
    def addDeviceCount(__vs: (_root_.scala.Predef.String, _root_.scala.Int) *): ConfigProto = addAllDeviceCount(__vs)
    def addAllDeviceCount(__vs: Iterable[(_root_.scala.Predef.String, _root_.scala.Int)]): ConfigProto = copy(deviceCount = deviceCount ++ __vs)
    def withDeviceCount(__v: _root_.scala.collection.immutable.Map[_root_.scala.Predef.String, _root_.scala.Int]): ConfigProto = copy(deviceCount = __v)
    def withIntraOpParallelismThreads(__v: _root_.scala.Int): ConfigProto = copy(intraOpParallelismThreads = __v)
    def withInterOpParallelismThreads(__v: _root_.scala.Int): ConfigProto = copy(interOpParallelismThreads = __v)
    def withUsePerSessionThreads(__v: _root_.scala.Boolean): ConfigProto = copy(usePerSessionThreads = __v)
    def clearSessionInterOpThreadPool = copy(sessionInterOpThreadPool = _root_.scala.Seq.empty)
    def addSessionInterOpThreadPool(__vs: org.tensorflow.framework.config.ThreadPoolOptionProto *): ConfigProto = addAllSessionInterOpThreadPool(__vs)
    def addAllSessionInterOpThreadPool(__vs: Iterable[org.tensorflow.framework.config.ThreadPoolOptionProto]): ConfigProto = copy(sessionInterOpThreadPool = sessionInterOpThreadPool ++ __vs)
    def withSessionInterOpThreadPool(__v: _root_.scala.Seq[org.tensorflow.framework.config.ThreadPoolOptionProto]): ConfigProto = copy(sessionInterOpThreadPool = __v)
    def withPlacementPeriod(__v: _root_.scala.Int): ConfigProto = copy(placementPeriod = __v)
    def clearDeviceFilters = copy(deviceFilters = _root_.scala.Seq.empty)
    def addDeviceFilters(__vs: _root_.scala.Predef.String *): ConfigProto = addAllDeviceFilters(__vs)
    def addAllDeviceFilters(__vs: Iterable[_root_.scala.Predef.String]): ConfigProto = copy(deviceFilters = deviceFilters ++ __vs)
    def withDeviceFilters(__v: _root_.scala.Seq[_root_.scala.Predef.String]): ConfigProto = copy(deviceFilters = __v)
    def getGpuOptions: org.tensorflow.framework.config.GPUOptions = gpuOptions.getOrElse(org.tensorflow.framework.config.GPUOptions.defaultInstance)
    def clearGpuOptions: ConfigProto = copy(gpuOptions = _root_.scala.None)
    def withGpuOptions(__v: org.tensorflow.framework.config.GPUOptions): ConfigProto = copy(gpuOptions = Option(__v))
    def getPluggableDeviceOptions: org.tensorflow.framework.config.GPUOptions = pluggableDeviceOptions.getOrElse(org.tensorflow.framework.config.GPUOptions.defaultInstance)
    def clearPluggableDeviceOptions: ConfigProto = copy(pluggableDeviceOptions = _root_.scala.None)
    def withPluggableDeviceOptions(__v: org.tensorflow.framework.config.GPUOptions): ConfigProto = copy(pluggableDeviceOptions = Option(__v))
    def withAllowSoftPlacement(__v: _root_.scala.Boolean): ConfigProto = copy(allowSoftPlacement = __v)
    def withLogDevicePlacement(__v: _root_.scala.Boolean): ConfigProto = copy(logDevicePlacement = __v)
    def getGraphOptions: org.tensorflow.framework.config.GraphOptions = graphOptions.getOrElse(org.tensorflow.framework.config.GraphOptions.defaultInstance)
    def clearGraphOptions: ConfigProto = copy(graphOptions = _root_.scala.None)
    def withGraphOptions(__v: org.tensorflow.framework.config.GraphOptions): ConfigProto = copy(graphOptions = Option(__v))
    def withOperationTimeoutInMs(__v: _root_.scala.Long): ConfigProto = copy(operationTimeoutInMs = __v)
    def getRpcOptions: tensorboard.rpc_options.RPCOptions = rpcOptions.getOrElse(tensorboard.rpc_options.RPCOptions.defaultInstance)
    def clearRpcOptions: ConfigProto = copy(rpcOptions = _root_.scala.None)
    def withRpcOptions(__v: tensorboard.rpc_options.RPCOptions): ConfigProto = copy(rpcOptions = Option(__v))
    def getClusterDef: org.tensorflow.distruntime.cluster.ClusterDef = clusterDef.getOrElse(org.tensorflow.distruntime.cluster.ClusterDef.defaultInstance)
    def clearClusterDef: ConfigProto = copy(clusterDef = _root_.scala.None)
    def withClusterDef(__v: org.tensorflow.distruntime.cluster.ClusterDef): ConfigProto = copy(clusterDef = Option(__v))
    def withIsolateSessionState(__v: _root_.scala.Boolean): ConfigProto = copy(isolateSessionState = __v)
    def withShareClusterDevicesInSession(__v: _root_.scala.Boolean): ConfigProto = copy(shareClusterDevicesInSession = __v)
    def getExperimental: org.tensorflow.framework.config.ConfigProto.Experimental = experimental.getOrElse(org.tensorflow.framework.config.ConfigProto.Experimental.defaultInstance)
    def clearExperimental: ConfigProto = copy(experimental = _root_.scala.None)
    def withExperimental(__v: org.tensorflow.framework.config.ConfigProto.Experimental): ConfigProto = copy(experimental = Option(__v))
    def withUnknownFields(__v: _root_.scalapb.UnknownFieldSet) = copy(unknownFields = __v)
    def discardUnknownFields = copy(unknownFields = _root_.scalapb.UnknownFieldSet.empty)
    def getFieldByNumber(__fieldNumber: _root_.scala.Int): _root_.scala.Any = {
      (__fieldNumber: @_root_.scala.unchecked) match {
        case 1 => deviceCount.iterator.map(org.tensorflow.framework.config.ConfigProto._typemapper_deviceCount.toBase(_)).toSeq
        case 2 => {
          val __t = intraOpParallelismThreads
          if (__t != 0) __t else null
        }
        case 5 => {
          val __t = interOpParallelismThreads
          if (__t != 0) __t else null
        }
        case 9 => {
          val __t = usePerSessionThreads
          if (__t != false) __t else null
        }
        case 12 => sessionInterOpThreadPool
        case 3 => {
          val __t = placementPeriod
          if (__t != 0) __t else null
        }
        case 4 => deviceFilters
        case 6 => gpuOptions.orNull
        case 18 => pluggableDeviceOptions.orNull
        case 7 => {
          val __t = allowSoftPlacement
          if (__t != false) __t else null
        }
        case 8 => {
          val __t = logDevicePlacement
          if (__t != false) __t else null
        }
        case 10 => graphOptions.orNull
        case 11 => {
          val __t = operationTimeoutInMs
          if (__t != 0L) __t else null
        }
        case 13 => rpcOptions.orNull
        case 14 => clusterDef.orNull
        case 15 => {
          val __t = isolateSessionState
          if (__t != false) __t else null
        }
        case 17 => {
          val __t = shareClusterDevicesInSession
          if (__t != false) __t else null
        }
        case 16 => experimental.orNull
      }
    }
    def getField(__field: _root_.scalapb.descriptors.FieldDescriptor): _root_.scalapb.descriptors.PValue = {
      _root_.scala.Predef.require(__field.containingMessage eq companion.scalaDescriptor)
      (__field.number: @_root_.scala.unchecked) match {
        case 1 => _root_.scalapb.descriptors.PRepeated(deviceCount.iterator.map(org.tensorflow.framework.config.ConfigProto._typemapper_deviceCount.toBase(_).toPMessage).toVector)
        case 2 => _root_.scalapb.descriptors.PInt(intraOpParallelismThreads)
        case 5 => _root_.scalapb.descriptors.PInt(interOpParallelismThreads)
        case 9 => _root_.scalapb.descriptors.PBoolean(usePerSessionThreads)
        case 12 => _root_.scalapb.descriptors.PRepeated(sessionInterOpThreadPool.iterator.map(_.toPMessage).toVector)
        case 3 => _root_.scalapb.descriptors.PInt(placementPeriod)
        case 4 => _root_.scalapb.descriptors.PRepeated(deviceFilters.iterator.map(_root_.scalapb.descriptors.PString(_)).toVector)
        case 6 => gpuOptions.map(_.toPMessage).getOrElse(_root_.scalapb.descriptors.PEmpty)
        case 18 => pluggableDeviceOptions.map(_.toPMessage).getOrElse(_root_.scalapb.descriptors.PEmpty)
        case 7 => _root_.scalapb.descriptors.PBoolean(allowSoftPlacement)
        case 8 => _root_.scalapb.descriptors.PBoolean(logDevicePlacement)
        case 10 => graphOptions.map(_.toPMessage).getOrElse(_root_.scalapb.descriptors.PEmpty)
        case 11 => _root_.scalapb.descriptors.PLong(operationTimeoutInMs)
        case 13 => rpcOptions.map(_.toPMessage).getOrElse(_root_.scalapb.descriptors.PEmpty)
        case 14 => clusterDef.map(_.toPMessage).getOrElse(_root_.scalapb.descriptors.PEmpty)
        case 15 => _root_.scalapb.descriptors.PBoolean(isolateSessionState)
        case 17 => _root_.scalapb.descriptors.PBoolean(shareClusterDevicesInSession)
        case 16 => experimental.map(_.toPMessage).getOrElse(_root_.scalapb.descriptors.PEmpty)
      }
    }
    def toProtoString: _root_.scala.Predef.String = _root_.scalapb.TextFormat.printToUnicodeString(this)
    def companion: org.tensorflow.framework.config.ConfigProto.type = org.tensorflow.framework.config.ConfigProto
    // @@protoc_insertion_point(GeneratedMessage[tensorboard.ConfigProto])
}

object ConfigProto extends scalapb.GeneratedMessageCompanion[org.tensorflow.framework.config.ConfigProto] {
  implicit def messageCompanion: scalapb.GeneratedMessageCompanion[org.tensorflow.framework.config.ConfigProto] = this
  def parseFrom(`_input__`: _root_.com.google.protobuf.CodedInputStream): org.tensorflow.framework.config.ConfigProto = {
    val __deviceCount: _root_.scala.collection.mutable.Builder[(_root_.scala.Predef.String, _root_.scala.Int), _root_.scala.collection.immutable.Map[_root_.scala.Predef.String, _root_.scala.Int]] = _root_.scala.collection.immutable.Map.newBuilder[_root_.scala.Predef.String, _root_.scala.Int]
    var __intraOpParallelismThreads: _root_.scala.Int = 0
    var __interOpParallelismThreads: _root_.scala.Int = 0
    var __usePerSessionThreads: _root_.scala.Boolean = false
    val __sessionInterOpThreadPool: _root_.scala.collection.immutable.VectorBuilder[org.tensorflow.framework.config.ThreadPoolOptionProto] = new _root_.scala.collection.immutable.VectorBuilder[org.tensorflow.framework.config.ThreadPoolOptionProto]
    var __placementPeriod: _root_.scala.Int = 0
    val __deviceFilters: _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Predef.String] = new _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Predef.String]
    var __gpuOptions: _root_.scala.Option[org.tensorflow.framework.config.GPUOptions] = _root_.scala.None
    var __pluggableDeviceOptions: _root_.scala.Option[org.tensorflow.framework.config.GPUOptions] = _root_.scala.None
    var __allowSoftPlacement: _root_.scala.Boolean = false
    var __logDevicePlacement: _root_.scala.Boolean = false
    var __graphOptions: _root_.scala.Option[org.tensorflow.framework.config.GraphOptions] = _root_.scala.None
    var __operationTimeoutInMs: _root_.scala.Long = 0L
    var __rpcOptions: _root_.scala.Option[tensorboard.rpc_options.RPCOptions] = _root_.scala.None
    var __clusterDef: _root_.scala.Option[org.tensorflow.distruntime.cluster.ClusterDef] = _root_.scala.None
    var __isolateSessionState: _root_.scala.Boolean = false
    var __shareClusterDevicesInSession: _root_.scala.Boolean = false
    var __experimental: _root_.scala.Option[org.tensorflow.framework.config.ConfigProto.Experimental] = _root_.scala.None
    var `_unknownFields__`: _root_.scalapb.UnknownFieldSet.Builder = null
    var _done__ = false
    while (!_done__) {
      val _tag__ = _input__.readTag()
      _tag__ match {
        case 0 => _done__ = true
        case 10 =>
          __deviceCount += org.tensorflow.framework.config.ConfigProto._typemapper_deviceCount.toCustom(_root_.scalapb.LiteParser.readMessage[org.tensorflow.framework.config.ConfigProto.DeviceCountEntry](_input__))
        case 16 =>
          __intraOpParallelismThreads = _input__.readInt32()
        case 40 =>
          __interOpParallelismThreads = _input__.readInt32()
        case 72 =>
          __usePerSessionThreads = _input__.readBool()
        case 98 =>
          __sessionInterOpThreadPool += _root_.scalapb.LiteParser.readMessage[org.tensorflow.framework.config.ThreadPoolOptionProto](_input__)
        case 24 =>
          __placementPeriod = _input__.readInt32()
        case 34 =>
          __deviceFilters += _input__.readStringRequireUtf8()
        case 50 =>
          __gpuOptions = _root_.scala.Option(__gpuOptions.fold(_root_.scalapb.LiteParser.readMessage[org.tensorflow.framework.config.GPUOptions](_input__))(_root_.scalapb.LiteParser.readMessage(_input__, _)))
        case 146 =>
          __pluggableDeviceOptions = _root_.scala.Option(__pluggableDeviceOptions.fold(_root_.scalapb.LiteParser.readMessage[org.tensorflow.framework.config.GPUOptions](_input__))(_root_.scalapb.LiteParser.readMessage(_input__, _)))
        case 56 =>
          __allowSoftPlacement = _input__.readBool()
        case 64 =>
          __logDevicePlacement = _input__.readBool()
        case 82 =>
          __graphOptions = _root_.scala.Option(__graphOptions.fold(_root_.scalapb.LiteParser.readMessage[org.tensorflow.framework.config.GraphOptions](_input__))(_root_.scalapb.LiteParser.readMessage(_input__, _)))
        case 88 =>
          __operationTimeoutInMs = _input__.readInt64()
        case 106 =>
          __rpcOptions = _root_.scala.Option(__rpcOptions.fold(_root_.scalapb.LiteParser.readMessage[tensorboard.rpc_options.RPCOptions](_input__))(_root_.scalapb.LiteParser.readMessage(_input__, _)))
        case 114 =>
          __clusterDef = _root_.scala.Option(__clusterDef.fold(_root_.scalapb.LiteParser.readMessage[org.tensorflow.distruntime.cluster.ClusterDef](_input__))(_root_.scalapb.LiteParser.readMessage(_input__, _)))
        case 120 =>
          __isolateSessionState = _input__.readBool()
        case 136 =>
          __shareClusterDevicesInSession = _input__.readBool()
        case 130 =>
          __experimental = _root_.scala.Option(__experimental.fold(_root_.scalapb.LiteParser.readMessage[org.tensorflow.framework.config.ConfigProto.Experimental](_input__))(_root_.scalapb.LiteParser.readMessage(_input__, _)))
        case tag =>
          if (_unknownFields__ == null) {
            _unknownFields__ = new _root_.scalapb.UnknownFieldSet.Builder()
          }
          _unknownFields__.parseField(tag, _input__)
      }
    }
    org.tensorflow.framework.config.ConfigProto(
        deviceCount = __deviceCount.result(),
        intraOpParallelismThreads = __intraOpParallelismThreads,
        interOpParallelismThreads = __interOpParallelismThreads,
        usePerSessionThreads = __usePerSessionThreads,
        sessionInterOpThreadPool = __sessionInterOpThreadPool.result(),
        placementPeriod = __placementPeriod,
        deviceFilters = __deviceFilters.result(),
        gpuOptions = __gpuOptions,
        pluggableDeviceOptions = __pluggableDeviceOptions,
        allowSoftPlacement = __allowSoftPlacement,
        logDevicePlacement = __logDevicePlacement,
        graphOptions = __graphOptions,
        operationTimeoutInMs = __operationTimeoutInMs,
        rpcOptions = __rpcOptions,
        clusterDef = __clusterDef,
        isolateSessionState = __isolateSessionState,
        shareClusterDevicesInSession = __shareClusterDevicesInSession,
        experimental = __experimental,
        unknownFields = if (_unknownFields__ == null) _root_.scalapb.UnknownFieldSet.empty else _unknownFields__.result()
    )
  }
  implicit def messageReads: _root_.scalapb.descriptors.Reads[org.tensorflow.framework.config.ConfigProto] = _root_.scalapb.descriptors.Reads{
    case _root_.scalapb.descriptors.PMessage(__fieldsMap) =>
      _root_.scala.Predef.require(__fieldsMap.keys.forall(_.containingMessage eq scalaDescriptor), "FieldDescriptor does not match message type.")
      org.tensorflow.framework.config.ConfigProto(
        deviceCount = __fieldsMap.get(scalaDescriptor.findFieldByNumber(1).get).map(_.as[_root_.scala.Seq[org.tensorflow.framework.config.ConfigProto.DeviceCountEntry]]).getOrElse(_root_.scala.Seq.empty).iterator.map(org.tensorflow.framework.config.ConfigProto._typemapper_deviceCount.toCustom(_)).toMap,
        intraOpParallelismThreads = __fieldsMap.get(scalaDescriptor.findFieldByNumber(2).get).map(_.as[_root_.scala.Int]).getOrElse(0),
        interOpParallelismThreads = __fieldsMap.get(scalaDescriptor.findFieldByNumber(5).get).map(_.as[_root_.scala.Int]).getOrElse(0),
        usePerSessionThreads = __fieldsMap.get(scalaDescriptor.findFieldByNumber(9).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
        sessionInterOpThreadPool = __fieldsMap.get(scalaDescriptor.findFieldByNumber(12).get).map(_.as[_root_.scala.Seq[org.tensorflow.framework.config.ThreadPoolOptionProto]]).getOrElse(_root_.scala.Seq.empty),
        placementPeriod = __fieldsMap.get(scalaDescriptor.findFieldByNumber(3).get).map(_.as[_root_.scala.Int]).getOrElse(0),
        deviceFilters = __fieldsMap.get(scalaDescriptor.findFieldByNumber(4).get).map(_.as[_root_.scala.Seq[_root_.scala.Predef.String]]).getOrElse(_root_.scala.Seq.empty),
        gpuOptions = __fieldsMap.get(scalaDescriptor.findFieldByNumber(6).get).flatMap(_.as[_root_.scala.Option[org.tensorflow.framework.config.GPUOptions]]),
        pluggableDeviceOptions = __fieldsMap.get(scalaDescriptor.findFieldByNumber(18).get).flatMap(_.as[_root_.scala.Option[org.tensorflow.framework.config.GPUOptions]]),
        allowSoftPlacement = __fieldsMap.get(scalaDescriptor.findFieldByNumber(7).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
        logDevicePlacement = __fieldsMap.get(scalaDescriptor.findFieldByNumber(8).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
        graphOptions = __fieldsMap.get(scalaDescriptor.findFieldByNumber(10).get).flatMap(_.as[_root_.scala.Option[org.tensorflow.framework.config.GraphOptions]]),
        operationTimeoutInMs = __fieldsMap.get(scalaDescriptor.findFieldByNumber(11).get).map(_.as[_root_.scala.Long]).getOrElse(0L),
        rpcOptions = __fieldsMap.get(scalaDescriptor.findFieldByNumber(13).get).flatMap(_.as[_root_.scala.Option[tensorboard.rpc_options.RPCOptions]]),
        clusterDef = __fieldsMap.get(scalaDescriptor.findFieldByNumber(14).get).flatMap(_.as[_root_.scala.Option[org.tensorflow.distruntime.cluster.ClusterDef]]),
        isolateSessionState = __fieldsMap.get(scalaDescriptor.findFieldByNumber(15).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
        shareClusterDevicesInSession = __fieldsMap.get(scalaDescriptor.findFieldByNumber(17).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
        experimental = __fieldsMap.get(scalaDescriptor.findFieldByNumber(16).get).flatMap(_.as[_root_.scala.Option[org.tensorflow.framework.config.ConfigProto.Experimental]])
      )
    case _ => throw new RuntimeException("Expected PMessage")
  }
  def javaDescriptor: _root_.com.google.protobuf.Descriptors.Descriptor = org.tensorflow.framework.config.ConfigProtoCompanion.javaDescriptor.getMessageTypes().get(5)
  def scalaDescriptor: _root_.scalapb.descriptors.Descriptor = org.tensorflow.framework.config.ConfigProtoCompanion.scalaDescriptor.messages(5)
  def messageCompanionForFieldNumber(__number: _root_.scala.Int): _root_.scalapb.GeneratedMessageCompanion[?]= {
    var __out: _root_.scalapb.GeneratedMessageCompanion[?]= null
    (__number: @_root_.scala.unchecked) match {
      case 1 => __out = org.tensorflow.framework.config.ConfigProto.DeviceCountEntry
      case 12 => __out = org.tensorflow.framework.config.ThreadPoolOptionProto
      case 6 => __out = org.tensorflow.framework.config.GPUOptions
      case 18 => __out = org.tensorflow.framework.config.GPUOptions
      case 10 => __out = org.tensorflow.framework.config.GraphOptions
      case 13 => __out = tensorboard.rpc_options.RPCOptions
      case 14 => __out = org.tensorflow.distruntime.cluster.ClusterDef
      case 16 => __out = org.tensorflow.framework.config.ConfigProto.Experimental
    }
    __out
  }
  lazy val nestedMessagesCompanions: Seq[_root_.scalapb.GeneratedMessageCompanion[? <: _root_.scalapb.GeneratedMessage]] =
    Seq[_root_.scalapb.GeneratedMessageCompanion[? <: _root_.scalapb.GeneratedMessage]](
      _root_.org.tensorflow.framework.config.ConfigProto.DeviceCountEntry,
      _root_.org.tensorflow.framework.config.ConfigProto.Experimental
    )
  def enumCompanionForFieldNumber(__fieldNumber: _root_.scala.Int): _root_.scalapb.GeneratedEnumCompanion[?]= throw new MatchError(__fieldNumber)
  lazy val defaultInstance = org.tensorflow.framework.config.ConfigProto(
    deviceCount = _root_.scala.collection.immutable.Map.empty,
    intraOpParallelismThreads = 0,
    interOpParallelismThreads = 0,
    usePerSessionThreads = false,
    sessionInterOpThreadPool = _root_.scala.Seq.empty,
    placementPeriod = 0,
    deviceFilters = _root_.scala.Seq.empty,
    gpuOptions = _root_.scala.None,
    pluggableDeviceOptions = _root_.scala.None,
    allowSoftPlacement = false,
    logDevicePlacement = false,
    graphOptions = _root_.scala.None,
    operationTimeoutInMs = 0L,
    rpcOptions = _root_.scala.None,
    clusterDef = _root_.scala.None,
    isolateSessionState = false,
    shareClusterDevicesInSession = false,
    experimental = _root_.scala.None
  )
  @SerialVersionUID(0L)
  final case class DeviceCountEntry(
      key: _root_.scala.Predef.String = "",
      value: _root_.scala.Int = 0,
      unknownFields: _root_.scalapb.UnknownFieldSet = _root_.scalapb.UnknownFieldSet.empty
      ) extends scalapb.GeneratedMessage with scalapb.lenses.Updatable[DeviceCountEntry] {
      @transient
      private[this] var __serializedSizeMemoized: _root_.scala.Int = 0
      private[this] def __computeSerializedSize(): _root_.scala.Int = {
        var __size = 0
        
        {
          val __value = key
          if (!__value.isEmpty) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(1, __value)
          }
        };
        
        {
          val __value = value
          if (__value != 0) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeInt32Size(2, __value)
          }
        };
        __size += unknownFields.serializedSize
        __size
      }
      override def serializedSize: _root_.scala.Int = {
        var __size = __serializedSizeMemoized
        if (__size == 0) {
          __size = __computeSerializedSize() + 1
          __serializedSizeMemoized = __size
        }
        __size - 1
        
      }
      def writeTo(`_output__`: _root_.com.google.protobuf.CodedOutputStream): _root_.scala.Unit = {
        {
          val __v = key
          if (!__v.isEmpty) {
            _output__.writeString(1, __v)
          }
        };
        {
          val __v = value
          if (__v != 0) {
            _output__.writeInt32(2, __v)
          }
        };
        unknownFields.writeTo(_output__)
      }
      def withKey(__v: _root_.scala.Predef.String): DeviceCountEntry = copy(key = __v)
      def withValue(__v: _root_.scala.Int): DeviceCountEntry = copy(value = __v)
      def withUnknownFields(__v: _root_.scalapb.UnknownFieldSet) = copy(unknownFields = __v)
      def discardUnknownFields = copy(unknownFields = _root_.scalapb.UnknownFieldSet.empty)
      def getFieldByNumber(__fieldNumber: _root_.scala.Int): _root_.scala.Any = {
        (__fieldNumber: @_root_.scala.unchecked) match {
          case 1 => {
            val __t = key
            if (__t != "") __t else null
          }
          case 2 => {
            val __t = value
            if (__t != 0) __t else null
          }
        }
      }
      def getField(__field: _root_.scalapb.descriptors.FieldDescriptor): _root_.scalapb.descriptors.PValue = {
        _root_.scala.Predef.require(__field.containingMessage eq companion.scalaDescriptor)
        (__field.number: @_root_.scala.unchecked) match {
          case 1 => _root_.scalapb.descriptors.PString(key)
          case 2 => _root_.scalapb.descriptors.PInt(value)
        }
      }
      def toProtoString: _root_.scala.Predef.String = _root_.scalapb.TextFormat.printToUnicodeString(this)
      def companion: org.tensorflow.framework.config.ConfigProto.DeviceCountEntry.type = org.tensorflow.framework.config.ConfigProto.DeviceCountEntry
      // @@protoc_insertion_point(GeneratedMessage[tensorboard.ConfigProto.DeviceCountEntry])
  }
  
  object DeviceCountEntry extends scalapb.GeneratedMessageCompanion[org.tensorflow.framework.config.ConfigProto.DeviceCountEntry] {
    implicit def messageCompanion: scalapb.GeneratedMessageCompanion[org.tensorflow.framework.config.ConfigProto.DeviceCountEntry] = this
    def parseFrom(`_input__`: _root_.com.google.protobuf.CodedInputStream): org.tensorflow.framework.config.ConfigProto.DeviceCountEntry = {
      var __key: _root_.scala.Predef.String = ""
      var __value: _root_.scala.Int = 0
      var `_unknownFields__`: _root_.scalapb.UnknownFieldSet.Builder = null
      var _done__ = false
      while (!_done__) {
        val _tag__ = _input__.readTag()
        _tag__ match {
          case 0 => _done__ = true
          case 10 =>
            __key = _input__.readStringRequireUtf8()
          case 16 =>
            __value = _input__.readInt32()
          case tag =>
            if (_unknownFields__ == null) {
              _unknownFields__ = new _root_.scalapb.UnknownFieldSet.Builder()
            }
            _unknownFields__.parseField(tag, _input__)
        }
      }
      org.tensorflow.framework.config.ConfigProto.DeviceCountEntry(
          key = __key,
          value = __value,
          unknownFields = if (_unknownFields__ == null) _root_.scalapb.UnknownFieldSet.empty else _unknownFields__.result()
      )
    }
    implicit def messageReads: _root_.scalapb.descriptors.Reads[org.tensorflow.framework.config.ConfigProto.DeviceCountEntry] = _root_.scalapb.descriptors.Reads{
      case _root_.scalapb.descriptors.PMessage(__fieldsMap) =>
        _root_.scala.Predef.require(__fieldsMap.keys.forall(_.containingMessage eq scalaDescriptor), "FieldDescriptor does not match message type.")
        org.tensorflow.framework.config.ConfigProto.DeviceCountEntry(
          key = __fieldsMap.get(scalaDescriptor.findFieldByNumber(1).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
          value = __fieldsMap.get(scalaDescriptor.findFieldByNumber(2).get).map(_.as[_root_.scala.Int]).getOrElse(0)
        )
      case _ => throw new RuntimeException("Expected PMessage")
    }
    def javaDescriptor: _root_.com.google.protobuf.Descriptors.Descriptor = org.tensorflow.framework.config.ConfigProto.javaDescriptor.getNestedTypes().get(0)
    def scalaDescriptor: _root_.scalapb.descriptors.Descriptor = org.tensorflow.framework.config.ConfigProto.scalaDescriptor.nestedMessages(0)
    def messageCompanionForFieldNumber(__number: _root_.scala.Int): _root_.scalapb.GeneratedMessageCompanion[?]= throw new MatchError(__number)
    lazy val nestedMessagesCompanions: Seq[_root_.scalapb.GeneratedMessageCompanion[? <: _root_.scalapb.GeneratedMessage]] = Seq.empty
    def enumCompanionForFieldNumber(__fieldNumber: _root_.scala.Int): _root_.scalapb.GeneratedEnumCompanion[?]= throw new MatchError(__fieldNumber)
    lazy val defaultInstance = org.tensorflow.framework.config.ConfigProto.DeviceCountEntry(
      key = "",
      value = 0
    )
    implicit class DeviceCountEntryLens[UpperPB](_l: _root_.scalapb.lenses.Lens[UpperPB, org.tensorflow.framework.config.ConfigProto.DeviceCountEntry]) extends _root_.scalapb.lenses.ObjectLens[UpperPB, org.tensorflow.framework.config.ConfigProto.DeviceCountEntry](_l) {
      def key: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.key)((c_, f_) => c_.copy(key = f_))
      def value: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Int] = field(_.value)((c_, f_) => c_.copy(value = f_))
    }
    final val KEY_FIELD_NUMBER = 1
    final val VALUE_FIELD_NUMBER = 2
    @transient
    implicit val keyValueMapper: _root_.scalapb.TypeMapper[org.tensorflow.framework.config.ConfigProto.DeviceCountEntry, (_root_.scala.Predef.String, _root_.scala.Int)] =
      _root_.scalapb.TypeMapper[org.tensorflow.framework.config.ConfigProto.DeviceCountEntry, (_root_.scala.Predef.String, _root_.scala.Int)](__m => (__m.key, __m.value))(__p => org.tensorflow.framework.config.ConfigProto.DeviceCountEntry(__p._1, __p._2))
    def of(
      key: _root_.scala.Predef.String,
      value: _root_.scala.Int
    ): _root_.org.tensorflow.framework.config.ConfigProto.DeviceCountEntry = _root_.org.tensorflow.framework.config.ConfigProto.DeviceCountEntry(
      key,
      value
    )
    // @@protoc_insertion_point(GeneratedMessageCompanion[tensorboard.ConfigProto.DeviceCountEntry])
  }
  
  /** Everything inside Experimental is subject to change and is not subject
    * to API stability guarantees in
    * https://www.tensorflow.org/guide/versions.
    *
    * @param collectiveGroupLeader
    *   Task name for group resolution.
    * @param executorType
    *   Which executor to use, the default executor will be used
    *   if it is an empty string or "DEFAULT"
    * @param recvBufMaxChunk
    *   Guidance to formatting of large RecvBuf fields for transfer.
    *   Any positive value sets the max chunk size.  0 defaults to 4096.
    *   Any negative value indicates no max, i.e. one chunk only.
    * @param useNumaAffinity
    *   If true, and supported by the platform, the runtime will attempt to
    *   use NUMA affinity where applicable.  One consequence will be the
    *   existence of as many CPU devices as there are available NUMA nodes.
    * @param collectiveDeterministicSequentialExecution
    *   If true, make collective op execution order sequential and deterministic
    *   for potentially concurrent collective instances.
    * @param collectiveNccl
    *   If true, use NCCL for CollectiveOps.  This feature is highly
    *   experimental.
    * @param shareSessionStateInClusterspecPropagation
    *   In the following, session state means the value of a variable, elements
    *   in a hash table, or any other resource, accessible by worker sessions
    *   held by a TF server.
    *  
    *   When ClusterSpec propagation is enabled, the value of
    *   isolate_session_state is ignored when deciding whether to share session
    *   states in a TF server (for backwards compatibility reasons).
    *   - If share_session_state_in_clusterspec_propagation is true, the session
    *   states are shared.
    *   - If share_session_state_in_clusterspec_propagation is false, session
    *   states are isolated.
    *  
    *   When clusterspec propagation is not used, the value of
    *   share_session_state_in_clusterspec_propagation is ignored when deciding
    *   whether to share session states in a TF server.
    *   - If isolate_session_state is true, session states are isolated.
    *   - If isolate_session_state is false, session states are shared.
    *  
    *   TODO(b/129330037): Add a single API that consistently treats
    *   isolate_session_state and ClusterSpec propagation.
    * @param disableThreadSpinning
    *   If using a direct session, disable spinning while waiting for work in
    *   the thread pool. This may result in higher latency for completing ops,
    *   but in the case where there is a lot of spinning may result in lower
    *   CPU usage.
    * @param shareClusterDevicesInSession
    *   This was promoted to a non-experimental API. Please use
    *   ConfigProto.share_cluster_devices_in_session instead.
    * @param sessionMetadata
    *   Metadata about the session.
    *  
    *   If set, this can be used by the runtime and the Ops for debugging,
    *   monitoring, etc.
    *  
    *   NOTE: This is currently used and propagated only by the direct session
    *   and EagerContext.
    * @param optimizeForStaticGraph
    *   If true, the session may treat the graph as being static for optimization
    *   purposes.
    *  
    *   If this option is set to true when a session is created, the full
    *   GraphDef must be passed in a single call to Session::Create(), and
    *   Session::Extend() may not be supported.
    * @param enableMlirBridge
    *   Whether to enable the MLIR-based TF-&gt;XLA bridge. This is only used if set
    *   to true. Default value or false is ignored. Use mlir_bridge_rollout for
    *   finer control.
    *  
    *   If this option is set to true when a session is created, MLIR is used to
    *   perform the set of graph transformations to put the graph in a form that
    *   can be executed with delegation of some computations to an accelerator.
    *   This builds on the model of XLA where a subset of the graph is
    *   encapsulated and attached to a "compile" operation, whose result is fed
    *   to an "execute" operation. The kernel for these operations is responsible
    *   to lower the encapsulated graph to a particular device.
    * @param mlirBridgeRollout
    *   Whether to enable the MLIR-based TF-&gt;XLA bridge.
    * @param enableMlirGraphOptimization
    *   Whether to enable the MLIR-based Graph optimizations.
    *  
    *   This will become a part of standard Tensorflow graph optimization
    *   pipeline, currently this is only used for gradual migration and testing
    *   new passes that are replacing existing optimizations in Grappler.
    * @param disableOutputPartitionGraphs
    *   If true, the session will not store an additional copy of the graph for
    *   each subgraph.
    *  
    *   If this option is set to true when a session is created, the
    *   `RunOptions.output_partition_graphs` options must not be set.
    * @param xlaFusionAutotunerThresh
    *   Minimum number of batches run through the XLA graph before XLA fusion
    *   autotuner is enabled. Default value of zero disables the autotuner.
    *  
    *   The XLA fusion autotuner can improve performance by executing a heuristic
    *   search on the compiler parameters.
    * @param useTfrt
    *   Whether runtime execution uses TFRT.
    * @param enableMultiHost
    *   If true, use Pathways with TFRT API for multi host support.
    * @param tfrtUseIfrt
    *   If true, use ifrt as the backend for TFRT. This is only used when
    *   `use_tfrt` is true.
    * @param backendServerPort
    *   Port for the Pathways server. Ignored if enable_multi_host=false.
    * @param targetTpu
    *   If true, TFRT will use TPU specific compiler passes and perform TPU
    *   specific initialization.
    * @param targetGpu
    *   If true, TFRT will use GPU specific compiler passes and perform GPU
    *   specific initialization.
    * @param streamMergeThreshold
    *   The threshold to merge small streams in TFRT. The stream with cost
    *   smaller than the threshold will be merged. Setting it to value 1
    *   disables all merges.
    * @param disableFunctionalOpsLowering
    *   Whether functional control flow op lowering should be disabled. This is
    *   useful when executing within a portable runtime where control flow op
    *   kernels may not be loaded due to selective registration.
    * @param xlaPreferSingleGraphCluster
    *   Provides a hint to XLA auto clustering to prefer forming a single large
    *   cluster that encompasses most of the graph.
    * @param coordinationConfig
    *   Distributed coordination service configurations.
    * @param disableOptimizeForStaticGraph
    *   If true, the session will treat the graph as being non-static for
    *   optimization purposes.
    *  
    *   If this option is set to true when a session is created, the full
    *   GraphDef will be retained to enable calls to Session::Extend().
    *   Calling Extend() without setting this flag will result in errors.
    *  
    *   This option is meant to replace `optimize_for_static_graph` and it
    *   aims to negate its value.
    * @param disableEagerExecutorStreamingEnqueue
    *   Whether eager remote execution will stream all the function calls or
    *   allow them to happen in parallel. When true, streaming execution is
    *   disabled, and parallel execution is allowed.
    */
  @SerialVersionUID(0L)
  final case class Experimental(
      collectiveGroupLeader: _root_.scala.Predef.String = "",
      executorType: _root_.scala.Predef.String = "",
      recvBufMaxChunk: _root_.scala.Int = 0,
      useNumaAffinity: _root_.scala.Boolean = false,
      collectiveDeterministicSequentialExecution: _root_.scala.Boolean = false,
      collectiveNccl: _root_.scala.Boolean = false,
      shareSessionStateInClusterspecPropagation: _root_.scala.Boolean = false,
      disableThreadSpinning: _root_.scala.Boolean = false,
      shareClusterDevicesInSession: _root_.scala.Boolean = false,
      sessionMetadata: _root_.scala.Option[org.tensorflow.framework.config.SessionMetadata] = _root_.scala.None,
      optimizeForStaticGraph: _root_.scala.Boolean = false,
      enableMlirBridge: _root_.scala.Boolean = false,
      mlirBridgeRollout: org.tensorflow.framework.config.ConfigProto.Experimental.MlirBridgeRollout = org.tensorflow.framework.config.ConfigProto.Experimental.MlirBridgeRollout.MLIR_BRIDGE_ROLLOUT_UNSPECIFIED,
      enableMlirGraphOptimization: _root_.scala.Boolean = false,
      disableOutputPartitionGraphs: _root_.scala.Boolean = false,
      xlaFusionAutotunerThresh: _root_.scala.Long = 0L,
      useTfrt: _root_.scala.Boolean = false,
      enableMultiHost: _root_.scala.Boolean = false,
      tfrtUseIfrt: _root_.scala.Boolean = false,
      backendServerPort: _root_.scala.Int = 0,
      targetTpu: _root_.scala.Boolean = false,
      targetGpu: _root_.scala.Boolean = false,
      streamMergeThreshold: _root_.scala.Int = 0,
      disableFunctionalOpsLowering: _root_.scala.Boolean = false,
      xlaPreferSingleGraphCluster: _root_.scala.Boolean = false,
      coordinationConfig: _root_.scala.Option[tensorboard.coordination_config.CoordinationServiceConfig] = _root_.scala.None,
      disableOptimizeForStaticGraph: _root_.scala.Boolean = false,
      disableEagerExecutorStreamingEnqueue: _root_.scala.Boolean = false,
      unknownFields: _root_.scalapb.UnknownFieldSet = _root_.scalapb.UnknownFieldSet.empty
      ) extends scalapb.GeneratedMessage with scalapb.lenses.Updatable[Experimental] {
      @transient
      private[this] var __serializedSizeMemoized: _root_.scala.Int = 0
      private[this] def __computeSerializedSize(): _root_.scala.Int = {
        var __size = 0
        
        {
          val __value = collectiveGroupLeader
          if (!__value.isEmpty) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(1, __value)
          }
        };
        
        {
          val __value = executorType
          if (!__value.isEmpty) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(3, __value)
          }
        };
        
        {
          val __value = recvBufMaxChunk
          if (__value != 0) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeInt32Size(4, __value)
          }
        };
        
        {
          val __value = useNumaAffinity
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(5, __value)
          }
        };
        
        {
          val __value = collectiveDeterministicSequentialExecution
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(6, __value)
          }
        };
        
        {
          val __value = collectiveNccl
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(7, __value)
          }
        };
        
        {
          val __value = shareSessionStateInClusterspecPropagation
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(8, __value)
          }
        };
        
        {
          val __value = disableThreadSpinning
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(9, __value)
          }
        };
        
        {
          val __value = shareClusterDevicesInSession
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(10, __value)
          }
        };
        if (sessionMetadata.isDefined) {
          val __value = sessionMetadata.get
          __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
        };
        
        {
          val __value = optimizeForStaticGraph
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(12, __value)
          }
        };
        
        {
          val __value = enableMlirBridge
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(13, __value)
          }
        };
        
        {
          val __value = mlirBridgeRollout.value
          if (__value != 0) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeEnumSize(17, __value)
          }
        };
        
        {
          val __value = enableMlirGraphOptimization
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(16, __value)
          }
        };
        
        {
          val __value = disableOutputPartitionGraphs
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(14, __value)
          }
        };
        
        {
          val __value = xlaFusionAutotunerThresh
          if (__value != 0L) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeInt64Size(15, __value)
          }
        };
        
        {
          val __value = useTfrt
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(18, __value)
          }
        };
        
        {
          val __value = enableMultiHost
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(27, __value)
          }
        };
        
        {
          val __value = tfrtUseIfrt
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(32, __value)
          }
        };
        
        {
          val __value = backendServerPort
          if (__value != 0) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeInt32Size(28, __value)
          }
        };
        
        {
          val __value = targetTpu
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(29, __value)
          }
        };
        
        {
          val __value = targetGpu
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(30, __value)
          }
        };
        
        {
          val __value = streamMergeThreshold
          if (__value != 0) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeInt32Size(31, __value)
          }
        };
        
        {
          val __value = disableFunctionalOpsLowering
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(21, __value)
          }
        };
        
        {
          val __value = xlaPreferSingleGraphCluster
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(22, __value)
          }
        };
        if (coordinationConfig.isDefined) {
          val __value = coordinationConfig.get
          __size += 2 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
        };
        
        {
          val __value = disableOptimizeForStaticGraph
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(24, __value)
          }
        };
        
        {
          val __value = disableEagerExecutorStreamingEnqueue
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(26, __value)
          }
        };
        __size += unknownFields.serializedSize
        __size
      }
      override def serializedSize: _root_.scala.Int = {
        var __size = __serializedSizeMemoized
        if (__size == 0) {
          __size = __computeSerializedSize() + 1
          __serializedSizeMemoized = __size
        }
        __size - 1
        
      }
      def writeTo(`_output__`: _root_.com.google.protobuf.CodedOutputStream): _root_.scala.Unit = {
        {
          val __v = collectiveGroupLeader
          if (!__v.isEmpty) {
            _output__.writeString(1, __v)
          }
        };
        {
          val __v = executorType
          if (!__v.isEmpty) {
            _output__.writeString(3, __v)
          }
        };
        {
          val __v = recvBufMaxChunk
          if (__v != 0) {
            _output__.writeInt32(4, __v)
          }
        };
        {
          val __v = useNumaAffinity
          if (__v != false) {
            _output__.writeBool(5, __v)
          }
        };
        {
          val __v = collectiveDeterministicSequentialExecution
          if (__v != false) {
            _output__.writeBool(6, __v)
          }
        };
        {
          val __v = collectiveNccl
          if (__v != false) {
            _output__.writeBool(7, __v)
          }
        };
        {
          val __v = shareSessionStateInClusterspecPropagation
          if (__v != false) {
            _output__.writeBool(8, __v)
          }
        };
        {
          val __v = disableThreadSpinning
          if (__v != false) {
            _output__.writeBool(9, __v)
          }
        };
        {
          val __v = shareClusterDevicesInSession
          if (__v != false) {
            _output__.writeBool(10, __v)
          }
        };
        sessionMetadata.foreach { __v =>
          val __m = __v
          _output__.writeTag(11, 2)
          _output__.writeUInt32NoTag(__m.serializedSize)
          __m.writeTo(_output__)
        };
        {
          val __v = optimizeForStaticGraph
          if (__v != false) {
            _output__.writeBool(12, __v)
          }
        };
        {
          val __v = enableMlirBridge
          if (__v != false) {
            _output__.writeBool(13, __v)
          }
        };
        {
          val __v = disableOutputPartitionGraphs
          if (__v != false) {
            _output__.writeBool(14, __v)
          }
        };
        {
          val __v = xlaFusionAutotunerThresh
          if (__v != 0L) {
            _output__.writeInt64(15, __v)
          }
        };
        {
          val __v = enableMlirGraphOptimization
          if (__v != false) {
            _output__.writeBool(16, __v)
          }
        };
        {
          val __v = mlirBridgeRollout.value
          if (__v != 0) {
            _output__.writeEnum(17, __v)
          }
        };
        {
          val __v = useTfrt
          if (__v != false) {
            _output__.writeBool(18, __v)
          }
        };
        {
          val __v = disableFunctionalOpsLowering
          if (__v != false) {
            _output__.writeBool(21, __v)
          }
        };
        {
          val __v = xlaPreferSingleGraphCluster
          if (__v != false) {
            _output__.writeBool(22, __v)
          }
        };
        coordinationConfig.foreach { __v =>
          val __m = __v
          _output__.writeTag(23, 2)
          _output__.writeUInt32NoTag(__m.serializedSize)
          __m.writeTo(_output__)
        };
        {
          val __v = disableOptimizeForStaticGraph
          if (__v != false) {
            _output__.writeBool(24, __v)
          }
        };
        {
          val __v = disableEagerExecutorStreamingEnqueue
          if (__v != false) {
            _output__.writeBool(26, __v)
          }
        };
        {
          val __v = enableMultiHost
          if (__v != false) {
            _output__.writeBool(27, __v)
          }
        };
        {
          val __v = backendServerPort
          if (__v != 0) {
            _output__.writeInt32(28, __v)
          }
        };
        {
          val __v = targetTpu
          if (__v != false) {
            _output__.writeBool(29, __v)
          }
        };
        {
          val __v = targetGpu
          if (__v != false) {
            _output__.writeBool(30, __v)
          }
        };
        {
          val __v = streamMergeThreshold
          if (__v != 0) {
            _output__.writeInt32(31, __v)
          }
        };
        {
          val __v = tfrtUseIfrt
          if (__v != false) {
            _output__.writeBool(32, __v)
          }
        };
        unknownFields.writeTo(_output__)
      }
      def withCollectiveGroupLeader(__v: _root_.scala.Predef.String): Experimental = copy(collectiveGroupLeader = __v)
      def withExecutorType(__v: _root_.scala.Predef.String): Experimental = copy(executorType = __v)
      def withRecvBufMaxChunk(__v: _root_.scala.Int): Experimental = copy(recvBufMaxChunk = __v)
      def withUseNumaAffinity(__v: _root_.scala.Boolean): Experimental = copy(useNumaAffinity = __v)
      def withCollectiveDeterministicSequentialExecution(__v: _root_.scala.Boolean): Experimental = copy(collectiveDeterministicSequentialExecution = __v)
      def withCollectiveNccl(__v: _root_.scala.Boolean): Experimental = copy(collectiveNccl = __v)
      def withShareSessionStateInClusterspecPropagation(__v: _root_.scala.Boolean): Experimental = copy(shareSessionStateInClusterspecPropagation = __v)
      def withDisableThreadSpinning(__v: _root_.scala.Boolean): Experimental = copy(disableThreadSpinning = __v)
      def withShareClusterDevicesInSession(__v: _root_.scala.Boolean): Experimental = copy(shareClusterDevicesInSession = __v)
      def getSessionMetadata: org.tensorflow.framework.config.SessionMetadata = sessionMetadata.getOrElse(org.tensorflow.framework.config.SessionMetadata.defaultInstance)
      def clearSessionMetadata: Experimental = copy(sessionMetadata = _root_.scala.None)
      def withSessionMetadata(__v: org.tensorflow.framework.config.SessionMetadata): Experimental = copy(sessionMetadata = Option(__v))
      def withOptimizeForStaticGraph(__v: _root_.scala.Boolean): Experimental = copy(optimizeForStaticGraph = __v)
      def withEnableMlirBridge(__v: _root_.scala.Boolean): Experimental = copy(enableMlirBridge = __v)
      def withMlirBridgeRollout(__v: org.tensorflow.framework.config.ConfigProto.Experimental.MlirBridgeRollout): Experimental = copy(mlirBridgeRollout = __v)
      def withEnableMlirGraphOptimization(__v: _root_.scala.Boolean): Experimental = copy(enableMlirGraphOptimization = __v)
      def withDisableOutputPartitionGraphs(__v: _root_.scala.Boolean): Experimental = copy(disableOutputPartitionGraphs = __v)
      def withXlaFusionAutotunerThresh(__v: _root_.scala.Long): Experimental = copy(xlaFusionAutotunerThresh = __v)
      def withUseTfrt(__v: _root_.scala.Boolean): Experimental = copy(useTfrt = __v)
      def withEnableMultiHost(__v: _root_.scala.Boolean): Experimental = copy(enableMultiHost = __v)
      def withTfrtUseIfrt(__v: _root_.scala.Boolean): Experimental = copy(tfrtUseIfrt = __v)
      def withBackendServerPort(__v: _root_.scala.Int): Experimental = copy(backendServerPort = __v)
      def withTargetTpu(__v: _root_.scala.Boolean): Experimental = copy(targetTpu = __v)
      def withTargetGpu(__v: _root_.scala.Boolean): Experimental = copy(targetGpu = __v)
      def withStreamMergeThreshold(__v: _root_.scala.Int): Experimental = copy(streamMergeThreshold = __v)
      def withDisableFunctionalOpsLowering(__v: _root_.scala.Boolean): Experimental = copy(disableFunctionalOpsLowering = __v)
      def withXlaPreferSingleGraphCluster(__v: _root_.scala.Boolean): Experimental = copy(xlaPreferSingleGraphCluster = __v)
      def getCoordinationConfig: tensorboard.coordination_config.CoordinationServiceConfig = coordinationConfig.getOrElse(tensorboard.coordination_config.CoordinationServiceConfig.defaultInstance)
      def clearCoordinationConfig: Experimental = copy(coordinationConfig = _root_.scala.None)
      def withCoordinationConfig(__v: tensorboard.coordination_config.CoordinationServiceConfig): Experimental = copy(coordinationConfig = Option(__v))
      def withDisableOptimizeForStaticGraph(__v: _root_.scala.Boolean): Experimental = copy(disableOptimizeForStaticGraph = __v)
      def withDisableEagerExecutorStreamingEnqueue(__v: _root_.scala.Boolean): Experimental = copy(disableEagerExecutorStreamingEnqueue = __v)
      def withUnknownFields(__v: _root_.scalapb.UnknownFieldSet) = copy(unknownFields = __v)
      def discardUnknownFields = copy(unknownFields = _root_.scalapb.UnknownFieldSet.empty)
      def getFieldByNumber(__fieldNumber: _root_.scala.Int): _root_.scala.Any = {
        (__fieldNumber: @_root_.scala.unchecked) match {
          case 1 => {
            val __t = collectiveGroupLeader
            if (__t != "") __t else null
          }
          case 3 => {
            val __t = executorType
            if (__t != "") __t else null
          }
          case 4 => {
            val __t = recvBufMaxChunk
            if (__t != 0) __t else null
          }
          case 5 => {
            val __t = useNumaAffinity
            if (__t != false) __t else null
          }
          case 6 => {
            val __t = collectiveDeterministicSequentialExecution
            if (__t != false) __t else null
          }
          case 7 => {
            val __t = collectiveNccl
            if (__t != false) __t else null
          }
          case 8 => {
            val __t = shareSessionStateInClusterspecPropagation
            if (__t != false) __t else null
          }
          case 9 => {
            val __t = disableThreadSpinning
            if (__t != false) __t else null
          }
          case 10 => {
            val __t = shareClusterDevicesInSession
            if (__t != false) __t else null
          }
          case 11 => sessionMetadata.orNull
          case 12 => {
            val __t = optimizeForStaticGraph
            if (__t != false) __t else null
          }
          case 13 => {
            val __t = enableMlirBridge
            if (__t != false) __t else null
          }
          case 17 => {
            val __t = mlirBridgeRollout.javaValueDescriptor
            if (__t.getNumber() != 0) __t else null
          }
          case 16 => {
            val __t = enableMlirGraphOptimization
            if (__t != false) __t else null
          }
          case 14 => {
            val __t = disableOutputPartitionGraphs
            if (__t != false) __t else null
          }
          case 15 => {
            val __t = xlaFusionAutotunerThresh
            if (__t != 0L) __t else null
          }
          case 18 => {
            val __t = useTfrt
            if (__t != false) __t else null
          }
          case 27 => {
            val __t = enableMultiHost
            if (__t != false) __t else null
          }
          case 32 => {
            val __t = tfrtUseIfrt
            if (__t != false) __t else null
          }
          case 28 => {
            val __t = backendServerPort
            if (__t != 0) __t else null
          }
          case 29 => {
            val __t = targetTpu
            if (__t != false) __t else null
          }
          case 30 => {
            val __t = targetGpu
            if (__t != false) __t else null
          }
          case 31 => {
            val __t = streamMergeThreshold
            if (__t != 0) __t else null
          }
          case 21 => {
            val __t = disableFunctionalOpsLowering
            if (__t != false) __t else null
          }
          case 22 => {
            val __t = xlaPreferSingleGraphCluster
            if (__t != false) __t else null
          }
          case 23 => coordinationConfig.orNull
          case 24 => {
            val __t = disableOptimizeForStaticGraph
            if (__t != false) __t else null
          }
          case 26 => {
            val __t = disableEagerExecutorStreamingEnqueue
            if (__t != false) __t else null
          }
        }
      }
      def getField(__field: _root_.scalapb.descriptors.FieldDescriptor): _root_.scalapb.descriptors.PValue = {
        _root_.scala.Predef.require(__field.containingMessage eq companion.scalaDescriptor)
        (__field.number: @_root_.scala.unchecked) match {
          case 1 => _root_.scalapb.descriptors.PString(collectiveGroupLeader)
          case 3 => _root_.scalapb.descriptors.PString(executorType)
          case 4 => _root_.scalapb.descriptors.PInt(recvBufMaxChunk)
          case 5 => _root_.scalapb.descriptors.PBoolean(useNumaAffinity)
          case 6 => _root_.scalapb.descriptors.PBoolean(collectiveDeterministicSequentialExecution)
          case 7 => _root_.scalapb.descriptors.PBoolean(collectiveNccl)
          case 8 => _root_.scalapb.descriptors.PBoolean(shareSessionStateInClusterspecPropagation)
          case 9 => _root_.scalapb.descriptors.PBoolean(disableThreadSpinning)
          case 10 => _root_.scalapb.descriptors.PBoolean(shareClusterDevicesInSession)
          case 11 => sessionMetadata.map(_.toPMessage).getOrElse(_root_.scalapb.descriptors.PEmpty)
          case 12 => _root_.scalapb.descriptors.PBoolean(optimizeForStaticGraph)
          case 13 => _root_.scalapb.descriptors.PBoolean(enableMlirBridge)
          case 17 => _root_.scalapb.descriptors.PEnum(mlirBridgeRollout.scalaValueDescriptor)
          case 16 => _root_.scalapb.descriptors.PBoolean(enableMlirGraphOptimization)
          case 14 => _root_.scalapb.descriptors.PBoolean(disableOutputPartitionGraphs)
          case 15 => _root_.scalapb.descriptors.PLong(xlaFusionAutotunerThresh)
          case 18 => _root_.scalapb.descriptors.PBoolean(useTfrt)
          case 27 => _root_.scalapb.descriptors.PBoolean(enableMultiHost)
          case 32 => _root_.scalapb.descriptors.PBoolean(tfrtUseIfrt)
          case 28 => _root_.scalapb.descriptors.PInt(backendServerPort)
          case 29 => _root_.scalapb.descriptors.PBoolean(targetTpu)
          case 30 => _root_.scalapb.descriptors.PBoolean(targetGpu)
          case 31 => _root_.scalapb.descriptors.PInt(streamMergeThreshold)
          case 21 => _root_.scalapb.descriptors.PBoolean(disableFunctionalOpsLowering)
          case 22 => _root_.scalapb.descriptors.PBoolean(xlaPreferSingleGraphCluster)
          case 23 => coordinationConfig.map(_.toPMessage).getOrElse(_root_.scalapb.descriptors.PEmpty)
          case 24 => _root_.scalapb.descriptors.PBoolean(disableOptimizeForStaticGraph)
          case 26 => _root_.scalapb.descriptors.PBoolean(disableEagerExecutorStreamingEnqueue)
        }
      }
      def toProtoString: _root_.scala.Predef.String = _root_.scalapb.TextFormat.printToUnicodeString(this)
      def companion: org.tensorflow.framework.config.ConfigProto.Experimental.type = org.tensorflow.framework.config.ConfigProto.Experimental
      // @@protoc_insertion_point(GeneratedMessage[tensorboard.ConfigProto.Experimental])
  }
  
  object Experimental extends scalapb.GeneratedMessageCompanion[org.tensorflow.framework.config.ConfigProto.Experimental] {
    implicit def messageCompanion: scalapb.GeneratedMessageCompanion[org.tensorflow.framework.config.ConfigProto.Experimental] = this
    def parseFrom(`_input__`: _root_.com.google.protobuf.CodedInputStream): org.tensorflow.framework.config.ConfigProto.Experimental = {
      var __collectiveGroupLeader: _root_.scala.Predef.String = ""
      var __executorType: _root_.scala.Predef.String = ""
      var __recvBufMaxChunk: _root_.scala.Int = 0
      var __useNumaAffinity: _root_.scala.Boolean = false
      var __collectiveDeterministicSequentialExecution: _root_.scala.Boolean = false
      var __collectiveNccl: _root_.scala.Boolean = false
      var __shareSessionStateInClusterspecPropagation: _root_.scala.Boolean = false
      var __disableThreadSpinning: _root_.scala.Boolean = false
      var __shareClusterDevicesInSession: _root_.scala.Boolean = false
      var __sessionMetadata: _root_.scala.Option[org.tensorflow.framework.config.SessionMetadata] = _root_.scala.None
      var __optimizeForStaticGraph: _root_.scala.Boolean = false
      var __enableMlirBridge: _root_.scala.Boolean = false
      var __mlirBridgeRollout: org.tensorflow.framework.config.ConfigProto.Experimental.MlirBridgeRollout = org.tensorflow.framework.config.ConfigProto.Experimental.MlirBridgeRollout.MLIR_BRIDGE_ROLLOUT_UNSPECIFIED
      var __enableMlirGraphOptimization: _root_.scala.Boolean = false
      var __disableOutputPartitionGraphs: _root_.scala.Boolean = false
      var __xlaFusionAutotunerThresh: _root_.scala.Long = 0L
      var __useTfrt: _root_.scala.Boolean = false
      var __enableMultiHost: _root_.scala.Boolean = false
      var __tfrtUseIfrt: _root_.scala.Boolean = false
      var __backendServerPort: _root_.scala.Int = 0
      var __targetTpu: _root_.scala.Boolean = false
      var __targetGpu: _root_.scala.Boolean = false
      var __streamMergeThreshold: _root_.scala.Int = 0
      var __disableFunctionalOpsLowering: _root_.scala.Boolean = false
      var __xlaPreferSingleGraphCluster: _root_.scala.Boolean = false
      var __coordinationConfig: _root_.scala.Option[tensorboard.coordination_config.CoordinationServiceConfig] = _root_.scala.None
      var __disableOptimizeForStaticGraph: _root_.scala.Boolean = false
      var __disableEagerExecutorStreamingEnqueue: _root_.scala.Boolean = false
      var `_unknownFields__`: _root_.scalapb.UnknownFieldSet.Builder = null
      var _done__ = false
      while (!_done__) {
        val _tag__ = _input__.readTag()
        _tag__ match {
          case 0 => _done__ = true
          case 10 =>
            __collectiveGroupLeader = _input__.readStringRequireUtf8()
          case 26 =>
            __executorType = _input__.readStringRequireUtf8()
          case 32 =>
            __recvBufMaxChunk = _input__.readInt32()
          case 40 =>
            __useNumaAffinity = _input__.readBool()
          case 48 =>
            __collectiveDeterministicSequentialExecution = _input__.readBool()
          case 56 =>
            __collectiveNccl = _input__.readBool()
          case 64 =>
            __shareSessionStateInClusterspecPropagation = _input__.readBool()
          case 72 =>
            __disableThreadSpinning = _input__.readBool()
          case 80 =>
            __shareClusterDevicesInSession = _input__.readBool()
          case 90 =>
            __sessionMetadata = _root_.scala.Option(__sessionMetadata.fold(_root_.scalapb.LiteParser.readMessage[org.tensorflow.framework.config.SessionMetadata](_input__))(_root_.scalapb.LiteParser.readMessage(_input__, _)))
          case 96 =>
            __optimizeForStaticGraph = _input__.readBool()
          case 104 =>
            __enableMlirBridge = _input__.readBool()
          case 136 =>
            __mlirBridgeRollout = org.tensorflow.framework.config.ConfigProto.Experimental.MlirBridgeRollout.fromValue(_input__.readEnum())
          case 128 =>
            __enableMlirGraphOptimization = _input__.readBool()
          case 112 =>
            __disableOutputPartitionGraphs = _input__.readBool()
          case 120 =>
            __xlaFusionAutotunerThresh = _input__.readInt64()
          case 144 =>
            __useTfrt = _input__.readBool()
          case 216 =>
            __enableMultiHost = _input__.readBool()
          case 256 =>
            __tfrtUseIfrt = _input__.readBool()
          case 224 =>
            __backendServerPort = _input__.readInt32()
          case 232 =>
            __targetTpu = _input__.readBool()
          case 240 =>
            __targetGpu = _input__.readBool()
          case 248 =>
            __streamMergeThreshold = _input__.readInt32()
          case 168 =>
            __disableFunctionalOpsLowering = _input__.readBool()
          case 176 =>
            __xlaPreferSingleGraphCluster = _input__.readBool()
          case 186 =>
            __coordinationConfig = _root_.scala.Option(__coordinationConfig.fold(_root_.scalapb.LiteParser.readMessage[tensorboard.coordination_config.CoordinationServiceConfig](_input__))(_root_.scalapb.LiteParser.readMessage(_input__, _)))
          case 192 =>
            __disableOptimizeForStaticGraph = _input__.readBool()
          case 208 =>
            __disableEagerExecutorStreamingEnqueue = _input__.readBool()
          case tag =>
            if (_unknownFields__ == null) {
              _unknownFields__ = new _root_.scalapb.UnknownFieldSet.Builder()
            }
            _unknownFields__.parseField(tag, _input__)
        }
      }
      org.tensorflow.framework.config.ConfigProto.Experimental(
          collectiveGroupLeader = __collectiveGroupLeader,
          executorType = __executorType,
          recvBufMaxChunk = __recvBufMaxChunk,
          useNumaAffinity = __useNumaAffinity,
          collectiveDeterministicSequentialExecution = __collectiveDeterministicSequentialExecution,
          collectiveNccl = __collectiveNccl,
          shareSessionStateInClusterspecPropagation = __shareSessionStateInClusterspecPropagation,
          disableThreadSpinning = __disableThreadSpinning,
          shareClusterDevicesInSession = __shareClusterDevicesInSession,
          sessionMetadata = __sessionMetadata,
          optimizeForStaticGraph = __optimizeForStaticGraph,
          enableMlirBridge = __enableMlirBridge,
          mlirBridgeRollout = __mlirBridgeRollout,
          enableMlirGraphOptimization = __enableMlirGraphOptimization,
          disableOutputPartitionGraphs = __disableOutputPartitionGraphs,
          xlaFusionAutotunerThresh = __xlaFusionAutotunerThresh,
          useTfrt = __useTfrt,
          enableMultiHost = __enableMultiHost,
          tfrtUseIfrt = __tfrtUseIfrt,
          backendServerPort = __backendServerPort,
          targetTpu = __targetTpu,
          targetGpu = __targetGpu,
          streamMergeThreshold = __streamMergeThreshold,
          disableFunctionalOpsLowering = __disableFunctionalOpsLowering,
          xlaPreferSingleGraphCluster = __xlaPreferSingleGraphCluster,
          coordinationConfig = __coordinationConfig,
          disableOptimizeForStaticGraph = __disableOptimizeForStaticGraph,
          disableEagerExecutorStreamingEnqueue = __disableEagerExecutorStreamingEnqueue,
          unknownFields = if (_unknownFields__ == null) _root_.scalapb.UnknownFieldSet.empty else _unknownFields__.result()
      )
    }
    implicit def messageReads: _root_.scalapb.descriptors.Reads[org.tensorflow.framework.config.ConfigProto.Experimental] = _root_.scalapb.descriptors.Reads{
      case _root_.scalapb.descriptors.PMessage(__fieldsMap) =>
        _root_.scala.Predef.require(__fieldsMap.keys.forall(_.containingMessage eq scalaDescriptor), "FieldDescriptor does not match message type.")
        org.tensorflow.framework.config.ConfigProto.Experimental(
          collectiveGroupLeader = __fieldsMap.get(scalaDescriptor.findFieldByNumber(1).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
          executorType = __fieldsMap.get(scalaDescriptor.findFieldByNumber(3).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
          recvBufMaxChunk = __fieldsMap.get(scalaDescriptor.findFieldByNumber(4).get).map(_.as[_root_.scala.Int]).getOrElse(0),
          useNumaAffinity = __fieldsMap.get(scalaDescriptor.findFieldByNumber(5).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          collectiveDeterministicSequentialExecution = __fieldsMap.get(scalaDescriptor.findFieldByNumber(6).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          collectiveNccl = __fieldsMap.get(scalaDescriptor.findFieldByNumber(7).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          shareSessionStateInClusterspecPropagation = __fieldsMap.get(scalaDescriptor.findFieldByNumber(8).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          disableThreadSpinning = __fieldsMap.get(scalaDescriptor.findFieldByNumber(9).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          shareClusterDevicesInSession = __fieldsMap.get(scalaDescriptor.findFieldByNumber(10).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          sessionMetadata = __fieldsMap.get(scalaDescriptor.findFieldByNumber(11).get).flatMap(_.as[_root_.scala.Option[org.tensorflow.framework.config.SessionMetadata]]),
          optimizeForStaticGraph = __fieldsMap.get(scalaDescriptor.findFieldByNumber(12).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          enableMlirBridge = __fieldsMap.get(scalaDescriptor.findFieldByNumber(13).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          mlirBridgeRollout = org.tensorflow.framework.config.ConfigProto.Experimental.MlirBridgeRollout.fromValue(__fieldsMap.get(scalaDescriptor.findFieldByNumber(17).get).map(_.as[_root_.scalapb.descriptors.EnumValueDescriptor]).getOrElse(org.tensorflow.framework.config.ConfigProto.Experimental.MlirBridgeRollout.MLIR_BRIDGE_ROLLOUT_UNSPECIFIED.scalaValueDescriptor).number),
          enableMlirGraphOptimization = __fieldsMap.get(scalaDescriptor.findFieldByNumber(16).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          disableOutputPartitionGraphs = __fieldsMap.get(scalaDescriptor.findFieldByNumber(14).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          xlaFusionAutotunerThresh = __fieldsMap.get(scalaDescriptor.findFieldByNumber(15).get).map(_.as[_root_.scala.Long]).getOrElse(0L),
          useTfrt = __fieldsMap.get(scalaDescriptor.findFieldByNumber(18).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          enableMultiHost = __fieldsMap.get(scalaDescriptor.findFieldByNumber(27).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          tfrtUseIfrt = __fieldsMap.get(scalaDescriptor.findFieldByNumber(32).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          backendServerPort = __fieldsMap.get(scalaDescriptor.findFieldByNumber(28).get).map(_.as[_root_.scala.Int]).getOrElse(0),
          targetTpu = __fieldsMap.get(scalaDescriptor.findFieldByNumber(29).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          targetGpu = __fieldsMap.get(scalaDescriptor.findFieldByNumber(30).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          streamMergeThreshold = __fieldsMap.get(scalaDescriptor.findFieldByNumber(31).get).map(_.as[_root_.scala.Int]).getOrElse(0),
          disableFunctionalOpsLowering = __fieldsMap.get(scalaDescriptor.findFieldByNumber(21).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          xlaPreferSingleGraphCluster = __fieldsMap.get(scalaDescriptor.findFieldByNumber(22).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          coordinationConfig = __fieldsMap.get(scalaDescriptor.findFieldByNumber(23).get).flatMap(_.as[_root_.scala.Option[tensorboard.coordination_config.CoordinationServiceConfig]]),
          disableOptimizeForStaticGraph = __fieldsMap.get(scalaDescriptor.findFieldByNumber(24).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          disableEagerExecutorStreamingEnqueue = __fieldsMap.get(scalaDescriptor.findFieldByNumber(26).get).map(_.as[_root_.scala.Boolean]).getOrElse(false)
        )
      case _ => throw new RuntimeException("Expected PMessage")
    }
    def javaDescriptor: _root_.com.google.protobuf.Descriptors.Descriptor = org.tensorflow.framework.config.ConfigProto.javaDescriptor.getNestedTypes().get(1)
    def scalaDescriptor: _root_.scalapb.descriptors.Descriptor = org.tensorflow.framework.config.ConfigProto.scalaDescriptor.nestedMessages(1)
    def messageCompanionForFieldNumber(__number: _root_.scala.Int): _root_.scalapb.GeneratedMessageCompanion[?]= {
      var __out: _root_.scalapb.GeneratedMessageCompanion[?]= null
      (__number: @_root_.scala.unchecked) match {
        case 11 => __out = org.tensorflow.framework.config.SessionMetadata
        case 23 => __out = tensorboard.coordination_config.CoordinationServiceConfig
      }
      __out
    }
    lazy val nestedMessagesCompanions: Seq[_root_.scalapb.GeneratedMessageCompanion[? <: _root_.scalapb.GeneratedMessage]] = Seq.empty
    def enumCompanionForFieldNumber(__fieldNumber: _root_.scala.Int): _root_.scalapb.GeneratedEnumCompanion[?]= {
      (__fieldNumber: @_root_.scala.unchecked) match {
        case 17 => org.tensorflow.framework.config.ConfigProto.Experimental.MlirBridgeRollout
      }
    }
    lazy val defaultInstance = org.tensorflow.framework.config.ConfigProto.Experimental(
      collectiveGroupLeader = "",
      executorType = "",
      recvBufMaxChunk = 0,
      useNumaAffinity = false,
      collectiveDeterministicSequentialExecution = false,
      collectiveNccl = false,
      shareSessionStateInClusterspecPropagation = false,
      disableThreadSpinning = false,
      shareClusterDevicesInSession = false,
      sessionMetadata = _root_.scala.None,
      optimizeForStaticGraph = false,
      enableMlirBridge = false,
      mlirBridgeRollout = org.tensorflow.framework.config.ConfigProto.Experimental.MlirBridgeRollout.MLIR_BRIDGE_ROLLOUT_UNSPECIFIED,
      enableMlirGraphOptimization = false,
      disableOutputPartitionGraphs = false,
      xlaFusionAutotunerThresh = 0L,
      useTfrt = false,
      enableMultiHost = false,
      tfrtUseIfrt = false,
      backendServerPort = 0,
      targetTpu = false,
      targetGpu = false,
      streamMergeThreshold = 0,
      disableFunctionalOpsLowering = false,
      xlaPreferSingleGraphCluster = false,
      coordinationConfig = _root_.scala.None,
      disableOptimizeForStaticGraph = false,
      disableEagerExecutorStreamingEnqueue = false
    )
    /** An enum that describes the state of the MLIR bridge rollout.
      */
    sealed abstract class MlirBridgeRollout(val value: _root_.scala.Int) extends _root_.scalapb.GeneratedEnum {
      type EnumType = org.tensorflow.framework.config.ConfigProto.Experimental.MlirBridgeRollout
      type RecognizedType = org.tensorflow.framework.config.ConfigProto.Experimental.MlirBridgeRollout.Recognized
      def isMlirBridgeRolloutUnspecified: _root_.scala.Boolean = false
      def isMlirBridgeRolloutEnabled: _root_.scala.Boolean = false
      def isMlirBridgeRolloutDisabled: _root_.scala.Boolean = false
      def companion: _root_.scalapb.GeneratedEnumCompanion[MlirBridgeRollout] = org.tensorflow.framework.config.ConfigProto.Experimental.MlirBridgeRollout
      final def asRecognized: _root_.scala.Option[org.tensorflow.framework.config.ConfigProto.Experimental.MlirBridgeRollout.Recognized] = if (isUnrecognized) _root_.scala.None else _root_.scala.Some(this.asInstanceOf[org.tensorflow.framework.config.ConfigProto.Experimental.MlirBridgeRollout.Recognized])
    }
    
    object MlirBridgeRollout extends _root_.scalapb.GeneratedEnumCompanion[MlirBridgeRollout] {
      sealed trait Recognized extends MlirBridgeRollout
      implicit def enumCompanion: _root_.scalapb.GeneratedEnumCompanion[MlirBridgeRollout] = this
      
      /** If this field is left unspecified, the MLIR bridge may be selectively
        * enabled on a per graph basis.
        */
      @SerialVersionUID(0L)
      case object MLIR_BRIDGE_ROLLOUT_UNSPECIFIED extends MlirBridgeRollout(0) with MlirBridgeRollout.Recognized {
        val index = 0
        val name = "MLIR_BRIDGE_ROLLOUT_UNSPECIFIED"
        override def isMlirBridgeRolloutUnspecified: _root_.scala.Boolean = true
      }
      
      /** Enabling the MLIR bridge enables it for all graphs in this session.
        */
      @SerialVersionUID(0L)
      case object MLIR_BRIDGE_ROLLOUT_ENABLED extends MlirBridgeRollout(1) with MlirBridgeRollout.Recognized {
        val index = 1
        val name = "MLIR_BRIDGE_ROLLOUT_ENABLED"
        override def isMlirBridgeRolloutEnabled: _root_.scala.Boolean = true
      }
      
      /** Disabling the MLIR bridge disables it for all graphs in this session.
        */
      @SerialVersionUID(0L)
      case object MLIR_BRIDGE_ROLLOUT_DISABLED extends MlirBridgeRollout(2) with MlirBridgeRollout.Recognized {
        val index = 2
        val name = "MLIR_BRIDGE_ROLLOUT_DISABLED"
        override def isMlirBridgeRolloutDisabled: _root_.scala.Boolean = true
      }
      
      @SerialVersionUID(0L)
      final case class Unrecognized(unrecognizedValue: _root_.scala.Int) extends MlirBridgeRollout(unrecognizedValue) with _root_.scalapb.UnrecognizedEnum
      lazy val values: scala.collection.immutable.Seq[ValueType] = scala.collection.immutable.Seq(MLIR_BRIDGE_ROLLOUT_UNSPECIFIED, MLIR_BRIDGE_ROLLOUT_ENABLED, MLIR_BRIDGE_ROLLOUT_DISABLED)
      def fromValue(__value: _root_.scala.Int): MlirBridgeRollout = __value match {
        case 0 => MLIR_BRIDGE_ROLLOUT_UNSPECIFIED
        case 1 => MLIR_BRIDGE_ROLLOUT_ENABLED
        case 2 => MLIR_BRIDGE_ROLLOUT_DISABLED
        case __other => Unrecognized(__other)
      }
      def javaDescriptor: _root_.com.google.protobuf.Descriptors.EnumDescriptor = org.tensorflow.framework.config.ConfigProto.Experimental.javaDescriptor.getEnumTypes().get(0)
      def scalaDescriptor: _root_.scalapb.descriptors.EnumDescriptor = org.tensorflow.framework.config.ConfigProto.Experimental.scalaDescriptor.enums(0)
    }
    implicit class ExperimentalLens[UpperPB](_l: _root_.scalapb.lenses.Lens[UpperPB, org.tensorflow.framework.config.ConfigProto.Experimental]) extends _root_.scalapb.lenses.ObjectLens[UpperPB, org.tensorflow.framework.config.ConfigProto.Experimental](_l) {
      def collectiveGroupLeader: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.collectiveGroupLeader)((c_, f_) => c_.copy(collectiveGroupLeader = f_))
      def executorType: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.executorType)((c_, f_) => c_.copy(executorType = f_))
      def recvBufMaxChunk: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Int] = field(_.recvBufMaxChunk)((c_, f_) => c_.copy(recvBufMaxChunk = f_))
      def useNumaAffinity: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.useNumaAffinity)((c_, f_) => c_.copy(useNumaAffinity = f_))
      def collectiveDeterministicSequentialExecution: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.collectiveDeterministicSequentialExecution)((c_, f_) => c_.copy(collectiveDeterministicSequentialExecution = f_))
      def collectiveNccl: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.collectiveNccl)((c_, f_) => c_.copy(collectiveNccl = f_))
      def shareSessionStateInClusterspecPropagation: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.shareSessionStateInClusterspecPropagation)((c_, f_) => c_.copy(shareSessionStateInClusterspecPropagation = f_))
      def disableThreadSpinning: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.disableThreadSpinning)((c_, f_) => c_.copy(disableThreadSpinning = f_))
      def shareClusterDevicesInSession: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.shareClusterDevicesInSession)((c_, f_) => c_.copy(shareClusterDevicesInSession = f_))
      def sessionMetadata: _root_.scalapb.lenses.Lens[UpperPB, org.tensorflow.framework.config.SessionMetadata] = field(_.getSessionMetadata)((c_, f_) => c_.copy(sessionMetadata = _root_.scala.Option(f_)))
      def optionalSessionMetadata: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Option[org.tensorflow.framework.config.SessionMetadata]] = field(_.sessionMetadata)((c_, f_) => c_.copy(sessionMetadata = f_))
      def optimizeForStaticGraph: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.optimizeForStaticGraph)((c_, f_) => c_.copy(optimizeForStaticGraph = f_))
      def enableMlirBridge: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.enableMlirBridge)((c_, f_) => c_.copy(enableMlirBridge = f_))
      def mlirBridgeRollout: _root_.scalapb.lenses.Lens[UpperPB, org.tensorflow.framework.config.ConfigProto.Experimental.MlirBridgeRollout] = field(_.mlirBridgeRollout)((c_, f_) => c_.copy(mlirBridgeRollout = f_))
      def enableMlirGraphOptimization: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.enableMlirGraphOptimization)((c_, f_) => c_.copy(enableMlirGraphOptimization = f_))
      def disableOutputPartitionGraphs: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.disableOutputPartitionGraphs)((c_, f_) => c_.copy(disableOutputPartitionGraphs = f_))
      def xlaFusionAutotunerThresh: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Long] = field(_.xlaFusionAutotunerThresh)((c_, f_) => c_.copy(xlaFusionAutotunerThresh = f_))
      def useTfrt: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.useTfrt)((c_, f_) => c_.copy(useTfrt = f_))
      def enableMultiHost: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.enableMultiHost)((c_, f_) => c_.copy(enableMultiHost = f_))
      def tfrtUseIfrt: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.tfrtUseIfrt)((c_, f_) => c_.copy(tfrtUseIfrt = f_))
      def backendServerPort: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Int] = field(_.backendServerPort)((c_, f_) => c_.copy(backendServerPort = f_))
      def targetTpu: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.targetTpu)((c_, f_) => c_.copy(targetTpu = f_))
      def targetGpu: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.targetGpu)((c_, f_) => c_.copy(targetGpu = f_))
      def streamMergeThreshold: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Int] = field(_.streamMergeThreshold)((c_, f_) => c_.copy(streamMergeThreshold = f_))
      def disableFunctionalOpsLowering: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.disableFunctionalOpsLowering)((c_, f_) => c_.copy(disableFunctionalOpsLowering = f_))
      def xlaPreferSingleGraphCluster: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.xlaPreferSingleGraphCluster)((c_, f_) => c_.copy(xlaPreferSingleGraphCluster = f_))
      def coordinationConfig: _root_.scalapb.lenses.Lens[UpperPB, tensorboard.coordination_config.CoordinationServiceConfig] = field(_.getCoordinationConfig)((c_, f_) => c_.copy(coordinationConfig = _root_.scala.Option(f_)))
      def optionalCoordinationConfig: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Option[tensorboard.coordination_config.CoordinationServiceConfig]] = field(_.coordinationConfig)((c_, f_) => c_.copy(coordinationConfig = f_))
      def disableOptimizeForStaticGraph: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.disableOptimizeForStaticGraph)((c_, f_) => c_.copy(disableOptimizeForStaticGraph = f_))
      def disableEagerExecutorStreamingEnqueue: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.disableEagerExecutorStreamingEnqueue)((c_, f_) => c_.copy(disableEagerExecutorStreamingEnqueue = f_))
    }
    final val COLLECTIVE_GROUP_LEADER_FIELD_NUMBER = 1
    final val EXECUTOR_TYPE_FIELD_NUMBER = 3
    final val RECV_BUF_MAX_CHUNK_FIELD_NUMBER = 4
    final val USE_NUMA_AFFINITY_FIELD_NUMBER = 5
    final val COLLECTIVE_DETERMINISTIC_SEQUENTIAL_EXECUTION_FIELD_NUMBER = 6
    final val COLLECTIVE_NCCL_FIELD_NUMBER = 7
    final val SHARE_SESSION_STATE_IN_CLUSTERSPEC_PROPAGATION_FIELD_NUMBER = 8
    final val DISABLE_THREAD_SPINNING_FIELD_NUMBER = 9
    final val SHARE_CLUSTER_DEVICES_IN_SESSION_FIELD_NUMBER = 10
    final val SESSION_METADATA_FIELD_NUMBER = 11
    final val OPTIMIZE_FOR_STATIC_GRAPH_FIELD_NUMBER = 12
    final val ENABLE_MLIR_BRIDGE_FIELD_NUMBER = 13
    final val MLIR_BRIDGE_ROLLOUT_FIELD_NUMBER = 17
    final val ENABLE_MLIR_GRAPH_OPTIMIZATION_FIELD_NUMBER = 16
    final val DISABLE_OUTPUT_PARTITION_GRAPHS_FIELD_NUMBER = 14
    final val XLA_FUSION_AUTOTUNER_THRESH_FIELD_NUMBER = 15
    final val USE_TFRT_FIELD_NUMBER = 18
    final val ENABLE_MULTI_HOST_FIELD_NUMBER = 27
    final val TFRT_USE_IFRT_FIELD_NUMBER = 32
    final val BACKEND_SERVER_PORT_FIELD_NUMBER = 28
    final val TARGET_TPU_FIELD_NUMBER = 29
    final val TARGET_GPU_FIELD_NUMBER = 30
    final val STREAM_MERGE_THRESHOLD_FIELD_NUMBER = 31
    final val DISABLE_FUNCTIONAL_OPS_LOWERING_FIELD_NUMBER = 21
    final val XLA_PREFER_SINGLE_GRAPH_CLUSTER_FIELD_NUMBER = 22
    final val COORDINATION_CONFIG_FIELD_NUMBER = 23
    final val DISABLE_OPTIMIZE_FOR_STATIC_GRAPH_FIELD_NUMBER = 24
    final val DISABLE_EAGER_EXECUTOR_STREAMING_ENQUEUE_FIELD_NUMBER = 26
    def of(
      collectiveGroupLeader: _root_.scala.Predef.String,
      executorType: _root_.scala.Predef.String,
      recvBufMaxChunk: _root_.scala.Int,
      useNumaAffinity: _root_.scala.Boolean,
      collectiveDeterministicSequentialExecution: _root_.scala.Boolean,
      collectiveNccl: _root_.scala.Boolean,
      shareSessionStateInClusterspecPropagation: _root_.scala.Boolean,
      disableThreadSpinning: _root_.scala.Boolean,
      shareClusterDevicesInSession: _root_.scala.Boolean,
      sessionMetadata: _root_.scala.Option[org.tensorflow.framework.config.SessionMetadata],
      optimizeForStaticGraph: _root_.scala.Boolean,
      enableMlirBridge: _root_.scala.Boolean,
      mlirBridgeRollout: org.tensorflow.framework.config.ConfigProto.Experimental.MlirBridgeRollout,
      enableMlirGraphOptimization: _root_.scala.Boolean,
      disableOutputPartitionGraphs: _root_.scala.Boolean,
      xlaFusionAutotunerThresh: _root_.scala.Long,
      useTfrt: _root_.scala.Boolean,
      enableMultiHost: _root_.scala.Boolean,
      tfrtUseIfrt: _root_.scala.Boolean,
      backendServerPort: _root_.scala.Int,
      targetTpu: _root_.scala.Boolean,
      targetGpu: _root_.scala.Boolean,
      streamMergeThreshold: _root_.scala.Int,
      disableFunctionalOpsLowering: _root_.scala.Boolean,
      xlaPreferSingleGraphCluster: _root_.scala.Boolean,
      coordinationConfig: _root_.scala.Option[tensorboard.coordination_config.CoordinationServiceConfig],
      disableOptimizeForStaticGraph: _root_.scala.Boolean,
      disableEagerExecutorStreamingEnqueue: _root_.scala.Boolean
    ): _root_.org.tensorflow.framework.config.ConfigProto.Experimental = _root_.org.tensorflow.framework.config.ConfigProto.Experimental(
      collectiveGroupLeader,
      executorType,
      recvBufMaxChunk,
      useNumaAffinity,
      collectiveDeterministicSequentialExecution,
      collectiveNccl,
      shareSessionStateInClusterspecPropagation,
      disableThreadSpinning,
      shareClusterDevicesInSession,
      sessionMetadata,
      optimizeForStaticGraph,
      enableMlirBridge,
      mlirBridgeRollout,
      enableMlirGraphOptimization,
      disableOutputPartitionGraphs,
      xlaFusionAutotunerThresh,
      useTfrt,
      enableMultiHost,
      tfrtUseIfrt,
      backendServerPort,
      targetTpu,
      targetGpu,
      streamMergeThreshold,
      disableFunctionalOpsLowering,
      xlaPreferSingleGraphCluster,
      coordinationConfig,
      disableOptimizeForStaticGraph,
      disableEagerExecutorStreamingEnqueue
    )
    // @@protoc_insertion_point(GeneratedMessageCompanion[tensorboard.ConfigProto.Experimental])
  }
  
  implicit class ConfigProtoLens[UpperPB](_l: _root_.scalapb.lenses.Lens[UpperPB, org.tensorflow.framework.config.ConfigProto]) extends _root_.scalapb.lenses.ObjectLens[UpperPB, org.tensorflow.framework.config.ConfigProto](_l) {
    def deviceCount: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.collection.immutable.Map[_root_.scala.Predef.String, _root_.scala.Int]] = field(_.deviceCount)((c_, f_) => c_.copy(deviceCount = f_))
    def intraOpParallelismThreads: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Int] = field(_.intraOpParallelismThreads)((c_, f_) => c_.copy(intraOpParallelismThreads = f_))
    def interOpParallelismThreads: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Int] = field(_.interOpParallelismThreads)((c_, f_) => c_.copy(interOpParallelismThreads = f_))
    def usePerSessionThreads: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.usePerSessionThreads)((c_, f_) => c_.copy(usePerSessionThreads = f_))
    def sessionInterOpThreadPool: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[org.tensorflow.framework.config.ThreadPoolOptionProto]] = field(_.sessionInterOpThreadPool)((c_, f_) => c_.copy(sessionInterOpThreadPool = f_))
    def placementPeriod: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Int] = field(_.placementPeriod)((c_, f_) => c_.copy(placementPeriod = f_))
    def deviceFilters: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[_root_.scala.Predef.String]] = field(_.deviceFilters)((c_, f_) => c_.copy(deviceFilters = f_))
    def gpuOptions: _root_.scalapb.lenses.Lens[UpperPB, org.tensorflow.framework.config.GPUOptions] = field(_.getGpuOptions)((c_, f_) => c_.copy(gpuOptions = _root_.scala.Option(f_)))
    def optionalGpuOptions: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Option[org.tensorflow.framework.config.GPUOptions]] = field(_.gpuOptions)((c_, f_) => c_.copy(gpuOptions = f_))
    def pluggableDeviceOptions: _root_.scalapb.lenses.Lens[UpperPB, org.tensorflow.framework.config.GPUOptions] = field(_.getPluggableDeviceOptions)((c_, f_) => c_.copy(pluggableDeviceOptions = _root_.scala.Option(f_)))
    def optionalPluggableDeviceOptions: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Option[org.tensorflow.framework.config.GPUOptions]] = field(_.pluggableDeviceOptions)((c_, f_) => c_.copy(pluggableDeviceOptions = f_))
    def allowSoftPlacement: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.allowSoftPlacement)((c_, f_) => c_.copy(allowSoftPlacement = f_))
    def logDevicePlacement: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.logDevicePlacement)((c_, f_) => c_.copy(logDevicePlacement = f_))
    def graphOptions: _root_.scalapb.lenses.Lens[UpperPB, org.tensorflow.framework.config.GraphOptions] = field(_.getGraphOptions)((c_, f_) => c_.copy(graphOptions = _root_.scala.Option(f_)))
    def optionalGraphOptions: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Option[org.tensorflow.framework.config.GraphOptions]] = field(_.graphOptions)((c_, f_) => c_.copy(graphOptions = f_))
    def operationTimeoutInMs: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Long] = field(_.operationTimeoutInMs)((c_, f_) => c_.copy(operationTimeoutInMs = f_))
    def rpcOptions: _root_.scalapb.lenses.Lens[UpperPB, tensorboard.rpc_options.RPCOptions] = field(_.getRpcOptions)((c_, f_) => c_.copy(rpcOptions = _root_.scala.Option(f_)))
    def optionalRpcOptions: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Option[tensorboard.rpc_options.RPCOptions]] = field(_.rpcOptions)((c_, f_) => c_.copy(rpcOptions = f_))
    def clusterDef: _root_.scalapb.lenses.Lens[UpperPB, org.tensorflow.distruntime.cluster.ClusterDef] = field(_.getClusterDef)((c_, f_) => c_.copy(clusterDef = _root_.scala.Option(f_)))
    def optionalClusterDef: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Option[org.tensorflow.distruntime.cluster.ClusterDef]] = field(_.clusterDef)((c_, f_) => c_.copy(clusterDef = f_))
    def isolateSessionState: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.isolateSessionState)((c_, f_) => c_.copy(isolateSessionState = f_))
    def shareClusterDevicesInSession: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.shareClusterDevicesInSession)((c_, f_) => c_.copy(shareClusterDevicesInSession = f_))
    def experimental: _root_.scalapb.lenses.Lens[UpperPB, org.tensorflow.framework.config.ConfigProto.Experimental] = field(_.getExperimental)((c_, f_) => c_.copy(experimental = _root_.scala.Option(f_)))
    def optionalExperimental: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Option[org.tensorflow.framework.config.ConfigProto.Experimental]] = field(_.experimental)((c_, f_) => c_.copy(experimental = f_))
  }
  final val DEVICE_COUNT_FIELD_NUMBER = 1
  final val INTRA_OP_PARALLELISM_THREADS_FIELD_NUMBER = 2
  final val INTER_OP_PARALLELISM_THREADS_FIELD_NUMBER = 5
  final val USE_PER_SESSION_THREADS_FIELD_NUMBER = 9
  final val SESSION_INTER_OP_THREAD_POOL_FIELD_NUMBER = 12
  final val PLACEMENT_PERIOD_FIELD_NUMBER = 3
  final val DEVICE_FILTERS_FIELD_NUMBER = 4
  final val GPU_OPTIONS_FIELD_NUMBER = 6
  final val PLUGGABLE_DEVICE_OPTIONS_FIELD_NUMBER = 18
  final val ALLOW_SOFT_PLACEMENT_FIELD_NUMBER = 7
  final val LOG_DEVICE_PLACEMENT_FIELD_NUMBER = 8
  final val GRAPH_OPTIONS_FIELD_NUMBER = 10
  final val OPERATION_TIMEOUT_IN_MS_FIELD_NUMBER = 11
  final val RPC_OPTIONS_FIELD_NUMBER = 13
  final val CLUSTER_DEF_FIELD_NUMBER = 14
  final val ISOLATE_SESSION_STATE_FIELD_NUMBER = 15
  final val SHARE_CLUSTER_DEVICES_IN_SESSION_FIELD_NUMBER = 17
  final val EXPERIMENTAL_FIELD_NUMBER = 16
  @transient
  private[config] val _typemapper_deviceCount: _root_.scalapb.TypeMapper[org.tensorflow.framework.config.ConfigProto.DeviceCountEntry, (_root_.scala.Predef.String, _root_.scala.Int)] = implicitly[_root_.scalapb.TypeMapper[org.tensorflow.framework.config.ConfigProto.DeviceCountEntry, (_root_.scala.Predef.String, _root_.scala.Int)]]
  def of(
    deviceCount: _root_.scala.collection.immutable.Map[_root_.scala.Predef.String, _root_.scala.Int],
    intraOpParallelismThreads: _root_.scala.Int,
    interOpParallelismThreads: _root_.scala.Int,
    usePerSessionThreads: _root_.scala.Boolean,
    sessionInterOpThreadPool: _root_.scala.Seq[org.tensorflow.framework.config.ThreadPoolOptionProto],
    placementPeriod: _root_.scala.Int,
    deviceFilters: _root_.scala.Seq[_root_.scala.Predef.String],
    gpuOptions: _root_.scala.Option[org.tensorflow.framework.config.GPUOptions],
    pluggableDeviceOptions: _root_.scala.Option[org.tensorflow.framework.config.GPUOptions],
    allowSoftPlacement: _root_.scala.Boolean,
    logDevicePlacement: _root_.scala.Boolean,
    graphOptions: _root_.scala.Option[org.tensorflow.framework.config.GraphOptions],
    operationTimeoutInMs: _root_.scala.Long,
    rpcOptions: _root_.scala.Option[tensorboard.rpc_options.RPCOptions],
    clusterDef: _root_.scala.Option[org.tensorflow.distruntime.cluster.ClusterDef],
    isolateSessionState: _root_.scala.Boolean,
    shareClusterDevicesInSession: _root_.scala.Boolean,
    experimental: _root_.scala.Option[org.tensorflow.framework.config.ConfigProto.Experimental]
  ): _root_.org.tensorflow.framework.config.ConfigProto = _root_.org.tensorflow.framework.config.ConfigProto(
    deviceCount,
    intraOpParallelismThreads,
    interOpParallelismThreads,
    usePerSessionThreads,
    sessionInterOpThreadPool,
    placementPeriod,
    deviceFilters,
    gpuOptions,
    pluggableDeviceOptions,
    allowSoftPlacement,
    logDevicePlacement,
    graphOptions,
    operationTimeoutInMs,
    rpcOptions,
    clusterDef,
    isolateSessionState,
    shareClusterDevicesInSession,
    experimental
  )
  // @@protoc_insertion_point(GeneratedMessageCompanion[tensorboard.ConfigProto])
}
