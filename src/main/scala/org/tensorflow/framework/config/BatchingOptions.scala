// Generated by the Scala Plugin for the Protocol Buffer Compiler.
// Do not edit!

package org.tensorflow.framework.config

/** @param numBatchThreads
  *   Number of scheduling threads for processing batches of work. Determines
  *   the number of batches processed in parallel. This should be roughly in line
  *   with the number of TPU cores available.
  * @param maxBatchSize
  *   The maximum allowed batch size. Can be larger than allowed_batch_sizes to
  *   utilize large batch splitting.
  * @param batchTimeoutMicros
  *   Maximum number of microseconds to wait before outputting an incomplete
  *   batch.
  * @param allowedBatchSizes
  *   Optional list of allowed batch sizes. If left empty, does nothing.
  *   Otherwise, supplies a list of batch sizes, causing the op to pad batches up
  *   to one of those sizes. The entries must increase monotonically, and the
  *   final entry must be equal or less than the max_batch_size.
  * @param maxEnqueuedBatches
  *   Maximum number of batches enqueued for processing before requests are
  *   failed fast.
  */
@SerialVersionUID(0L)
final case class BatchingOptions(
    numBatchThreads: _root_.scala.Int = 0,
    maxBatchSize: _root_.scala.Int = 0,
    batchTimeoutMicros: _root_.scala.Int = 0,
    allowedBatchSizes: _root_.scala.Seq[_root_.scala.Int] = _root_.scala.Seq.empty,
    maxEnqueuedBatches: _root_.scala.Int = 0,
    unknownFields: _root_.scalapb.UnknownFieldSet = _root_.scalapb.UnknownFieldSet.empty
    ) extends scalapb.GeneratedMessage with scalapb.lenses.Updatable[BatchingOptions] {
    private[this] def allowedBatchSizesSerializedSize = {
      if (__allowedBatchSizesSerializedSizeField == 0) __allowedBatchSizesSerializedSizeField = {
        var __s: _root_.scala.Int = 0
        allowedBatchSizes.foreach(__i => __s += _root_.com.google.protobuf.CodedOutputStream.computeInt32SizeNoTag(__i))
        __s
      }
      __allowedBatchSizesSerializedSizeField
    }
    @transient private[this] var __allowedBatchSizesSerializedSizeField: _root_.scala.Int = 0
    @transient
    private[this] var __serializedSizeMemoized: _root_.scala.Int = 0
    private[this] def __computeSerializedSize(): _root_.scala.Int = {
      var __size = 0
      
      {
        val __value = numBatchThreads
        if (__value != 0) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeInt32Size(1, __value)
        }
      };
      
      {
        val __value = maxBatchSize
        if (__value != 0) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeInt32Size(2, __value)
        }
      };
      
      {
        val __value = batchTimeoutMicros
        if (__value != 0) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeInt32Size(3, __value)
        }
      };
      if (allowedBatchSizes.nonEmpty) {
        val __localsize = allowedBatchSizesSerializedSize
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__localsize) + __localsize
      }
      
      {
        val __value = maxEnqueuedBatches
        if (__value != 0) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeInt32Size(5, __value)
        }
      };
      __size += unknownFields.serializedSize
      __size
    }
    override def serializedSize: _root_.scala.Int = {
      var __size = __serializedSizeMemoized
      if (__size == 0) {
        __size = __computeSerializedSize() + 1
        __serializedSizeMemoized = __size
      }
      __size - 1
      
    }
    def writeTo(`_output__`: _root_.com.google.protobuf.CodedOutputStream): _root_.scala.Unit = {
      {
        val __v = numBatchThreads
        if (__v != 0) {
          _output__.writeInt32(1, __v)
        }
      };
      {
        val __v = maxBatchSize
        if (__v != 0) {
          _output__.writeInt32(2, __v)
        }
      };
      {
        val __v = batchTimeoutMicros
        if (__v != 0) {
          _output__.writeInt32(3, __v)
        }
      };
      if (allowedBatchSizes.nonEmpty) {
        _output__.writeTag(4, 2)
        _output__.writeUInt32NoTag(allowedBatchSizesSerializedSize)
        allowedBatchSizes.foreach(_output__.writeInt32NoTag)
      };
      {
        val __v = maxEnqueuedBatches
        if (__v != 0) {
          _output__.writeInt32(5, __v)
        }
      };
      unknownFields.writeTo(_output__)
    }
    def withNumBatchThreads(__v: _root_.scala.Int): BatchingOptions = copy(numBatchThreads = __v)
    def withMaxBatchSize(__v: _root_.scala.Int): BatchingOptions = copy(maxBatchSize = __v)
    def withBatchTimeoutMicros(__v: _root_.scala.Int): BatchingOptions = copy(batchTimeoutMicros = __v)
    def clearAllowedBatchSizes = copy(allowedBatchSizes = _root_.scala.Seq.empty)
    def addAllowedBatchSizes(__vs: _root_.scala.Int *): BatchingOptions = addAllAllowedBatchSizes(__vs)
    def addAllAllowedBatchSizes(__vs: Iterable[_root_.scala.Int]): BatchingOptions = copy(allowedBatchSizes = allowedBatchSizes ++ __vs)
    def withAllowedBatchSizes(__v: _root_.scala.Seq[_root_.scala.Int]): BatchingOptions = copy(allowedBatchSizes = __v)
    def withMaxEnqueuedBatches(__v: _root_.scala.Int): BatchingOptions = copy(maxEnqueuedBatches = __v)
    def withUnknownFields(__v: _root_.scalapb.UnknownFieldSet) = copy(unknownFields = __v)
    def discardUnknownFields = copy(unknownFields = _root_.scalapb.UnknownFieldSet.empty)
    def getFieldByNumber(__fieldNumber: _root_.scala.Int): _root_.scala.Any = {
      (__fieldNumber: @_root_.scala.unchecked) match {
        case 1 => {
          val __t = numBatchThreads
          if (__t != 0) __t else null
        }
        case 2 => {
          val __t = maxBatchSize
          if (__t != 0) __t else null
        }
        case 3 => {
          val __t = batchTimeoutMicros
          if (__t != 0) __t else null
        }
        case 4 => allowedBatchSizes
        case 5 => {
          val __t = maxEnqueuedBatches
          if (__t != 0) __t else null
        }
      }
    }
    def getField(__field: _root_.scalapb.descriptors.FieldDescriptor): _root_.scalapb.descriptors.PValue = {
      _root_.scala.Predef.require(__field.containingMessage eq companion.scalaDescriptor)
      (__field.number: @_root_.scala.unchecked) match {
        case 1 => _root_.scalapb.descriptors.PInt(numBatchThreads)
        case 2 => _root_.scalapb.descriptors.PInt(maxBatchSize)
        case 3 => _root_.scalapb.descriptors.PInt(batchTimeoutMicros)
        case 4 => _root_.scalapb.descriptors.PRepeated(allowedBatchSizes.iterator.map(_root_.scalapb.descriptors.PInt(_)).toVector)
        case 5 => _root_.scalapb.descriptors.PInt(maxEnqueuedBatches)
      }
    }
    def toProtoString: _root_.scala.Predef.String = _root_.scalapb.TextFormat.printToUnicodeString(this)
    def companion: org.tensorflow.framework.config.BatchingOptions.type = org.tensorflow.framework.config.BatchingOptions
    // @@protoc_insertion_point(GeneratedMessage[tensorboard.BatchingOptions])
}

object BatchingOptions extends scalapb.GeneratedMessageCompanion[org.tensorflow.framework.config.BatchingOptions] {
  implicit def messageCompanion: scalapb.GeneratedMessageCompanion[org.tensorflow.framework.config.BatchingOptions] = this
  def parseFrom(`_input__`: _root_.com.google.protobuf.CodedInputStream): org.tensorflow.framework.config.BatchingOptions = {
    var __numBatchThreads: _root_.scala.Int = 0
    var __maxBatchSize: _root_.scala.Int = 0
    var __batchTimeoutMicros: _root_.scala.Int = 0
    val __allowedBatchSizes: _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Int] = new _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Int]
    var __maxEnqueuedBatches: _root_.scala.Int = 0
    var `_unknownFields__`: _root_.scalapb.UnknownFieldSet.Builder = null
    var _done__ = false
    while (!_done__) {
      val _tag__ = _input__.readTag()
      _tag__ match {
        case 0 => _done__ = true
        case 8 =>
          __numBatchThreads = _input__.readInt32()
        case 16 =>
          __maxBatchSize = _input__.readInt32()
        case 24 =>
          __batchTimeoutMicros = _input__.readInt32()
        case 32 =>
          __allowedBatchSizes += _input__.readInt32()
        case 34 => {
          val length = _input__.readRawVarint32()
          val oldLimit = _input__.pushLimit(length)
          while (_input__.getBytesUntilLimit > 0) {
            __allowedBatchSizes += _input__.readInt32()
          }
          _input__.popLimit(oldLimit)
        }
        case 40 =>
          __maxEnqueuedBatches = _input__.readInt32()
        case tag =>
          if (_unknownFields__ == null) {
            _unknownFields__ = new _root_.scalapb.UnknownFieldSet.Builder()
          }
          _unknownFields__.parseField(tag, _input__)
      }
    }
    org.tensorflow.framework.config.BatchingOptions(
        numBatchThreads = __numBatchThreads,
        maxBatchSize = __maxBatchSize,
        batchTimeoutMicros = __batchTimeoutMicros,
        allowedBatchSizes = __allowedBatchSizes.result(),
        maxEnqueuedBatches = __maxEnqueuedBatches,
        unknownFields = if (_unknownFields__ == null) _root_.scalapb.UnknownFieldSet.empty else _unknownFields__.result()
    )
  }
  implicit def messageReads: _root_.scalapb.descriptors.Reads[org.tensorflow.framework.config.BatchingOptions] = _root_.scalapb.descriptors.Reads{
    case _root_.scalapb.descriptors.PMessage(__fieldsMap) =>
      _root_.scala.Predef.require(__fieldsMap.keys.forall(_.containingMessage eq scalaDescriptor), "FieldDescriptor does not match message type.")
      org.tensorflow.framework.config.BatchingOptions(
        numBatchThreads = __fieldsMap.get(scalaDescriptor.findFieldByNumber(1).get).map(_.as[_root_.scala.Int]).getOrElse(0),
        maxBatchSize = __fieldsMap.get(scalaDescriptor.findFieldByNumber(2).get).map(_.as[_root_.scala.Int]).getOrElse(0),
        batchTimeoutMicros = __fieldsMap.get(scalaDescriptor.findFieldByNumber(3).get).map(_.as[_root_.scala.Int]).getOrElse(0),
        allowedBatchSizes = __fieldsMap.get(scalaDescriptor.findFieldByNumber(4).get).map(_.as[_root_.scala.Seq[_root_.scala.Int]]).getOrElse(_root_.scala.Seq.empty),
        maxEnqueuedBatches = __fieldsMap.get(scalaDescriptor.findFieldByNumber(5).get).map(_.as[_root_.scala.Int]).getOrElse(0)
      )
    case _ => throw new RuntimeException("Expected PMessage")
  }
  def javaDescriptor: _root_.com.google.protobuf.Descriptors.Descriptor = org.tensorflow.framework.config.ConfigProtoCompanion.javaDescriptor.getMessageTypes().get(10)
  def scalaDescriptor: _root_.scalapb.descriptors.Descriptor = org.tensorflow.framework.config.ConfigProtoCompanion.scalaDescriptor.messages(10)
  def messageCompanionForFieldNumber(__number: _root_.scala.Int): _root_.scalapb.GeneratedMessageCompanion[?]= throw new MatchError(__number)
  lazy val nestedMessagesCompanions: Seq[_root_.scalapb.GeneratedMessageCompanion[? <: _root_.scalapb.GeneratedMessage]] = Seq.empty
  def enumCompanionForFieldNumber(__fieldNumber: _root_.scala.Int): _root_.scalapb.GeneratedEnumCompanion[?]= throw new MatchError(__fieldNumber)
  lazy val defaultInstance = org.tensorflow.framework.config.BatchingOptions(
    numBatchThreads = 0,
    maxBatchSize = 0,
    batchTimeoutMicros = 0,
    allowedBatchSizes = _root_.scala.Seq.empty,
    maxEnqueuedBatches = 0
  )
  implicit class BatchingOptionsLens[UpperPB](_l: _root_.scalapb.lenses.Lens[UpperPB, org.tensorflow.framework.config.BatchingOptions]) extends _root_.scalapb.lenses.ObjectLens[UpperPB, org.tensorflow.framework.config.BatchingOptions](_l) {
    def numBatchThreads: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Int] = field(_.numBatchThreads)((c_, f_) => c_.copy(numBatchThreads = f_))
    def maxBatchSize: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Int] = field(_.maxBatchSize)((c_, f_) => c_.copy(maxBatchSize = f_))
    def batchTimeoutMicros: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Int] = field(_.batchTimeoutMicros)((c_, f_) => c_.copy(batchTimeoutMicros = f_))
    def allowedBatchSizes: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[_root_.scala.Int]] = field(_.allowedBatchSizes)((c_, f_) => c_.copy(allowedBatchSizes = f_))
    def maxEnqueuedBatches: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Int] = field(_.maxEnqueuedBatches)((c_, f_) => c_.copy(maxEnqueuedBatches = f_))
  }
  final val NUM_BATCH_THREADS_FIELD_NUMBER = 1
  final val MAX_BATCH_SIZE_FIELD_NUMBER = 2
  final val BATCH_TIMEOUT_MICROS_FIELD_NUMBER = 3
  final val ALLOWED_BATCH_SIZES_FIELD_NUMBER = 4
  final val MAX_ENQUEUED_BATCHES_FIELD_NUMBER = 5
  def of(
    numBatchThreads: _root_.scala.Int,
    maxBatchSize: _root_.scala.Int,
    batchTimeoutMicros: _root_.scala.Int,
    allowedBatchSizes: _root_.scala.Seq[_root_.scala.Int],
    maxEnqueuedBatches: _root_.scala.Int
  ): _root_.org.tensorflow.framework.config.BatchingOptions = _root_.org.tensorflow.framework.config.BatchingOptions(
    numBatchThreads,
    maxBatchSize,
    batchTimeoutMicros,
    allowedBatchSizes,
    maxEnqueuedBatches
  )
  // @@protoc_insertion_point(GeneratedMessageCompanion[tensorboard.BatchingOptions])
}
