// Generated by the Scala Plugin for the Protocol Buffer Compiler.
// Do not edit!

package tensorflow.framework.config

import tensorflow.framework.config

/** @param perProcessGpuMemoryFraction
  *   Fraction of the total GPU memory to allocate for each process.
  *   1 means to allocate all of the GPU memory, 0.5 means the process
  *   allocates up to ~50% of the total GPU memory.
  *  
  *   GPU memory is pre-allocated unless the allow_growth option is enabled.
  *  
  *   If greater than 1.0, uses CUDA unified memory to potentially oversubscribe
  *   the amount of memory available on the GPU device by using host memory as a
  *   swap space. Accessing memory not available on the device will be
  *   significantly slower as that would require memory transfer between the host
  *   and the device. Options to reduce the memory requirement should be
  *   considered before enabling this option as this may come with a negative
  *   performance impact. Oversubscription using the unified memory requires
  *   Pascal class or newer GPUs and it is currently only supported on the Linux
  *   operating system. See
  *   https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-requirements
  *   for the detailed requirements.
  * @param allowGrowth
  *   If true, the allocator does not pre-allocate the entire specified
  *   GPU memory region, instead starting small and growing as needed.
  * @param allocatorType
  *   The type of GPU allocation strategy to use.
  *  
  *   Allowed values:
  *   "": The empty string (default) uses a system-chosen default
  *       which may change over time.
  *  
  *   "BFC": A "Best-fit with coalescing" algorithm, simplified from a
  *          version of dlmalloc.
  * @param deferredDeletionBytes
  *   Delay deletion of up to this many bytes to reduce the number of
  *   interactions with gpu driver code.  If 0, the system chooses
  *   a reasonable default (several MBs).
  * @param visibleDeviceList
  *   A comma-separated list of GPU ids that determines the 'visible'
  *   to 'virtual' mapping of GPU devices.  For example, if TensorFlow
  *   can see 8 GPU devices in the process, and one wanted to map
  *   visible GPU devices 5 and 3 as "/device:GPU:0", and "/device:GPU:1",
  *   then one would specify this field as "5,3".  This field is similar in
  *   spirit to the CUDA_VISIBLE_DEVICES environment variable, except
  *   it applies to the visible GPU devices in the process.
  *  
  *   NOTE:
  *   1. The GPU driver provides the process with the visible GPUs
  *      in an order which is not guaranteed to have any correlation to
  *      the *physical* GPU id in the machine.  This field is used for
  *      remapping "visible" to "virtual", which means this operates only
  *      after the process starts.  Users are required to use vendor
  *      specific mechanisms (e.g., CUDA_VISIBLE_DEVICES) to control the
  *      physical to visible device mapping prior to invoking TensorFlow.
  *   2. In the code, the ids in this list are also called "platform GPU id"s,
  *      and the 'virtual' ids of GPU devices (i.e. the ids in the device
  *      name "/device:GPU:&lt;id&gt;") are also called "TF GPU id"s. Please
  *      refer to third_party/tensorflow/core/common_runtime/gpu/gpu_id.h
  *      for more information.
  *   3. The visible_device_list is also used for PluggableDevice. And
  *      different types of PluggableDevices share this field. In that case,
  *      the pluggable_device_type is used to distinguish them, making the
  *      visible_device_list a list of &lt;pluggable_device_type&gt;:&lt;device_index&gt;,
  *      e.g. "PluggableDeviceA:0,PluggableDeviceA:1,PluggableDeviceB:0".
  * @param pollingActiveDelayUsecs
  *   In the event polling loop sleep this many microseconds between
  *   PollEvents calls, when the queue is not empty.  If value is not
  *   set or set to 0, gets set to a non-zero default.
  * @param pollingInactiveDelayMsecs
  *   This field is deprecated and ignored.
  * @param forceGpuCompatible
  *   Force all tensors to be gpu_compatible. On a GPU-enabled TensorFlow,
  *   enabling this option forces all CPU tensors to be allocated with Cuda
  *   pinned memory. Normally, TensorFlow will infer which tensors should be
  *   allocated as the pinned memory. But in case where the inference is
  *   incomplete, this option can significantly speed up the cross-device memory
  *   copy performance as long as it fits the memory.
  *   Note that this option is not something that should be
  *   enabled by default for unknown or very large models, since all Cuda pinned
  *   memory is unpageable, having too much pinned memory might negatively impact
  *   the overall host system performance.
  * @param experimental
  *   Everything inside experimental is subject to change and is not subject
  *   to API stability guarantees in
  *   https://www.tensorflow.org/guide/versions.
  */
@SerialVersionUID(0L)
final case class GPUOptions(
                             perProcessGpuMemoryFraction: _root_.scala.Double = 0.0,
                             allowGrowth: _root_.scala.Boolean = false,
                             allocatorType: _root_.scala.Predef.String = "",
                             deferredDeletionBytes: _root_.scala.Long = 0L,
                             visibleDeviceList: _root_.scala.Predef.String = "",
                             pollingActiveDelayUsecs: _root_.scala.Int = 0,
                             pollingInactiveDelayMsecs: _root_.scala.Int = 0,
                             forceGpuCompatible: _root_.scala.Boolean = false,
                             experimental: _root_.scala.Option[GPUOptions.Experimental] = _root_.scala.None,
                             unknownFields: _root_.scalapb.UnknownFieldSet = _root_.scalapb.UnknownFieldSet.empty
    ) extends scalapb.GeneratedMessage with scalapb.lenses.Updatable[GPUOptions] {
    @transient
    private[this] var __serializedSizeMemoized: _root_.scala.Int = 0
    private[this] def __computeSerializedSize(): _root_.scala.Int = {
      var __size = 0
      
      {
        val __value = perProcessGpuMemoryFraction
        if (__value != 0.0) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeDoubleSize(1, __value)
        }
      };
      
      {
        val __value = allowGrowth
        if (__value != false) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(4, __value)
        }
      };
      
      {
        val __value = allocatorType
        if (!__value.isEmpty) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(2, __value)
        }
      };
      
      {
        val __value = deferredDeletionBytes
        if (__value != 0L) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeInt64Size(3, __value)
        }
      };
      
      {
        val __value = visibleDeviceList
        if (!__value.isEmpty) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(5, __value)
        }
      };
      
      {
        val __value = pollingActiveDelayUsecs
        if (__value != 0) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeInt32Size(6, __value)
        }
      };
      
      {
        val __value = pollingInactiveDelayMsecs
        if (__value != 0) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeInt32Size(7, __value)
        }
      };
      
      {
        val __value = forceGpuCompatible
        if (__value != false) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(8, __value)
        }
      };
      if (experimental.isDefined) {
        val __value = experimental.get
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
      };
      __size += unknownFields.serializedSize
      __size
    }
    override def serializedSize: _root_.scala.Int = {
      var __size = __serializedSizeMemoized
      if (__size == 0) {
        __size = __computeSerializedSize() + 1
        __serializedSizeMemoized = __size
      }
      __size - 1
      
    }
    def writeTo(`_output__`: _root_.com.google.protobuf.CodedOutputStream): _root_.scala.Unit = {
      {
        val __v = perProcessGpuMemoryFraction
        if (__v != 0.0) {
          _output__.writeDouble(1, __v)
        }
      };
      {
        val __v = allocatorType
        if (!__v.isEmpty) {
          _output__.writeString(2, __v)
        }
      };
      {
        val __v = deferredDeletionBytes
        if (__v != 0L) {
          _output__.writeInt64(3, __v)
        }
      };
      {
        val __v = allowGrowth
        if (__v != false) {
          _output__.writeBool(4, __v)
        }
      };
      {
        val __v = visibleDeviceList
        if (!__v.isEmpty) {
          _output__.writeString(5, __v)
        }
      };
      {
        val __v = pollingActiveDelayUsecs
        if (__v != 0) {
          _output__.writeInt32(6, __v)
        }
      };
      {
        val __v = pollingInactiveDelayMsecs
        if (__v != 0) {
          _output__.writeInt32(7, __v)
        }
      };
      {
        val __v = forceGpuCompatible
        if (__v != false) {
          _output__.writeBool(8, __v)
        }
      };
      experimental.foreach { __v =>
        val __m = __v
        _output__.writeTag(9, 2)
        _output__.writeUInt32NoTag(__m.serializedSize)
        __m.writeTo(_output__)
      };
      unknownFields.writeTo(_output__)
    }
    def withPerProcessGpuMemoryFraction(__v: _root_.scala.Double): GPUOptions = copy(perProcessGpuMemoryFraction = __v)
    def withAllowGrowth(__v: _root_.scala.Boolean): GPUOptions = copy(allowGrowth = __v)
    def withAllocatorType(__v: _root_.scala.Predef.String): GPUOptions = copy(allocatorType = __v)
    def withDeferredDeletionBytes(__v: _root_.scala.Long): GPUOptions = copy(deferredDeletionBytes = __v)
    def withVisibleDeviceList(__v: _root_.scala.Predef.String): GPUOptions = copy(visibleDeviceList = __v)
    def withPollingActiveDelayUsecs(__v: _root_.scala.Int): GPUOptions = copy(pollingActiveDelayUsecs = __v)
    def withPollingInactiveDelayMsecs(__v: _root_.scala.Int): GPUOptions = copy(pollingInactiveDelayMsecs = __v)
    def withForceGpuCompatible(__v: _root_.scala.Boolean): GPUOptions = copy(forceGpuCompatible = __v)
    def getExperimental: GPUOptions.Experimental = experimental.getOrElse(config.GPUOptions.Experimental.defaultInstance)
    def clearExperimental: GPUOptions = copy(experimental = _root_.scala.None)
    def withExperimental(__v: GPUOptions.Experimental): GPUOptions = copy(experimental = Option(__v))
    def withUnknownFields(__v: _root_.scalapb.UnknownFieldSet) = copy(unknownFields = __v)
    def discardUnknownFields = copy(unknownFields = _root_.scalapb.UnknownFieldSet.empty)
    def getFieldByNumber(__fieldNumber: _root_.scala.Int): _root_.scala.Any = {
      (__fieldNumber: @_root_.scala.unchecked) match {
        case 1 => {
          val __t = perProcessGpuMemoryFraction
          if (__t != 0.0) __t else null
        }
        case 4 => {
          val __t = allowGrowth
          if (__t != false) __t else null
        }
        case 2 => {
          val __t = allocatorType
          if (__t != "") __t else null
        }
        case 3 => {
          val __t = deferredDeletionBytes
          if (__t != 0L) __t else null
        }
        case 5 => {
          val __t = visibleDeviceList
          if (__t != "") __t else null
        }
        case 6 => {
          val __t = pollingActiveDelayUsecs
          if (__t != 0) __t else null
        }
        case 7 => {
          val __t = pollingInactiveDelayMsecs
          if (__t != 0) __t else null
        }
        case 8 => {
          val __t = forceGpuCompatible
          if (__t != false) __t else null
        }
        case 9 => experimental.orNull
      }
    }
    def getField(__field: _root_.scalapb.descriptors.FieldDescriptor): _root_.scalapb.descriptors.PValue = {
      _root_.scala.Predef.require(__field.containingMessage eq companion.scalaDescriptor)
      (__field.number: @_root_.scala.unchecked) match {
        case 1 => _root_.scalapb.descriptors.PDouble(perProcessGpuMemoryFraction)
        case 4 => _root_.scalapb.descriptors.PBoolean(allowGrowth)
        case 2 => _root_.scalapb.descriptors.PString(allocatorType)
        case 3 => _root_.scalapb.descriptors.PLong(deferredDeletionBytes)
        case 5 => _root_.scalapb.descriptors.PString(visibleDeviceList)
        case 6 => _root_.scalapb.descriptors.PInt(pollingActiveDelayUsecs)
        case 7 => _root_.scalapb.descriptors.PInt(pollingInactiveDelayMsecs)
        case 8 => _root_.scalapb.descriptors.PBoolean(forceGpuCompatible)
        case 9 => experimental.map(_.toPMessage).getOrElse(_root_.scalapb.descriptors.PEmpty)
      }
    }
    def toProtoString: _root_.scala.Predef.String = _root_.scalapb.TextFormat.printToUnicodeString(this)
    def companion: GPUOptions.type = config.GPUOptions
    // @@protoc_insertion_point(GeneratedMessage[tensorboard.GPUOptions])
}

object GPUOptions extends scalapb.GeneratedMessageCompanion[GPUOptions] {
  implicit def messageCompanion: scalapb.GeneratedMessageCompanion[GPUOptions] = this
  def parseFrom(`_input__`: _root_.com.google.protobuf.CodedInputStream): GPUOptions = {
    var __perProcessGpuMemoryFraction: _root_.scala.Double = 0.0
    var __allowGrowth: _root_.scala.Boolean = false
    var __allocatorType: _root_.scala.Predef.String = ""
    var __deferredDeletionBytes: _root_.scala.Long = 0L
    var __visibleDeviceList: _root_.scala.Predef.String = ""
    var __pollingActiveDelayUsecs: _root_.scala.Int = 0
    var __pollingInactiveDelayMsecs: _root_.scala.Int = 0
    var __forceGpuCompatible: _root_.scala.Boolean = false
    var __experimental: _root_.scala.Option[GPUOptions.Experimental] = _root_.scala.None
    var `_unknownFields__`: _root_.scalapb.UnknownFieldSet.Builder = null
    var _done__ = false
    while (!_done__) {
      val _tag__ = _input__.readTag()
      _tag__ match {
        case 0 => _done__ = true
        case 9 =>
          __perProcessGpuMemoryFraction = _input__.readDouble()
        case 32 =>
          __allowGrowth = _input__.readBool()
        case 18 =>
          __allocatorType = _input__.readStringRequireUtf8()
        case 24 =>
          __deferredDeletionBytes = _input__.readInt64()
        case 42 =>
          __visibleDeviceList = _input__.readStringRequireUtf8()
        case 48 =>
          __pollingActiveDelayUsecs = _input__.readInt32()
        case 56 =>
          __pollingInactiveDelayMsecs = _input__.readInt32()
        case 64 =>
          __forceGpuCompatible = _input__.readBool()
        case 74 =>
          __experimental = _root_.scala.Option(__experimental.fold(_root_.scalapb.LiteParser.readMessage[GPUOptions.Experimental](_input__))(_root_.scalapb.LiteParser.readMessage(_input__, _)))
        case tag =>
          if (_unknownFields__ == null) {
            _unknownFields__ = new _root_.scalapb.UnknownFieldSet.Builder()
          }
          _unknownFields__.parseField(tag, _input__)
      }
    }
    GPUOptions(
        perProcessGpuMemoryFraction = __perProcessGpuMemoryFraction,
        allowGrowth = __allowGrowth,
        allocatorType = __allocatorType,
        deferredDeletionBytes = __deferredDeletionBytes,
        visibleDeviceList = __visibleDeviceList,
        pollingActiveDelayUsecs = __pollingActiveDelayUsecs,
        pollingInactiveDelayMsecs = __pollingInactiveDelayMsecs,
        forceGpuCompatible = __forceGpuCompatible,
        experimental = __experimental,
        unknownFields = if (_unknownFields__ == null) _root_.scalapb.UnknownFieldSet.empty else _unknownFields__.result()
    )
  }
  implicit def messageReads: _root_.scalapb.descriptors.Reads[GPUOptions] = _root_.scalapb.descriptors.Reads{
    case _root_.scalapb.descriptors.PMessage(__fieldsMap) =>
      _root_.scala.Predef.require(__fieldsMap.keys.forall(_.containingMessage eq scalaDescriptor), "FieldDescriptor does not match message type.")
      GPUOptions(
        perProcessGpuMemoryFraction = __fieldsMap.get(scalaDescriptor.findFieldByNumber(1).get).map(_.as[_root_.scala.Double]).getOrElse(0.0),
        allowGrowth = __fieldsMap.get(scalaDescriptor.findFieldByNumber(4).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
        allocatorType = __fieldsMap.get(scalaDescriptor.findFieldByNumber(2).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
        deferredDeletionBytes = __fieldsMap.get(scalaDescriptor.findFieldByNumber(3).get).map(_.as[_root_.scala.Long]).getOrElse(0L),
        visibleDeviceList = __fieldsMap.get(scalaDescriptor.findFieldByNumber(5).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
        pollingActiveDelayUsecs = __fieldsMap.get(scalaDescriptor.findFieldByNumber(6).get).map(_.as[_root_.scala.Int]).getOrElse(0),
        pollingInactiveDelayMsecs = __fieldsMap.get(scalaDescriptor.findFieldByNumber(7).get).map(_.as[_root_.scala.Int]).getOrElse(0),
        forceGpuCompatible = __fieldsMap.get(scalaDescriptor.findFieldByNumber(8).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
        experimental = __fieldsMap.get(scalaDescriptor.findFieldByNumber(9).get).flatMap(_.as[_root_.scala.Option[GPUOptions.Experimental]])
      )
    case _ => throw new RuntimeException("Expected PMessage")
  }
  def javaDescriptor: _root_.com.google.protobuf.Descriptors.Descriptor = org.tensorflow.framework.config.ConfigProtoCompanion.javaDescriptor.getMessageTypes().get(0)
  def scalaDescriptor: _root_.scalapb.descriptors.Descriptor = org.tensorflow.framework.config.ConfigProtoCompanion.scalaDescriptor.messages(0)
  def messageCompanionForFieldNumber(__number: _root_.scala.Int): _root_.scalapb.GeneratedMessageCompanion[_] = {
    var __out: _root_.scalapb.GeneratedMessageCompanion[_] = null
    (__number: @_root_.scala.unchecked) match {
      case 9 => __out = config.GPUOptions.Experimental
    }
    __out
  }
  lazy val nestedMessagesCompanions: Seq[_root_.scalapb.GeneratedMessageCompanion[_ <: _root_.scalapb.GeneratedMessage]] =
    Seq[_root_.scalapb.GeneratedMessageCompanion[_ <: _root_.scalapb.GeneratedMessage]](
      config.GPUOptions.Experimental
    )
  def enumCompanionForFieldNumber(__fieldNumber: _root_.scala.Int): _root_.scalapb.GeneratedEnumCompanion[_] = throw new MatchError(__fieldNumber)
  lazy val defaultInstance = GPUOptions(
    perProcessGpuMemoryFraction = 0.0,
    allowGrowth = false,
    allocatorType = "",
    deferredDeletionBytes = 0L,
    visibleDeviceList = "",
    pollingActiveDelayUsecs = 0,
    pollingInactiveDelayMsecs = 0,
    forceGpuCompatible = false,
    experimental = _root_.scala.None
  )
  /** @param virtualDevices
    *   The multi virtual device settings. If empty (not set), it will create
    *   single virtual device on each visible GPU, according to the settings
    *   in "visible_device_list" above. Otherwise, the number of elements in the
    *   list must be the same as the number of visible GPUs (after
    *   "visible_device_list" filtering if it is set), and the string represented
    *   device names (e.g. /device:GPU:&lt;id&gt;) will refer to the virtual
    *   devices and have the &lt;id&gt; field assigned sequentially starting from 0,
    *   according to the order of the virtual devices determined by
    *   device_ordinal and the location in the virtual device list.
    *  
    *   For example,
    *     visible_device_list = "1,0"
    *     virtual_devices { memory_limit: 1GB memory_limit: 2GB }
    *     virtual_devices { memory_limit: 3GB memory_limit: 4GB }
    *   will create 4 virtual devices as:
    *     /device:GPU:0 -&gt; visible GPU 1 with 1GB memory
    *     /device:GPU:1 -&gt; visible GPU 1 with 2GB memory
    *     /device:GPU:2 -&gt; visible GPU 0 with 3GB memory
    *     /device:GPU:3 -&gt; visible GPU 0 with 4GB memory
    *  
    *   but
    *     visible_device_list = "1,0"
    *     virtual_devices { memory_limit: 1GB memory_limit: 2GB
    *                       device_ordinal: 10 device_ordinal: 20}
    *     virtual_devices { memory_limit: 3GB memory_limit: 4GB
    *                       device_ordinal: 10 device_ordinal: 20}
    *   will create 4 virtual devices as:
    *     /device:GPU:0 -&gt; visible GPU 1 with 1GB memory  (ordinal 10)
    *     /device:GPU:1 -&gt; visible GPU 0 with 3GB memory  (ordinal 10)
    *     /device:GPU:2 -&gt; visible GPU 1 with 2GB memory  (ordinal 20)
    *     /device:GPU:3 -&gt; visible GPU 0 with 4GB memory  (ordinal 20)
    *  
    *   NOTE:
    *   1. It's invalid to set both this and "per_process_gpu_memory_fraction"
    *      at the same time.
    *   2. Currently this setting is per-process, not per-session. Using
    *      different settings in different sessions within same process will
    *      result in undefined behavior.
    * @param numVirtualDevicesPerGpu
    *   The number of virtual devices to create on each visible GPU. The
    *   available memory will be split equally among all virtual devices. If the
    *   field `memory_limit_mb` in `VirtualDevices` is not empty, this field will
    *   be ignored.
    * @param useUnifiedMemory
    *   If true, uses CUDA unified memory for memory allocations. If
    *   per_process_gpu_memory_fraction option is greater than 1.0, then unified
    *   memory is used regardless of the value for this field. See comments for
    *   per_process_gpu_memory_fraction field for more details and requirements
    *   of the unified memory. This option is useful to oversubscribe memory if
    *   multiple processes are sharing a single GPU while individually using less
    *   than 1.0 per process memory fraction.
    * @param numDevToDevCopyStreams
    *   If &gt; 1, the number of device-to-device copy streams to create
    *   for each GPUDevice.  Default value is 0, which is automatically
    *   converted to 1.
    * @param collectiveRingOrder
    *   If non-empty, defines a good GPU ring order on a single worker based on
    *   device interconnect.  This assumes that all workers have the same GPU
    *   topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
    *   This ring order is used by the RingReducer implementation of
    *   CollectiveReduce, and serves as an override to automatic ring order
    *   generation in OrderTaskDeviceMap() during CollectiveParam resolution.
    * @param timestampedAllocator
    *   If true then extra work is done by GPUDevice and GPUBFCAllocator to
    *   keep track of when GPU memory is freed and when kernels actually
    *   complete so that we can know when a nominally free memory chunk
    *   is really not subject to pending use.
    * @param kernelTrackerMaxInterval
    *   Parameters for GPUKernelTracker.  By default no kernel tracking is done.
    *   Note that timestamped_allocator is only effective if some tracking is
    *   specified.
    *  
    *   If kernel_tracker_max_interval = n &gt; 0, then a tracking event
    *   is inserted after every n kernels without an event.
    * @param kernelTrackerMaxBytes
    *   If kernel_tracker_max_bytes = n &gt; 0, then a tracking event is
    *   inserted after every series of kernels allocating a sum of
    *   memory &gt;= n.  If one kernel allocates b * n bytes, then one
    *   event will be inserted after it, but it will count as b against
    *   the pending limit.
    * @param kernelTrackerMaxPending
    *   If kernel_tracker_max_pending &gt; 0 then no more than this many
    *   tracking events can be outstanding at a time.  An attempt to
    *   launch an additional kernel will stall until an event
    *   completes.
    * @param internalFragmentationFraction
    *   BFC Allocator can return an allocated chunk of memory upto 2x the
    *   requested size. For virtual devices with tight memory constraints, and
    *   proportionately large allocation requests, this can lead to a significant
    *   reduction in available memory. The threshold below controls when a chunk
    *   should be split if the chunk size exceeds requested memory size. It is
    *   expressed as a fraction of total available memory for the tf device. For
    *   example setting it to 0.05 would imply a chunk needs to be split if its
    *   size exceeds the requested memory by 5% of the total virtual device/gpu
    *   memory size.
    * @param useCudaMallocAsync
    *   When true, use CUDA cudaMallocAsync API instead of TF gpu allocator.
    * @param disallowRetryOnAllocationFailure
    *   By default, BFCAllocator may sleep when it runs out of memory, in the
    *   hopes that another thread will free up memory in the meantime.  Setting
    *   this to true disables the sleep; instead we'll OOM immediately.
    * @param gpuHostMemLimitInMb
    *   Memory limit for "GPU host allocator", aka pinned memory allocator.  This
    *   can also be set via the envvar TF_GPU_HOST_MEM_LIMIT_IN_MB.
    * @param gpuHostMemDisallowGrowth
    *   If true, then the host allocator allocates its max memory all upfront and
    *   never grows.  This can be useful for latency-sensitive systems, because
    *   growing the GPU host memory pool can be expensive.
    *  
    *   You probably only want to use this in combination with
    *   gpu_host_mem_limit_in_mb, because the default GPU host memory limit is
    *   quite high.
    * @param gpuSystemMemorySizeInMb
    *   Memory limit for gpu system. This can also be set by
    *   TF_DEVICE_MIN_SYS_MEMORY_IN_MB, which takes precedence over
    *   gpu_system_memory_size_in_mb. With this, user can configure the gpu
    *   system memory size for better resource estimation of multi-tenancy(one
    *   gpu with multiple model) use case.
    * @param populatePjrtGpuClientCreationInfo
    *   If true, save information needed for created a PjRt GPU client for
    *   creating a client with remote devices.
    * @param nodeId
    *   node_id for use when creating a PjRt GPU client with remote devices,
    *   which enumerates jobs*tasks from a ServerDef.
    */
  @SerialVersionUID(0L)
  final case class Experimental(
                                 virtualDevices: _root_.scala.Seq[GPUOptions.Experimental.VirtualDevices] = _root_.scala.Seq.empty,
                                 numVirtualDevicesPerGpu: _root_.scala.Int = 0,
                                 useUnifiedMemory: _root_.scala.Boolean = false,
                                 numDevToDevCopyStreams: _root_.scala.Int = 0,
                                 collectiveRingOrder: _root_.scala.Predef.String = "",
                                 timestampedAllocator: _root_.scala.Boolean = false,
                                 kernelTrackerMaxInterval: _root_.scala.Int = 0,
                                 kernelTrackerMaxBytes: _root_.scala.Int = 0,
                                 kernelTrackerMaxPending: _root_.scala.Int = 0,
                                 internalFragmentationFraction: _root_.scala.Double = 0.0,
                                 useCudaMallocAsync: _root_.scala.Boolean = false,
                                 disallowRetryOnAllocationFailure: _root_.scala.Boolean = false,
                                 gpuHostMemLimitInMb: _root_.scala.Float = 0.0f,
                                 gpuHostMemDisallowGrowth: _root_.scala.Boolean = false,
                                 gpuSystemMemorySizeInMb: _root_.scala.Int = 0,
                                 populatePjrtGpuClientCreationInfo: _root_.scala.Boolean = false,
                                 nodeId: _root_.scala.Int = 0,
                                 streamMergeOptions: _root_.scala.Option[GPUOptions.Experimental.StreamMergeOptions] = _root_.scala.None,
                                 unknownFields: _root_.scalapb.UnknownFieldSet = _root_.scalapb.UnknownFieldSet.empty
      ) extends scalapb.GeneratedMessage with scalapb.lenses.Updatable[Experimental] {
      @transient
      private[this] var __serializedSizeMemoized: _root_.scala.Int = 0
      private[this] def __computeSerializedSize(): _root_.scala.Int = {
        var __size = 0
        virtualDevices.foreach { __item =>
          val __value = __item
          __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
        }
        
        {
          val __value = numVirtualDevicesPerGpu
          if (__value != 0) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeInt32Size(15, __value)
          }
        };
        
        {
          val __value = useUnifiedMemory
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(2, __value)
          }
        };
        
        {
          val __value = numDevToDevCopyStreams
          if (__value != 0) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeInt32Size(3, __value)
          }
        };
        
        {
          val __value = collectiveRingOrder
          if (!__value.isEmpty) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(4, __value)
          }
        };
        
        {
          val __value = timestampedAllocator
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(5, __value)
          }
        };
        
        {
          val __value = kernelTrackerMaxInterval
          if (__value != 0) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeInt32Size(7, __value)
          }
        };
        
        {
          val __value = kernelTrackerMaxBytes
          if (__value != 0) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeInt32Size(8, __value)
          }
        };
        
        {
          val __value = kernelTrackerMaxPending
          if (__value != 0) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeInt32Size(9, __value)
          }
        };
        
        {
          val __value = internalFragmentationFraction
          if (__value != 0.0) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeDoubleSize(10, __value)
          }
        };
        
        {
          val __value = useCudaMallocAsync
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(11, __value)
          }
        };
        
        {
          val __value = disallowRetryOnAllocationFailure
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(12, __value)
          }
        };
        
        {
          val __value = gpuHostMemLimitInMb
          if (__value != 0.0f) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeFloatSize(13, __value)
          }
        };
        
        {
          val __value = gpuHostMemDisallowGrowth
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(14, __value)
          }
        };
        
        {
          val __value = gpuSystemMemorySizeInMb
          if (__value != 0) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeInt32Size(16, __value)
          }
        };
        
        {
          val __value = populatePjrtGpuClientCreationInfo
          if (__value != false) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(17, __value)
          }
        };
        
        {
          val __value = nodeId
          if (__value != 0) {
            __size += _root_.com.google.protobuf.CodedOutputStream.computeInt32Size(18, __value)
          }
        };
        if (streamMergeOptions.isDefined) {
          val __value = streamMergeOptions.get
          __size += 2 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
        };
        __size += unknownFields.serializedSize
        __size
      }
      override def serializedSize: _root_.scala.Int = {
        var __size = __serializedSizeMemoized
        if (__size == 0) {
          __size = __computeSerializedSize() + 1
          __serializedSizeMemoized = __size
        }
        __size - 1
        
      }
      def writeTo(`_output__`: _root_.com.google.protobuf.CodedOutputStream): _root_.scala.Unit = {
        virtualDevices.foreach { __v =>
          val __m = __v
          _output__.writeTag(1, 2)
          _output__.writeUInt32NoTag(__m.serializedSize)
          __m.writeTo(_output__)
        };
        {
          val __v = useUnifiedMemory
          if (__v != false) {
            _output__.writeBool(2, __v)
          }
        };
        {
          val __v = numDevToDevCopyStreams
          if (__v != 0) {
            _output__.writeInt32(3, __v)
          }
        };
        {
          val __v = collectiveRingOrder
          if (!__v.isEmpty) {
            _output__.writeString(4, __v)
          }
        };
        {
          val __v = timestampedAllocator
          if (__v != false) {
            _output__.writeBool(5, __v)
          }
        };
        {
          val __v = kernelTrackerMaxInterval
          if (__v != 0) {
            _output__.writeInt32(7, __v)
          }
        };
        {
          val __v = kernelTrackerMaxBytes
          if (__v != 0) {
            _output__.writeInt32(8, __v)
          }
        };
        {
          val __v = kernelTrackerMaxPending
          if (__v != 0) {
            _output__.writeInt32(9, __v)
          }
        };
        {
          val __v = internalFragmentationFraction
          if (__v != 0.0) {
            _output__.writeDouble(10, __v)
          }
        };
        {
          val __v = useCudaMallocAsync
          if (__v != false) {
            _output__.writeBool(11, __v)
          }
        };
        {
          val __v = disallowRetryOnAllocationFailure
          if (__v != false) {
            _output__.writeBool(12, __v)
          }
        };
        {
          val __v = gpuHostMemLimitInMb
          if (__v != 0.0f) {
            _output__.writeFloat(13, __v)
          }
        };
        {
          val __v = gpuHostMemDisallowGrowth
          if (__v != false) {
            _output__.writeBool(14, __v)
          }
        };
        {
          val __v = numVirtualDevicesPerGpu
          if (__v != 0) {
            _output__.writeInt32(15, __v)
          }
        };
        {
          val __v = gpuSystemMemorySizeInMb
          if (__v != 0) {
            _output__.writeInt32(16, __v)
          }
        };
        {
          val __v = populatePjrtGpuClientCreationInfo
          if (__v != false) {
            _output__.writeBool(17, __v)
          }
        };
        {
          val __v = nodeId
          if (__v != 0) {
            _output__.writeInt32(18, __v)
          }
        };
        streamMergeOptions.foreach { __v =>
          val __m = __v
          _output__.writeTag(19, 2)
          _output__.writeUInt32NoTag(__m.serializedSize)
          __m.writeTo(_output__)
        };
        unknownFields.writeTo(_output__)
      }
      def clearVirtualDevices = copy(virtualDevices = _root_.scala.Seq.empty)
      def addVirtualDevices(__vs: GPUOptions.Experimental.VirtualDevices *): Experimental = addAllVirtualDevices(__vs)
      def addAllVirtualDevices(__vs: Iterable[GPUOptions.Experimental.VirtualDevices]): Experimental = copy(virtualDevices = virtualDevices ++ __vs)
      def withVirtualDevices(__v: _root_.scala.Seq[GPUOptions.Experimental.VirtualDevices]): Experimental = copy(virtualDevices = __v)
      def withNumVirtualDevicesPerGpu(__v: _root_.scala.Int): Experimental = copy(numVirtualDevicesPerGpu = __v)
      def withUseUnifiedMemory(__v: _root_.scala.Boolean): Experimental = copy(useUnifiedMemory = __v)
      def withNumDevToDevCopyStreams(__v: _root_.scala.Int): Experimental = copy(numDevToDevCopyStreams = __v)
      def withCollectiveRingOrder(__v: _root_.scala.Predef.String): Experimental = copy(collectiveRingOrder = __v)
      def withTimestampedAllocator(__v: _root_.scala.Boolean): Experimental = copy(timestampedAllocator = __v)
      def withKernelTrackerMaxInterval(__v: _root_.scala.Int): Experimental = copy(kernelTrackerMaxInterval = __v)
      def withKernelTrackerMaxBytes(__v: _root_.scala.Int): Experimental = copy(kernelTrackerMaxBytes = __v)
      def withKernelTrackerMaxPending(__v: _root_.scala.Int): Experimental = copy(kernelTrackerMaxPending = __v)
      def withInternalFragmentationFraction(__v: _root_.scala.Double): Experimental = copy(internalFragmentationFraction = __v)
      def withUseCudaMallocAsync(__v: _root_.scala.Boolean): Experimental = copy(useCudaMallocAsync = __v)
      def withDisallowRetryOnAllocationFailure(__v: _root_.scala.Boolean): Experimental = copy(disallowRetryOnAllocationFailure = __v)
      def withGpuHostMemLimitInMb(__v: _root_.scala.Float): Experimental = copy(gpuHostMemLimitInMb = __v)
      def withGpuHostMemDisallowGrowth(__v: _root_.scala.Boolean): Experimental = copy(gpuHostMemDisallowGrowth = __v)
      def withGpuSystemMemorySizeInMb(__v: _root_.scala.Int): Experimental = copy(gpuSystemMemorySizeInMb = __v)
      def withPopulatePjrtGpuClientCreationInfo(__v: _root_.scala.Boolean): Experimental = copy(populatePjrtGpuClientCreationInfo = __v)
      def withNodeId(__v: _root_.scala.Int): Experimental = copy(nodeId = __v)
      def getStreamMergeOptions: GPUOptions.Experimental.StreamMergeOptions = streamMergeOptions.getOrElse(config.GPUOptions.Experimental.StreamMergeOptions.defaultInstance)
      def clearStreamMergeOptions: Experimental = copy(streamMergeOptions = _root_.scala.None)
      def withStreamMergeOptions(__v: GPUOptions.Experimental.StreamMergeOptions): Experimental = copy(streamMergeOptions = Option(__v))
      def withUnknownFields(__v: _root_.scalapb.UnknownFieldSet) = copy(unknownFields = __v)
      def discardUnknownFields = copy(unknownFields = _root_.scalapb.UnknownFieldSet.empty)
      def getFieldByNumber(__fieldNumber: _root_.scala.Int): _root_.scala.Any = {
        (__fieldNumber: @_root_.scala.unchecked) match {
          case 1 => virtualDevices
          case 15 => {
            val __t = numVirtualDevicesPerGpu
            if (__t != 0) __t else null
          }
          case 2 => {
            val __t = useUnifiedMemory
            if (__t != false) __t else null
          }
          case 3 => {
            val __t = numDevToDevCopyStreams
            if (__t != 0) __t else null
          }
          case 4 => {
            val __t = collectiveRingOrder
            if (__t != "") __t else null
          }
          case 5 => {
            val __t = timestampedAllocator
            if (__t != false) __t else null
          }
          case 7 => {
            val __t = kernelTrackerMaxInterval
            if (__t != 0) __t else null
          }
          case 8 => {
            val __t = kernelTrackerMaxBytes
            if (__t != 0) __t else null
          }
          case 9 => {
            val __t = kernelTrackerMaxPending
            if (__t != 0) __t else null
          }
          case 10 => {
            val __t = internalFragmentationFraction
            if (__t != 0.0) __t else null
          }
          case 11 => {
            val __t = useCudaMallocAsync
            if (__t != false) __t else null
          }
          case 12 => {
            val __t = disallowRetryOnAllocationFailure
            if (__t != false) __t else null
          }
          case 13 => {
            val __t = gpuHostMemLimitInMb
            if (__t != 0.0f) __t else null
          }
          case 14 => {
            val __t = gpuHostMemDisallowGrowth
            if (__t != false) __t else null
          }
          case 16 => {
            val __t = gpuSystemMemorySizeInMb
            if (__t != 0) __t else null
          }
          case 17 => {
            val __t = populatePjrtGpuClientCreationInfo
            if (__t != false) __t else null
          }
          case 18 => {
            val __t = nodeId
            if (__t != 0) __t else null
          }
          case 19 => streamMergeOptions.orNull
        }
      }
      def getField(__field: _root_.scalapb.descriptors.FieldDescriptor): _root_.scalapb.descriptors.PValue = {
        _root_.scala.Predef.require(__field.containingMessage eq companion.scalaDescriptor)
        (__field.number: @_root_.scala.unchecked) match {
          case 1 => _root_.scalapb.descriptors.PRepeated(virtualDevices.iterator.map(_.toPMessage).toVector)
          case 15 => _root_.scalapb.descriptors.PInt(numVirtualDevicesPerGpu)
          case 2 => _root_.scalapb.descriptors.PBoolean(useUnifiedMemory)
          case 3 => _root_.scalapb.descriptors.PInt(numDevToDevCopyStreams)
          case 4 => _root_.scalapb.descriptors.PString(collectiveRingOrder)
          case 5 => _root_.scalapb.descriptors.PBoolean(timestampedAllocator)
          case 7 => _root_.scalapb.descriptors.PInt(kernelTrackerMaxInterval)
          case 8 => _root_.scalapb.descriptors.PInt(kernelTrackerMaxBytes)
          case 9 => _root_.scalapb.descriptors.PInt(kernelTrackerMaxPending)
          case 10 => _root_.scalapb.descriptors.PDouble(internalFragmentationFraction)
          case 11 => _root_.scalapb.descriptors.PBoolean(useCudaMallocAsync)
          case 12 => _root_.scalapb.descriptors.PBoolean(disallowRetryOnAllocationFailure)
          case 13 => _root_.scalapb.descriptors.PFloat(gpuHostMemLimitInMb)
          case 14 => _root_.scalapb.descriptors.PBoolean(gpuHostMemDisallowGrowth)
          case 16 => _root_.scalapb.descriptors.PInt(gpuSystemMemorySizeInMb)
          case 17 => _root_.scalapb.descriptors.PBoolean(populatePjrtGpuClientCreationInfo)
          case 18 => _root_.scalapb.descriptors.PInt(nodeId)
          case 19 => streamMergeOptions.map(_.toPMessage).getOrElse(_root_.scalapb.descriptors.PEmpty)
        }
      }
      def toProtoString: _root_.scala.Predef.String = _root_.scalapb.TextFormat.printToUnicodeString(this)
      def companion: GPUOptions.Experimental.type = config.GPUOptions.Experimental
      // @@protoc_insertion_point(GeneratedMessage[tensorboard.GPUOptions.Experimental])
  }
  
  object Experimental extends scalapb.GeneratedMessageCompanion[GPUOptions.Experimental] {
    implicit def messageCompanion: scalapb.GeneratedMessageCompanion[GPUOptions.Experimental] = this
    def parseFrom(`_input__`: _root_.com.google.protobuf.CodedInputStream): GPUOptions.Experimental = {
      val __virtualDevices: _root_.scala.collection.immutable.VectorBuilder[GPUOptions.Experimental.VirtualDevices] = new _root_.scala.collection.immutable.VectorBuilder[GPUOptions.Experimental.VirtualDevices]
      var __numVirtualDevicesPerGpu: _root_.scala.Int = 0
      var __useUnifiedMemory: _root_.scala.Boolean = false
      var __numDevToDevCopyStreams: _root_.scala.Int = 0
      var __collectiveRingOrder: _root_.scala.Predef.String = ""
      var __timestampedAllocator: _root_.scala.Boolean = false
      var __kernelTrackerMaxInterval: _root_.scala.Int = 0
      var __kernelTrackerMaxBytes: _root_.scala.Int = 0
      var __kernelTrackerMaxPending: _root_.scala.Int = 0
      var __internalFragmentationFraction: _root_.scala.Double = 0.0
      var __useCudaMallocAsync: _root_.scala.Boolean = false
      var __disallowRetryOnAllocationFailure: _root_.scala.Boolean = false
      var __gpuHostMemLimitInMb: _root_.scala.Float = 0.0f
      var __gpuHostMemDisallowGrowth: _root_.scala.Boolean = false
      var __gpuSystemMemorySizeInMb: _root_.scala.Int = 0
      var __populatePjrtGpuClientCreationInfo: _root_.scala.Boolean = false
      var __nodeId: _root_.scala.Int = 0
      var __streamMergeOptions: _root_.scala.Option[GPUOptions.Experimental.StreamMergeOptions] = _root_.scala.None
      var `_unknownFields__`: _root_.scalapb.UnknownFieldSet.Builder = null
      var _done__ = false
      while (!_done__) {
        val _tag__ = _input__.readTag()
        _tag__ match {
          case 0 => _done__ = true
          case 10 =>
            __virtualDevices += _root_.scalapb.LiteParser.readMessage[GPUOptions.Experimental.VirtualDevices](_input__)
          case 120 =>
            __numVirtualDevicesPerGpu = _input__.readInt32()
          case 16 =>
            __useUnifiedMemory = _input__.readBool()
          case 24 =>
            __numDevToDevCopyStreams = _input__.readInt32()
          case 34 =>
            __collectiveRingOrder = _input__.readStringRequireUtf8()
          case 40 =>
            __timestampedAllocator = _input__.readBool()
          case 56 =>
            __kernelTrackerMaxInterval = _input__.readInt32()
          case 64 =>
            __kernelTrackerMaxBytes = _input__.readInt32()
          case 72 =>
            __kernelTrackerMaxPending = _input__.readInt32()
          case 81 =>
            __internalFragmentationFraction = _input__.readDouble()
          case 88 =>
            __useCudaMallocAsync = _input__.readBool()
          case 96 =>
            __disallowRetryOnAllocationFailure = _input__.readBool()
          case 109 =>
            __gpuHostMemLimitInMb = _input__.readFloat()
          case 112 =>
            __gpuHostMemDisallowGrowth = _input__.readBool()
          case 128 =>
            __gpuSystemMemorySizeInMb = _input__.readInt32()
          case 136 =>
            __populatePjrtGpuClientCreationInfo = _input__.readBool()
          case 144 =>
            __nodeId = _input__.readInt32()
          case 154 =>
            __streamMergeOptions = _root_.scala.Option(__streamMergeOptions.fold(_root_.scalapb.LiteParser.readMessage[GPUOptions.Experimental.StreamMergeOptions](_input__))(_root_.scalapb.LiteParser.readMessage(_input__, _)))
          case tag =>
            if (_unknownFields__ == null) {
              _unknownFields__ = new _root_.scalapb.UnknownFieldSet.Builder()
            }
            _unknownFields__.parseField(tag, _input__)
        }
      }
      config.GPUOptions.Experimental(
          virtualDevices = __virtualDevices.result(),
          numVirtualDevicesPerGpu = __numVirtualDevicesPerGpu,
          useUnifiedMemory = __useUnifiedMemory,
          numDevToDevCopyStreams = __numDevToDevCopyStreams,
          collectiveRingOrder = __collectiveRingOrder,
          timestampedAllocator = __timestampedAllocator,
          kernelTrackerMaxInterval = __kernelTrackerMaxInterval,
          kernelTrackerMaxBytes = __kernelTrackerMaxBytes,
          kernelTrackerMaxPending = __kernelTrackerMaxPending,
          internalFragmentationFraction = __internalFragmentationFraction,
          useCudaMallocAsync = __useCudaMallocAsync,
          disallowRetryOnAllocationFailure = __disallowRetryOnAllocationFailure,
          gpuHostMemLimitInMb = __gpuHostMemLimitInMb,
          gpuHostMemDisallowGrowth = __gpuHostMemDisallowGrowth,
          gpuSystemMemorySizeInMb = __gpuSystemMemorySizeInMb,
          populatePjrtGpuClientCreationInfo = __populatePjrtGpuClientCreationInfo,
          nodeId = __nodeId,
          streamMergeOptions = __streamMergeOptions,
          unknownFields = if (_unknownFields__ == null) _root_.scalapb.UnknownFieldSet.empty else _unknownFields__.result()
      )
    }
    implicit def messageReads: _root_.scalapb.descriptors.Reads[GPUOptions.Experimental] = _root_.scalapb.descriptors.Reads{
      case _root_.scalapb.descriptors.PMessage(__fieldsMap) =>
        _root_.scala.Predef.require(__fieldsMap.keys.forall(_.containingMessage eq scalaDescriptor), "FieldDescriptor does not match message type.")
        config.GPUOptions.Experimental(
          virtualDevices = __fieldsMap.get(scalaDescriptor.findFieldByNumber(1).get).map(_.as[_root_.scala.Seq[GPUOptions.Experimental.VirtualDevices]]).getOrElse(_root_.scala.Seq.empty),
          numVirtualDevicesPerGpu = __fieldsMap.get(scalaDescriptor.findFieldByNumber(15).get).map(_.as[_root_.scala.Int]).getOrElse(0),
          useUnifiedMemory = __fieldsMap.get(scalaDescriptor.findFieldByNumber(2).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          numDevToDevCopyStreams = __fieldsMap.get(scalaDescriptor.findFieldByNumber(3).get).map(_.as[_root_.scala.Int]).getOrElse(0),
          collectiveRingOrder = __fieldsMap.get(scalaDescriptor.findFieldByNumber(4).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
          timestampedAllocator = __fieldsMap.get(scalaDescriptor.findFieldByNumber(5).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          kernelTrackerMaxInterval = __fieldsMap.get(scalaDescriptor.findFieldByNumber(7).get).map(_.as[_root_.scala.Int]).getOrElse(0),
          kernelTrackerMaxBytes = __fieldsMap.get(scalaDescriptor.findFieldByNumber(8).get).map(_.as[_root_.scala.Int]).getOrElse(0),
          kernelTrackerMaxPending = __fieldsMap.get(scalaDescriptor.findFieldByNumber(9).get).map(_.as[_root_.scala.Int]).getOrElse(0),
          internalFragmentationFraction = __fieldsMap.get(scalaDescriptor.findFieldByNumber(10).get).map(_.as[_root_.scala.Double]).getOrElse(0.0),
          useCudaMallocAsync = __fieldsMap.get(scalaDescriptor.findFieldByNumber(11).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          disallowRetryOnAllocationFailure = __fieldsMap.get(scalaDescriptor.findFieldByNumber(12).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          gpuHostMemLimitInMb = __fieldsMap.get(scalaDescriptor.findFieldByNumber(13).get).map(_.as[_root_.scala.Float]).getOrElse(0.0f),
          gpuHostMemDisallowGrowth = __fieldsMap.get(scalaDescriptor.findFieldByNumber(14).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          gpuSystemMemorySizeInMb = __fieldsMap.get(scalaDescriptor.findFieldByNumber(16).get).map(_.as[_root_.scala.Int]).getOrElse(0),
          populatePjrtGpuClientCreationInfo = __fieldsMap.get(scalaDescriptor.findFieldByNumber(17).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
          nodeId = __fieldsMap.get(scalaDescriptor.findFieldByNumber(18).get).map(_.as[_root_.scala.Int]).getOrElse(0),
          streamMergeOptions = __fieldsMap.get(scalaDescriptor.findFieldByNumber(19).get).flatMap(_.as[_root_.scala.Option[GPUOptions.Experimental.StreamMergeOptions]])
        )
      case _ => throw new RuntimeException("Expected PMessage")
    }
    def javaDescriptor: _root_.com.google.protobuf.Descriptors.Descriptor = config.GPUOptions.javaDescriptor.getNestedTypes().get(0)
    def scalaDescriptor: _root_.scalapb.descriptors.Descriptor = config.GPUOptions.scalaDescriptor.nestedMessages(0)
    def messageCompanionForFieldNumber(__number: _root_.scala.Int): _root_.scalapb.GeneratedMessageCompanion[_] = {
      var __out: _root_.scalapb.GeneratedMessageCompanion[_] = null
      (__number: @_root_.scala.unchecked) match {
        case 1 => __out = config.GPUOptions.Experimental.VirtualDevices
        case 19 => __out = config.GPUOptions.Experimental.StreamMergeOptions
      }
      __out
    }
    lazy val nestedMessagesCompanions: Seq[_root_.scalapb.GeneratedMessageCompanion[_ <: _root_.scalapb.GeneratedMessage]] =
      Seq[_root_.scalapb.GeneratedMessageCompanion[_ <: _root_.scalapb.GeneratedMessage]](
        config.GPUOptions.Experimental.VirtualDevices,
        config.GPUOptions.Experimental.StreamMergeOptions
      )
    def enumCompanionForFieldNumber(__fieldNumber: _root_.scala.Int): _root_.scalapb.GeneratedEnumCompanion[_] = throw new MatchError(__fieldNumber)
    lazy val defaultInstance = config.GPUOptions.Experimental(
      virtualDevices = _root_.scala.Seq.empty,
      numVirtualDevicesPerGpu = 0,
      useUnifiedMemory = false,
      numDevToDevCopyStreams = 0,
      collectiveRingOrder = "",
      timestampedAllocator = false,
      kernelTrackerMaxInterval = 0,
      kernelTrackerMaxBytes = 0,
      kernelTrackerMaxPending = 0,
      internalFragmentationFraction = 0.0,
      useCudaMallocAsync = false,
      disallowRetryOnAllocationFailure = false,
      gpuHostMemLimitInMb = 0.0f,
      gpuHostMemDisallowGrowth = false,
      gpuSystemMemorySizeInMb = 0,
      populatePjrtGpuClientCreationInfo = false,
      nodeId = 0,
      streamMergeOptions = _root_.scala.None
    )
    /** Configuration for breaking down a visible GPU into multiple "virtual"
      * devices.
      *
      * @param memoryLimitMb
      *   Per "virtual" device memory limit, in MB. The number of elements in
      *   the list is the number of virtual devices to create on the
      *   corresponding visible GPU (see "virtual_devices" below).
      *   If empty and `num_virtual_devices_per_gpu` is not set, it will create
      *   single virtual device taking all available memory from the device.
      *  
      *   For the concept of "visible" and "virtual" GPU, see the comments for
      *   "visible_device_list" above for more information.
      * @param priority
      *   Priority values to use with the virtual devices. Use the cuda function
      *   cudaDeviceGetStreamPriorityRange to query for valid range of values for
      *   priority.
      *  
      *   On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
      *   least priority and -1 for greatest priority.
      *  
      *   If this field is not specified, then the virtual devices will be
      *   created with the default. If this field has values set, then the size
      *   of this must match with the above memory_limit_mb.
      * @param deviceOrdinal
      *   Virtual Device ordinal number determines the device ID of the device.
      *   A Virtual device with a lower ordinal number always receives the a
      *   smaller device id. The phyiscal device id and location in the
      *   virtual device list is used to break ties.
      */
    @SerialVersionUID(0L)
    final case class VirtualDevices(
        memoryLimitMb: _root_.scala.Seq[_root_.scala.Float] = _root_.scala.Seq.empty,
        priority: _root_.scala.Seq[_root_.scala.Int] = _root_.scala.Seq.empty,
        deviceOrdinal: _root_.scala.Seq[_root_.scala.Int] = _root_.scala.Seq.empty,
        unknownFields: _root_.scalapb.UnknownFieldSet = _root_.scalapb.UnknownFieldSet.empty
        ) extends scalapb.GeneratedMessage with scalapb.lenses.Updatable[VirtualDevices] {
        private[this] def memoryLimitMbSerializedSize = {
          4 * memoryLimitMb.size
        }
        private[this] def prioritySerializedSize = {
          if (__prioritySerializedSizeField == 0) __prioritySerializedSizeField = {
            var __s: _root_.scala.Int = 0
            priority.foreach(__i => __s += _root_.com.google.protobuf.CodedOutputStream.computeInt32SizeNoTag(__i))
            __s
          }
          __prioritySerializedSizeField
        }
        @transient private[this] var __prioritySerializedSizeField: _root_.scala.Int = 0
        private[this] def deviceOrdinalSerializedSize = {
          if (__deviceOrdinalSerializedSizeField == 0) __deviceOrdinalSerializedSizeField = {
            var __s: _root_.scala.Int = 0
            deviceOrdinal.foreach(__i => __s += _root_.com.google.protobuf.CodedOutputStream.computeInt32SizeNoTag(__i))
            __s
          }
          __deviceOrdinalSerializedSizeField
        }
        @transient private[this] var __deviceOrdinalSerializedSizeField: _root_.scala.Int = 0
        @transient
        private[this] var __serializedSizeMemoized: _root_.scala.Int = 0
        private[this] def __computeSerializedSize(): _root_.scala.Int = {
          var __size = 0
          if (memoryLimitMb.nonEmpty) {
            val __localsize = memoryLimitMbSerializedSize
            __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__localsize) + __localsize
          }
          if (priority.nonEmpty) {
            val __localsize = prioritySerializedSize
            __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__localsize) + __localsize
          }
          if (deviceOrdinal.nonEmpty) {
            val __localsize = deviceOrdinalSerializedSize
            __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__localsize) + __localsize
          }
          __size += unknownFields.serializedSize
          __size
        }
        override def serializedSize: _root_.scala.Int = {
          var __size = __serializedSizeMemoized
          if (__size == 0) {
            __size = __computeSerializedSize() + 1
            __serializedSizeMemoized = __size
          }
          __size - 1
          
        }
        def writeTo(`_output__`: _root_.com.google.protobuf.CodedOutputStream): _root_.scala.Unit = {
          if (memoryLimitMb.nonEmpty) {
            _output__.writeTag(1, 2)
            _output__.writeUInt32NoTag(memoryLimitMbSerializedSize)
            memoryLimitMb.foreach(_output__.writeFloatNoTag)
          };
          if (priority.nonEmpty) {
            _output__.writeTag(2, 2)
            _output__.writeUInt32NoTag(prioritySerializedSize)
            priority.foreach(_output__.writeInt32NoTag)
          };
          if (deviceOrdinal.nonEmpty) {
            _output__.writeTag(3, 2)
            _output__.writeUInt32NoTag(deviceOrdinalSerializedSize)
            deviceOrdinal.foreach(_output__.writeInt32NoTag)
          };
          unknownFields.writeTo(_output__)
        }
        def clearMemoryLimitMb = copy(memoryLimitMb = _root_.scala.Seq.empty)
        def addMemoryLimitMb(__vs: _root_.scala.Float *): VirtualDevices = addAllMemoryLimitMb(__vs)
        def addAllMemoryLimitMb(__vs: Iterable[_root_.scala.Float]): VirtualDevices = copy(memoryLimitMb = memoryLimitMb ++ __vs)
        def withMemoryLimitMb(__v: _root_.scala.Seq[_root_.scala.Float]): VirtualDevices = copy(memoryLimitMb = __v)
        def clearPriority = copy(priority = _root_.scala.Seq.empty)
        def addPriority(__vs: _root_.scala.Int *): VirtualDevices = addAllPriority(__vs)
        def addAllPriority(__vs: Iterable[_root_.scala.Int]): VirtualDevices = copy(priority = priority ++ __vs)
        def withPriority(__v: _root_.scala.Seq[_root_.scala.Int]): VirtualDevices = copy(priority = __v)
        def clearDeviceOrdinal = copy(deviceOrdinal = _root_.scala.Seq.empty)
        def addDeviceOrdinal(__vs: _root_.scala.Int *): VirtualDevices = addAllDeviceOrdinal(__vs)
        def addAllDeviceOrdinal(__vs: Iterable[_root_.scala.Int]): VirtualDevices = copy(deviceOrdinal = deviceOrdinal ++ __vs)
        def withDeviceOrdinal(__v: _root_.scala.Seq[_root_.scala.Int]): VirtualDevices = copy(deviceOrdinal = __v)
        def withUnknownFields(__v: _root_.scalapb.UnknownFieldSet) = copy(unknownFields = __v)
        def discardUnknownFields = copy(unknownFields = _root_.scalapb.UnknownFieldSet.empty)
        def getFieldByNumber(__fieldNumber: _root_.scala.Int): _root_.scala.Any = {
          (__fieldNumber: @_root_.scala.unchecked) match {
            case 1 => memoryLimitMb
            case 2 => priority
            case 3 => deviceOrdinal
          }
        }
        def getField(__field: _root_.scalapb.descriptors.FieldDescriptor): _root_.scalapb.descriptors.PValue = {
          _root_.scala.Predef.require(__field.containingMessage eq companion.scalaDescriptor)
          (__field.number: @_root_.scala.unchecked) match {
            case 1 => _root_.scalapb.descriptors.PRepeated(memoryLimitMb.iterator.map(_root_.scalapb.descriptors.PFloat(_)).toVector)
            case 2 => _root_.scalapb.descriptors.PRepeated(priority.iterator.map(_root_.scalapb.descriptors.PInt(_)).toVector)
            case 3 => _root_.scalapb.descriptors.PRepeated(deviceOrdinal.iterator.map(_root_.scalapb.descriptors.PInt(_)).toVector)
          }
        }
        def toProtoString: _root_.scala.Predef.String = _root_.scalapb.TextFormat.printToUnicodeString(this)
        def companion: GPUOptions.Experimental.VirtualDevices.type = config.GPUOptions.Experimental.VirtualDevices
        // @@protoc_insertion_point(GeneratedMessage[tensorboard.GPUOptions.Experimental.VirtualDevices])
    }
    
    object VirtualDevices extends scalapb.GeneratedMessageCompanion[GPUOptions.Experimental.VirtualDevices] {
      implicit def messageCompanion: scalapb.GeneratedMessageCompanion[GPUOptions.Experimental.VirtualDevices] = this
      def parseFrom(`_input__`: _root_.com.google.protobuf.CodedInputStream): GPUOptions.Experimental.VirtualDevices = {
        val __memoryLimitMb: _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Float] = new _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Float]
        val __priority: _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Int] = new _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Int]
        val __deviceOrdinal: _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Int] = new _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Int]
        var `_unknownFields__`: _root_.scalapb.UnknownFieldSet.Builder = null
        var _done__ = false
        while (!_done__) {
          val _tag__ = _input__.readTag()
          _tag__ match {
            case 0 => _done__ = true
            case 13 =>
              __memoryLimitMb += _input__.readFloat()
            case 10 => {
              val length = _input__.readRawVarint32()
              val oldLimit = _input__.pushLimit(length)
              while (_input__.getBytesUntilLimit > 0) {
                __memoryLimitMb += _input__.readFloat()
              }
              _input__.popLimit(oldLimit)
            }
            case 16 =>
              __priority += _input__.readInt32()
            case 18 => {
              val length = _input__.readRawVarint32()
              val oldLimit = _input__.pushLimit(length)
              while (_input__.getBytesUntilLimit > 0) {
                __priority += _input__.readInt32()
              }
              _input__.popLimit(oldLimit)
            }
            case 24 =>
              __deviceOrdinal += _input__.readInt32()
            case 26 => {
              val length = _input__.readRawVarint32()
              val oldLimit = _input__.pushLimit(length)
              while (_input__.getBytesUntilLimit > 0) {
                __deviceOrdinal += _input__.readInt32()
              }
              _input__.popLimit(oldLimit)
            }
            case tag =>
              if (_unknownFields__ == null) {
                _unknownFields__ = new _root_.scalapb.UnknownFieldSet.Builder()
              }
              _unknownFields__.parseField(tag, _input__)
          }
        }
        config.GPUOptions.Experimental.VirtualDevices(
            memoryLimitMb = __memoryLimitMb.result(),
            priority = __priority.result(),
            deviceOrdinal = __deviceOrdinal.result(),
            unknownFields = if (_unknownFields__ == null) _root_.scalapb.UnknownFieldSet.empty else _unknownFields__.result()
        )
      }
      implicit def messageReads: _root_.scalapb.descriptors.Reads[GPUOptions.Experimental.VirtualDevices] = _root_.scalapb.descriptors.Reads{
        case _root_.scalapb.descriptors.PMessage(__fieldsMap) =>
          _root_.scala.Predef.require(__fieldsMap.keys.forall(_.containingMessage eq scalaDescriptor), "FieldDescriptor does not match message type.")
          config.GPUOptions.Experimental.VirtualDevices(
            memoryLimitMb = __fieldsMap.get(scalaDescriptor.findFieldByNumber(1).get).map(_.as[_root_.scala.Seq[_root_.scala.Float]]).getOrElse(_root_.scala.Seq.empty),
            priority = __fieldsMap.get(scalaDescriptor.findFieldByNumber(2).get).map(_.as[_root_.scala.Seq[_root_.scala.Int]]).getOrElse(_root_.scala.Seq.empty),
            deviceOrdinal = __fieldsMap.get(scalaDescriptor.findFieldByNumber(3).get).map(_.as[_root_.scala.Seq[_root_.scala.Int]]).getOrElse(_root_.scala.Seq.empty)
          )
        case _ => throw new RuntimeException("Expected PMessage")
      }
      def javaDescriptor: _root_.com.google.protobuf.Descriptors.Descriptor = config.GPUOptions.Experimental.javaDescriptor.getNestedTypes().get(0)
      def scalaDescriptor: _root_.scalapb.descriptors.Descriptor = config.GPUOptions.Experimental.scalaDescriptor.nestedMessages(0)
      def messageCompanionForFieldNumber(__number: _root_.scala.Int): _root_.scalapb.GeneratedMessageCompanion[_] = throw new MatchError(__number)
      lazy val nestedMessagesCompanions: Seq[_root_.scalapb.GeneratedMessageCompanion[_ <: _root_.scalapb.GeneratedMessage]] = Seq.empty
      def enumCompanionForFieldNumber(__fieldNumber: _root_.scala.Int): _root_.scalapb.GeneratedEnumCompanion[_] = throw new MatchError(__fieldNumber)
      lazy val defaultInstance = config.GPUOptions.Experimental.VirtualDevices(
        memoryLimitMb = _root_.scala.Seq.empty,
        priority = _root_.scala.Seq.empty,
        deviceOrdinal = _root_.scala.Seq.empty
      )
      implicit class VirtualDevicesLens[UpperPB](_l: _root_.scalapb.lenses.Lens[UpperPB, GPUOptions.Experimental.VirtualDevices]) extends _root_.scalapb.lenses.ObjectLens[UpperPB, GPUOptions.Experimental.VirtualDevices](_l) {
        def memoryLimitMb: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[_root_.scala.Float]] = field(_.memoryLimitMb)((c_, f_) => c_.copy(memoryLimitMb = f_))
        def priority: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[_root_.scala.Int]] = field(_.priority)((c_, f_) => c_.copy(priority = f_))
        def deviceOrdinal: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[_root_.scala.Int]] = field(_.deviceOrdinal)((c_, f_) => c_.copy(deviceOrdinal = f_))
      }
      final val MEMORY_LIMIT_MB_FIELD_NUMBER = 1
      final val PRIORITY_FIELD_NUMBER = 2
      final val DEVICE_ORDINAL_FIELD_NUMBER = 3
      def of(
        memoryLimitMb: _root_.scala.Seq[_root_.scala.Float],
        priority: _root_.scala.Seq[_root_.scala.Int],
        deviceOrdinal: _root_.scala.Seq[_root_.scala.Int]
      ): GPUOptions.Experimental.VirtualDevices = config.GPUOptions.Experimental.VirtualDevices(
        memoryLimitMb,
        priority,
        deviceOrdinal
      )
      // @@protoc_insertion_point(GeneratedMessageCompanion[tensorboard.GPUOptions.Experimental.VirtualDevices])
    }
    
    /** Whether to merge data transfer streams into the compute stream in the
      * same stream group. Stream merging helps reduce the overhead caused by
      * stream synchronization, especially when data transfers are frequent. For
      * example, setting "merge_host_to_device_stream = true" will make the
      * compute stream responsible for both computation and host to device memory
      * copy.
      *
      * @param mergeHostToDeviceStream
      *   If true, the compute stream will be used for host_to_device copy as
      *   well. It's no longer necessary to record an event before the copy to
      *   let the copy stream wait for the compute stream to finish. There is
      *   also no need to wait for the copy to complete before executing the
      *   callback function.
      * @param mergeDeviceToHostStream
      *   If true, the compute stream will be used for device_to_host copy as
      *   well. It's no longer necessary to record an event before the copy to
      *   let the copy stream wait for the compute stream to finish.
      * @param mergeDeviceToDeviceStream
      *   If true, the compute stream will be used for device_to_device copy as
      *   well. It's no longer necessary to record an event before the copy to
      *   let the copy stream wait for the compute stream of the sending device
      *   to finish. There is also no need to wait for the compute stream of the
      *   receiving device to finish if the copy is within the same device.
      */
    @SerialVersionUID(0L)
    final case class StreamMergeOptions(
        mergeHostToDeviceStream: _root_.scala.Boolean = false,
        mergeDeviceToHostStream: _root_.scala.Boolean = false,
        mergeDeviceToDeviceStream: _root_.scala.Boolean = false,
        unknownFields: _root_.scalapb.UnknownFieldSet = _root_.scalapb.UnknownFieldSet.empty
        ) extends scalapb.GeneratedMessage with scalapb.lenses.Updatable[StreamMergeOptions] {
        @transient
        private[this] var __serializedSizeMemoized: _root_.scala.Int = 0
        private[this] def __computeSerializedSize(): _root_.scala.Int = {
          var __size = 0
          
          {
            val __value = mergeHostToDeviceStream
            if (__value != false) {
              __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(1, __value)
            }
          };
          
          {
            val __value = mergeDeviceToHostStream
            if (__value != false) {
              __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(2, __value)
            }
          };
          
          {
            val __value = mergeDeviceToDeviceStream
            if (__value != false) {
              __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(3, __value)
            }
          };
          __size += unknownFields.serializedSize
          __size
        }
        override def serializedSize: _root_.scala.Int = {
          var __size = __serializedSizeMemoized
          if (__size == 0) {
            __size = __computeSerializedSize() + 1
            __serializedSizeMemoized = __size
          }
          __size - 1
          
        }
        def writeTo(`_output__`: _root_.com.google.protobuf.CodedOutputStream): _root_.scala.Unit = {
          {
            val __v = mergeHostToDeviceStream
            if (__v != false) {
              _output__.writeBool(1, __v)
            }
          };
          {
            val __v = mergeDeviceToHostStream
            if (__v != false) {
              _output__.writeBool(2, __v)
            }
          };
          {
            val __v = mergeDeviceToDeviceStream
            if (__v != false) {
              _output__.writeBool(3, __v)
            }
          };
          unknownFields.writeTo(_output__)
        }
        def withMergeHostToDeviceStream(__v: _root_.scala.Boolean): StreamMergeOptions = copy(mergeHostToDeviceStream = __v)
        def withMergeDeviceToHostStream(__v: _root_.scala.Boolean): StreamMergeOptions = copy(mergeDeviceToHostStream = __v)
        def withMergeDeviceToDeviceStream(__v: _root_.scala.Boolean): StreamMergeOptions = copy(mergeDeviceToDeviceStream = __v)
        def withUnknownFields(__v: _root_.scalapb.UnknownFieldSet) = copy(unknownFields = __v)
        def discardUnknownFields = copy(unknownFields = _root_.scalapb.UnknownFieldSet.empty)
        def getFieldByNumber(__fieldNumber: _root_.scala.Int): _root_.scala.Any = {
          (__fieldNumber: @_root_.scala.unchecked) match {
            case 1 => {
              val __t = mergeHostToDeviceStream
              if (__t != false) __t else null
            }
            case 2 => {
              val __t = mergeDeviceToHostStream
              if (__t != false) __t else null
            }
            case 3 => {
              val __t = mergeDeviceToDeviceStream
              if (__t != false) __t else null
            }
          }
        }
        def getField(__field: _root_.scalapb.descriptors.FieldDescriptor): _root_.scalapb.descriptors.PValue = {
          _root_.scala.Predef.require(__field.containingMessage eq companion.scalaDescriptor)
          (__field.number: @_root_.scala.unchecked) match {
            case 1 => _root_.scalapb.descriptors.PBoolean(mergeHostToDeviceStream)
            case 2 => _root_.scalapb.descriptors.PBoolean(mergeDeviceToHostStream)
            case 3 => _root_.scalapb.descriptors.PBoolean(mergeDeviceToDeviceStream)
          }
        }
        def toProtoString: _root_.scala.Predef.String = _root_.scalapb.TextFormat.printToUnicodeString(this)
        def companion: GPUOptions.Experimental.StreamMergeOptions.type = config.GPUOptions.Experimental.StreamMergeOptions
        // @@protoc_insertion_point(GeneratedMessage[tensorboard.GPUOptions.Experimental.StreamMergeOptions])
    }
    
    object StreamMergeOptions extends scalapb.GeneratedMessageCompanion[GPUOptions.Experimental.StreamMergeOptions] {
      implicit def messageCompanion: scalapb.GeneratedMessageCompanion[GPUOptions.Experimental.StreamMergeOptions] = this
      def parseFrom(`_input__`: _root_.com.google.protobuf.CodedInputStream): GPUOptions.Experimental.StreamMergeOptions = {
        var __mergeHostToDeviceStream: _root_.scala.Boolean = false
        var __mergeDeviceToHostStream: _root_.scala.Boolean = false
        var __mergeDeviceToDeviceStream: _root_.scala.Boolean = false
        var `_unknownFields__`: _root_.scalapb.UnknownFieldSet.Builder = null
        var _done__ = false
        while (!_done__) {
          val _tag__ = _input__.readTag()
          _tag__ match {
            case 0 => _done__ = true
            case 8 =>
              __mergeHostToDeviceStream = _input__.readBool()
            case 16 =>
              __mergeDeviceToHostStream = _input__.readBool()
            case 24 =>
              __mergeDeviceToDeviceStream = _input__.readBool()
            case tag =>
              if (_unknownFields__ == null) {
                _unknownFields__ = new _root_.scalapb.UnknownFieldSet.Builder()
              }
              _unknownFields__.parseField(tag, _input__)
          }
        }
        config.GPUOptions.Experimental.StreamMergeOptions(
            mergeHostToDeviceStream = __mergeHostToDeviceStream,
            mergeDeviceToHostStream = __mergeDeviceToHostStream,
            mergeDeviceToDeviceStream = __mergeDeviceToDeviceStream,
            unknownFields = if (_unknownFields__ == null) _root_.scalapb.UnknownFieldSet.empty else _unknownFields__.result()
        )
      }
      implicit def messageReads: _root_.scalapb.descriptors.Reads[GPUOptions.Experimental.StreamMergeOptions] = _root_.scalapb.descriptors.Reads{
        case _root_.scalapb.descriptors.PMessage(__fieldsMap) =>
          _root_.scala.Predef.require(__fieldsMap.keys.forall(_.containingMessage eq scalaDescriptor), "FieldDescriptor does not match message type.")
          config.GPUOptions.Experimental.StreamMergeOptions(
            mergeHostToDeviceStream = __fieldsMap.get(scalaDescriptor.findFieldByNumber(1).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
            mergeDeviceToHostStream = __fieldsMap.get(scalaDescriptor.findFieldByNumber(2).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
            mergeDeviceToDeviceStream = __fieldsMap.get(scalaDescriptor.findFieldByNumber(3).get).map(_.as[_root_.scala.Boolean]).getOrElse(false)
          )
        case _ => throw new RuntimeException("Expected PMessage")
      }
      def javaDescriptor: _root_.com.google.protobuf.Descriptors.Descriptor = config.GPUOptions.Experimental.javaDescriptor.getNestedTypes().get(1)
      def scalaDescriptor: _root_.scalapb.descriptors.Descriptor = config.GPUOptions.Experimental.scalaDescriptor.nestedMessages(1)
      def messageCompanionForFieldNumber(__number: _root_.scala.Int): _root_.scalapb.GeneratedMessageCompanion[_] = throw new MatchError(__number)
      lazy val nestedMessagesCompanions: Seq[_root_.scalapb.GeneratedMessageCompanion[_ <: _root_.scalapb.GeneratedMessage]] = Seq.empty
      def enumCompanionForFieldNumber(__fieldNumber: _root_.scala.Int): _root_.scalapb.GeneratedEnumCompanion[_] = throw new MatchError(__fieldNumber)
      lazy val defaultInstance = config.GPUOptions.Experimental.StreamMergeOptions(
        mergeHostToDeviceStream = false,
        mergeDeviceToHostStream = false,
        mergeDeviceToDeviceStream = false
      )
      implicit class StreamMergeOptionsLens[UpperPB](_l: _root_.scalapb.lenses.Lens[UpperPB, GPUOptions.Experimental.StreamMergeOptions]) extends _root_.scalapb.lenses.ObjectLens[UpperPB, GPUOptions.Experimental.StreamMergeOptions](_l) {
        def mergeHostToDeviceStream: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.mergeHostToDeviceStream)((c_, f_) => c_.copy(mergeHostToDeviceStream = f_))
        def mergeDeviceToHostStream: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.mergeDeviceToHostStream)((c_, f_) => c_.copy(mergeDeviceToHostStream = f_))
        def mergeDeviceToDeviceStream: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.mergeDeviceToDeviceStream)((c_, f_) => c_.copy(mergeDeviceToDeviceStream = f_))
      }
      final val MERGE_HOST_TO_DEVICE_STREAM_FIELD_NUMBER = 1
      final val MERGE_DEVICE_TO_HOST_STREAM_FIELD_NUMBER = 2
      final val MERGE_DEVICE_TO_DEVICE_STREAM_FIELD_NUMBER = 3
      def of(
        mergeHostToDeviceStream: _root_.scala.Boolean,
        mergeDeviceToHostStream: _root_.scala.Boolean,
        mergeDeviceToDeviceStream: _root_.scala.Boolean
      ): GPUOptions.Experimental.StreamMergeOptions = config.GPUOptions.Experimental.StreamMergeOptions(
        mergeHostToDeviceStream,
        mergeDeviceToHostStream,
        mergeDeviceToDeviceStream
      )
      // @@protoc_insertion_point(GeneratedMessageCompanion[tensorboard.GPUOptions.Experimental.StreamMergeOptions])
    }
    
    implicit class ExperimentalLens[UpperPB](_l: _root_.scalapb.lenses.Lens[UpperPB, GPUOptions.Experimental]) extends _root_.scalapb.lenses.ObjectLens[UpperPB, GPUOptions.Experimental](_l) {
      def virtualDevices: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[GPUOptions.Experimental.VirtualDevices]] = field(_.virtualDevices)((c_, f_) => c_.copy(virtualDevices = f_))
      def numVirtualDevicesPerGpu: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Int] = field(_.numVirtualDevicesPerGpu)((c_, f_) => c_.copy(numVirtualDevicesPerGpu = f_))
      def useUnifiedMemory: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.useUnifiedMemory)((c_, f_) => c_.copy(useUnifiedMemory = f_))
      def numDevToDevCopyStreams: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Int] = field(_.numDevToDevCopyStreams)((c_, f_) => c_.copy(numDevToDevCopyStreams = f_))
      def collectiveRingOrder: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.collectiveRingOrder)((c_, f_) => c_.copy(collectiveRingOrder = f_))
      def timestampedAllocator: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.timestampedAllocator)((c_, f_) => c_.copy(timestampedAllocator = f_))
      def kernelTrackerMaxInterval: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Int] = field(_.kernelTrackerMaxInterval)((c_, f_) => c_.copy(kernelTrackerMaxInterval = f_))
      def kernelTrackerMaxBytes: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Int] = field(_.kernelTrackerMaxBytes)((c_, f_) => c_.copy(kernelTrackerMaxBytes = f_))
      def kernelTrackerMaxPending: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Int] = field(_.kernelTrackerMaxPending)((c_, f_) => c_.copy(kernelTrackerMaxPending = f_))
      def internalFragmentationFraction: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Double] = field(_.internalFragmentationFraction)((c_, f_) => c_.copy(internalFragmentationFraction = f_))
      def useCudaMallocAsync: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.useCudaMallocAsync)((c_, f_) => c_.copy(useCudaMallocAsync = f_))
      def disallowRetryOnAllocationFailure: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.disallowRetryOnAllocationFailure)((c_, f_) => c_.copy(disallowRetryOnAllocationFailure = f_))
      def gpuHostMemLimitInMb: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Float] = field(_.gpuHostMemLimitInMb)((c_, f_) => c_.copy(gpuHostMemLimitInMb = f_))
      def gpuHostMemDisallowGrowth: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.gpuHostMemDisallowGrowth)((c_, f_) => c_.copy(gpuHostMemDisallowGrowth = f_))
      def gpuSystemMemorySizeInMb: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Int] = field(_.gpuSystemMemorySizeInMb)((c_, f_) => c_.copy(gpuSystemMemorySizeInMb = f_))
      def populatePjrtGpuClientCreationInfo: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.populatePjrtGpuClientCreationInfo)((c_, f_) => c_.copy(populatePjrtGpuClientCreationInfo = f_))
      def nodeId: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Int] = field(_.nodeId)((c_, f_) => c_.copy(nodeId = f_))
      def streamMergeOptions: _root_.scalapb.lenses.Lens[UpperPB, GPUOptions.Experimental.StreamMergeOptions] = field(_.getStreamMergeOptions)((c_, f_) => c_.copy(streamMergeOptions = _root_.scala.Option(f_)))
      def optionalStreamMergeOptions: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Option[GPUOptions.Experimental.StreamMergeOptions]] = field(_.streamMergeOptions)((c_, f_) => c_.copy(streamMergeOptions = f_))
    }
    final val VIRTUAL_DEVICES_FIELD_NUMBER = 1
    final val NUM_VIRTUAL_DEVICES_PER_GPU_FIELD_NUMBER = 15
    final val USE_UNIFIED_MEMORY_FIELD_NUMBER = 2
    final val NUM_DEV_TO_DEV_COPY_STREAMS_FIELD_NUMBER = 3
    final val COLLECTIVE_RING_ORDER_FIELD_NUMBER = 4
    final val TIMESTAMPED_ALLOCATOR_FIELD_NUMBER = 5
    final val KERNEL_TRACKER_MAX_INTERVAL_FIELD_NUMBER = 7
    final val KERNEL_TRACKER_MAX_BYTES_FIELD_NUMBER = 8
    final val KERNEL_TRACKER_MAX_PENDING_FIELD_NUMBER = 9
    final val INTERNAL_FRAGMENTATION_FRACTION_FIELD_NUMBER = 10
    final val USE_CUDA_MALLOC_ASYNC_FIELD_NUMBER = 11
    final val DISALLOW_RETRY_ON_ALLOCATION_FAILURE_FIELD_NUMBER = 12
    final val GPU_HOST_MEM_LIMIT_IN_MB_FIELD_NUMBER = 13
    final val GPU_HOST_MEM_DISALLOW_GROWTH_FIELD_NUMBER = 14
    final val GPU_SYSTEM_MEMORY_SIZE_IN_MB_FIELD_NUMBER = 16
    final val POPULATE_PJRT_GPU_CLIENT_CREATION_INFO_FIELD_NUMBER = 17
    final val NODE_ID_FIELD_NUMBER = 18
    final val STREAM_MERGE_OPTIONS_FIELD_NUMBER = 19
    def of(
            virtualDevices: _root_.scala.Seq[GPUOptions.Experimental.VirtualDevices],
            numVirtualDevicesPerGpu: _root_.scala.Int,
            useUnifiedMemory: _root_.scala.Boolean,
            numDevToDevCopyStreams: _root_.scala.Int,
            collectiveRingOrder: _root_.scala.Predef.String,
            timestampedAllocator: _root_.scala.Boolean,
            kernelTrackerMaxInterval: _root_.scala.Int,
            kernelTrackerMaxBytes: _root_.scala.Int,
            kernelTrackerMaxPending: _root_.scala.Int,
            internalFragmentationFraction: _root_.scala.Double,
            useCudaMallocAsync: _root_.scala.Boolean,
            disallowRetryOnAllocationFailure: _root_.scala.Boolean,
            gpuHostMemLimitInMb: _root_.scala.Float,
            gpuHostMemDisallowGrowth: _root_.scala.Boolean,
            gpuSystemMemorySizeInMb: _root_.scala.Int,
            populatePjrtGpuClientCreationInfo: _root_.scala.Boolean,
            nodeId: _root_.scala.Int,
            streamMergeOptions: _root_.scala.Option[GPUOptions.Experimental.StreamMergeOptions]
    ): GPUOptions.Experimental = config.GPUOptions.Experimental(
      virtualDevices,
      numVirtualDevicesPerGpu,
      useUnifiedMemory,
      numDevToDevCopyStreams,
      collectiveRingOrder,
      timestampedAllocator,
      kernelTrackerMaxInterval,
      kernelTrackerMaxBytes,
      kernelTrackerMaxPending,
      internalFragmentationFraction,
      useCudaMallocAsync,
      disallowRetryOnAllocationFailure,
      gpuHostMemLimitInMb,
      gpuHostMemDisallowGrowth,
      gpuSystemMemorySizeInMb,
      populatePjrtGpuClientCreationInfo,
      nodeId,
      streamMergeOptions
    )
    // @@protoc_insertion_point(GeneratedMessageCompanion[tensorboard.GPUOptions.Experimental])
  }
  
  implicit class GPUOptionsLens[UpperPB](_l: _root_.scalapb.lenses.Lens[UpperPB, GPUOptions]) extends _root_.scalapb.lenses.ObjectLens[UpperPB, GPUOptions](_l) {
    def perProcessGpuMemoryFraction: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Double] = field(_.perProcessGpuMemoryFraction)((c_, f_) => c_.copy(perProcessGpuMemoryFraction = f_))
    def allowGrowth: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.allowGrowth)((c_, f_) => c_.copy(allowGrowth = f_))
    def allocatorType: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.allocatorType)((c_, f_) => c_.copy(allocatorType = f_))
    def deferredDeletionBytes: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Long] = field(_.deferredDeletionBytes)((c_, f_) => c_.copy(deferredDeletionBytes = f_))
    def visibleDeviceList: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.visibleDeviceList)((c_, f_) => c_.copy(visibleDeviceList = f_))
    def pollingActiveDelayUsecs: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Int] = field(_.pollingActiveDelayUsecs)((c_, f_) => c_.copy(pollingActiveDelayUsecs = f_))
    def pollingInactiveDelayMsecs: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Int] = field(_.pollingInactiveDelayMsecs)((c_, f_) => c_.copy(pollingInactiveDelayMsecs = f_))
    def forceGpuCompatible: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.forceGpuCompatible)((c_, f_) => c_.copy(forceGpuCompatible = f_))
    def experimental: _root_.scalapb.lenses.Lens[UpperPB, GPUOptions.Experimental] = field(_.getExperimental)((c_, f_) => c_.copy(experimental = _root_.scala.Option(f_)))
    def optionalExperimental: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Option[GPUOptions.Experimental]] = field(_.experimental)((c_, f_) => c_.copy(experimental = f_))
  }
  final val PER_PROCESS_GPU_MEMORY_FRACTION_FIELD_NUMBER = 1
  final val ALLOW_GROWTH_FIELD_NUMBER = 4
  final val ALLOCATOR_TYPE_FIELD_NUMBER = 2
  final val DEFERRED_DELETION_BYTES_FIELD_NUMBER = 3
  final val VISIBLE_DEVICE_LIST_FIELD_NUMBER = 5
  final val POLLING_ACTIVE_DELAY_USECS_FIELD_NUMBER = 6
  final val POLLING_INACTIVE_DELAY_MSECS_FIELD_NUMBER = 7
  final val FORCE_GPU_COMPATIBLE_FIELD_NUMBER = 8
  final val EXPERIMENTAL_FIELD_NUMBER = 9
  def of(
    perProcessGpuMemoryFraction: _root_.scala.Double,
    allowGrowth: _root_.scala.Boolean,
    allocatorType: _root_.scala.Predef.String,
    deferredDeletionBytes: _root_.scala.Long,
    visibleDeviceList: _root_.scala.Predef.String,
    pollingActiveDelayUsecs: _root_.scala.Int,
    pollingInactiveDelayMsecs: _root_.scala.Int,
    forceGpuCompatible: _root_.scala.Boolean,
    experimental: _root_.scala.Option[GPUOptions.Experimental]
  ): GPUOptions = GPUOptions(
    perProcessGpuMemoryFraction,
    allowGrowth,
    allocatorType,
    deferredDeletionBytes,
    visibleDeviceList,
    pollingActiveDelayUsecs,
    pollingInactiveDelayMsecs,
    forceGpuCompatible,
    experimental
  )
  // @@protoc_insertion_point(GeneratedMessageCompanion[tensorboard.GPUOptions])
}
